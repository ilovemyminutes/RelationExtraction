{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python377jvsc74a57bd098b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f",
   "display_name": "Python 3.7.7 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from transformers import BertTokenizer, BertTokenizerFast, BertForSequenceClassification, BertModel, BertConfig\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "from models import load_model\n",
    "from dataset import REDataset\n",
    "from config import Config, ModelType, PreTrainedType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig.from_pretrained(PreTrainedType.MultiLingual)\n",
    "config.num_labels = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(PreTrainedType.MultiLingual, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Load raw data...\tpreprocessing for 'Base'...\tdone!\n",
      "Load Tokenizer...\tdone!\n",
      "Apply Tokenization...\tdone!\n"
     ]
    }
   ],
   "source": [
    "dataset = REDataset(device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sents, labels in loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.bert(**sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooler = output.last_hidden_state[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([4, 42])"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "model.classifier(pooler).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaBert_v2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_type: str = ModelType.SequenceClf,  # BertForSequenceClassification\n",
    "        pretrained_type: str = PreTrainedType.MultiLingual,  # bert-base-multilingual-cased\n",
    "        num_labels: int = Config.NumClasses,  # 42\n",
    "        pooler_idx: int = 0\n",
    "    ):\n",
    "        super(VanillaBert_v2, self).__init__()\n",
    "        bert = self.load_bert(\n",
    "            model_type=model_type,\n",
    "            pretrained_type=pretrained_type,\n",
    "        )\n",
    "        self.backbone = bert.bert\n",
    "        self.dropout = bert.dropout\n",
    "        self.clf = bert.classifier\n",
    "        self.idx = 0 if pooler_idx == 0 else pooler_idx\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
    "        x = self.backbone(\n",
    "            input_ids=input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        x = x.last_hidden_state[:, self.idx, :]\n",
    "        x = self.dropout(x)\n",
    "        output = self.clf(x)\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def load_bert(model_type, pretrained_type):\n",
    "        config = BertConfig.from_pretrained(pretrained_type)\n",
    "        config.num_labels = 42\n",
    "        if model_type == ModelType.SequenceClf:\n",
    "            model = BertForSequenceClassification.from_pretrained(pretrained_type, config=config)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = VanillaBert_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-0.3085, -0.0310,  0.1683, -0.1096,  0.0322,  0.2899, -0.2387, -0.1071,\n",
       "          0.1112, -0.3434, -0.0009, -0.3300, -0.0189, -0.0739,  0.0318, -0.1613,\n",
       "          0.1797, -0.2363, -0.1003, -0.0215, -0.3221, -0.0310,  0.0659, -0.0153,\n",
       "          0.1841,  0.1360,  0.0297, -0.2904,  0.0027, -0.1875, -0.1578, -0.0472,\n",
       "          0.2544,  0.3674,  0.0224, -0.3766,  0.2296, -0.2980,  0.4823, -0.1805,\n",
       "          0.1215,  0.0884],\n",
       "        [-0.3178, -0.0420,  0.5670, -0.1458,  0.3103,  0.3062, -0.4243, -0.1841,\n",
       "          0.0180, -0.4984,  0.0176, -0.2405,  0.0719, -0.3171,  0.0287, -0.3817,\n",
       "          0.1676, -0.0818, -0.1787,  0.0245, -0.2646,  0.0456,  0.2499,  0.0211,\n",
       "          0.5569,  0.2341, -0.1474, -0.2639, -0.1208, -0.1170, -0.2079,  0.0302,\n",
       "         -0.0138,  0.3647,  0.0919, -0.4525,  0.3361, -0.0224,  0.3537, -0.3153,\n",
       "         -0.0371,  0.0761],\n",
       "        [-0.1305, -0.0609,  0.4915, -0.0239,  0.0627,  0.1599, -0.1920, -0.2792,\n",
       "          0.1001, -0.3685,  0.0419, -0.2608, -0.0087, -0.1381, -0.0463, -0.2021,\n",
       "          0.1034, -0.1794, -0.1384, -0.0544, -0.1763, -0.1280,  0.0806,  0.0696,\n",
       "          0.3274,  0.1910,  0.0291, -0.2343, -0.0489, -0.2149, -0.0605, -0.1470,\n",
       "          0.1708,  0.3493,  0.0546, -0.3818,  0.1025, -0.1922,  0.2025, -0.0817,\n",
       "          0.0399,  0.0775],\n",
       "        [-0.3324, -0.1546,  0.5358, -0.0141,  0.1724,  0.3538, -0.0277, -0.3849,\n",
       "          0.2843, -0.2575, -0.0066, -0.0141, -0.0665, -0.2008,  0.0020, -0.3455,\n",
       "          0.1137, -0.4120, -0.1799, -0.0884, -0.1748,  0.1247,  0.1147, -0.0386,\n",
       "          0.1345,  0.1740, -0.1737, -0.1824, -0.2183, -0.0781,  0.0122,  0.1430,\n",
       "          0.1853,  0.5613,  0.2649, -0.2512,  0.1894, -0.1758,  0.3394, -0.2700,\n",
       "          0.0754,  0.1125]], grad_fn=<AddmmBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "output"
   ]
  }
 ]
}