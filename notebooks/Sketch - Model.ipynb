{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python377jvsc74a57bd098b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f",
   "display_name": "Python 3.7.7 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertTokenizerFast, BertModel, BertConfig, DataCollatorForLanguageModeling, TrainingArguments, Trainer, DataCollatorForWholeWordMask\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "from models import load_model\n",
    "from dataset import REDataset\n",
    "from config import Config, ModelType, PreTrainedType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Load Tokenizer...\tdone!\n",
      "Load raw data...\tdone!\n",
      "Apply Tokenization...\tdone!\n"
     ]
    }
   ],
   "source": [
    "data = REDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CUDA\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained(PreTrainedType.BertMultiLingual)\n",
    "model.cuda()\n",
    "print('CUDA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent, label = data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = DataLoader(data, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sents, labels in temp:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "embeddings.word_embeddings.weight\nembeddings.position_embeddings.weight\nembeddings.token_type_embeddings.weight\nembeddings.LayerNorm.weight\nembeddings.LayerNorm.bias\nencoder.layer.0.attention.self.query.weight\nencoder.layer.0.attention.self.query.bias\nencoder.layer.0.attention.self.key.weight\nencoder.layer.0.attention.self.key.bias\nencoder.layer.0.attention.self.value.weight\nencoder.layer.0.attention.self.value.bias\nencoder.layer.0.attention.output.dense.weight\nencoder.layer.0.attention.output.dense.bias\nencoder.layer.0.attention.output.LayerNorm.weight\nencoder.layer.0.attention.output.LayerNorm.bias\nencoder.layer.0.intermediate.dense.weight\nencoder.layer.0.intermediate.dense.bias\nencoder.layer.0.output.dense.weight\nencoder.layer.0.output.dense.bias\nencoder.layer.0.output.LayerNorm.weight\nencoder.layer.0.output.LayerNorm.bias\nencoder.layer.1.attention.self.query.weight\nencoder.layer.1.attention.self.query.bias\nencoder.layer.1.attention.self.key.weight\nencoder.layer.1.attention.self.key.bias\nencoder.layer.1.attention.self.value.weight\nencoder.layer.1.attention.self.value.bias\nencoder.layer.1.attention.output.dense.weight\nencoder.layer.1.attention.output.dense.bias\nencoder.layer.1.attention.output.LayerNorm.weight\nencoder.layer.1.attention.output.LayerNorm.bias\nencoder.layer.1.intermediate.dense.weight\nencoder.layer.1.intermediate.dense.bias\nencoder.layer.1.output.dense.weight\nencoder.layer.1.output.dense.bias\nencoder.layer.1.output.LayerNorm.weight\nencoder.layer.1.output.LayerNorm.bias\nencoder.layer.2.attention.self.query.weight\nencoder.layer.2.attention.self.query.bias\nencoder.layer.2.attention.self.key.weight\nencoder.layer.2.attention.self.key.bias\nencoder.layer.2.attention.self.value.weight\nencoder.layer.2.attention.self.value.bias\nencoder.layer.2.attention.output.dense.weight\nencoder.layer.2.attention.output.dense.bias\nencoder.layer.2.attention.output.LayerNorm.weight\nencoder.layer.2.attention.output.LayerNorm.bias\nencoder.layer.2.intermediate.dense.weight\nencoder.layer.2.intermediate.dense.bias\nencoder.layer.2.output.dense.weight\nencoder.layer.2.output.dense.bias\nencoder.layer.2.output.LayerNorm.weight\nencoder.layer.2.output.LayerNorm.bias\nencoder.layer.3.attention.self.query.weight\nencoder.layer.3.attention.self.query.bias\nencoder.layer.3.attention.self.key.weight\nencoder.layer.3.attention.self.key.bias\nencoder.layer.3.attention.self.value.weight\nencoder.layer.3.attention.self.value.bias\nencoder.layer.3.attention.output.dense.weight\nencoder.layer.3.attention.output.dense.bias\nencoder.layer.3.attention.output.LayerNorm.weight\nencoder.layer.3.attention.output.LayerNorm.bias\nencoder.layer.3.intermediate.dense.weight\nencoder.layer.3.intermediate.dense.bias\nencoder.layer.3.output.dense.weight\nencoder.layer.3.output.dense.bias\nencoder.layer.3.output.LayerNorm.weight\nencoder.layer.3.output.LayerNorm.bias\nencoder.layer.4.attention.self.query.weight\nencoder.layer.4.attention.self.query.bias\nencoder.layer.4.attention.self.key.weight\nencoder.layer.4.attention.self.key.bias\nencoder.layer.4.attention.self.value.weight\nencoder.layer.4.attention.self.value.bias\nencoder.layer.4.attention.output.dense.weight\nencoder.layer.4.attention.output.dense.bias\nencoder.layer.4.attention.output.LayerNorm.weight\nencoder.layer.4.attention.output.LayerNorm.bias\nencoder.layer.4.intermediate.dense.weight\nencoder.layer.4.intermediate.dense.bias\nencoder.layer.4.output.dense.weight\nencoder.layer.4.output.dense.bias\nencoder.layer.4.output.LayerNorm.weight\nencoder.layer.4.output.LayerNorm.bias\nencoder.layer.5.attention.self.query.weight\nencoder.layer.5.attention.self.query.bias\nencoder.layer.5.attention.self.key.weight\nencoder.layer.5.attention.self.key.bias\nencoder.layer.5.attention.self.value.weight\nencoder.layer.5.attention.self.value.bias\nencoder.layer.5.attention.output.dense.weight\nencoder.layer.5.attention.output.dense.bias\nencoder.layer.5.attention.output.LayerNorm.weight\nencoder.layer.5.attention.output.LayerNorm.bias\nencoder.layer.5.intermediate.dense.weight\nencoder.layer.5.intermediate.dense.bias\nencoder.layer.5.output.dense.weight\nencoder.layer.5.output.dense.bias\nencoder.layer.5.output.LayerNorm.weight\nencoder.layer.5.output.LayerNorm.bias\nencoder.layer.6.attention.self.query.weight\nencoder.layer.6.attention.self.query.bias\nencoder.layer.6.attention.self.key.weight\nencoder.layer.6.attention.self.key.bias\nencoder.layer.6.attention.self.value.weight\nencoder.layer.6.attention.self.value.bias\nencoder.layer.6.attention.output.dense.weight\nencoder.layer.6.attention.output.dense.bias\nencoder.layer.6.attention.output.LayerNorm.weight\nencoder.layer.6.attention.output.LayerNorm.bias\nencoder.layer.6.intermediate.dense.weight\nencoder.layer.6.intermediate.dense.bias\nencoder.layer.6.output.dense.weight\nencoder.layer.6.output.dense.bias\nencoder.layer.6.output.LayerNorm.weight\nencoder.layer.6.output.LayerNorm.bias\nencoder.layer.7.attention.self.query.weight\nencoder.layer.7.attention.self.query.bias\nencoder.layer.7.attention.self.key.weight\nencoder.layer.7.attention.self.key.bias\nencoder.layer.7.attention.self.value.weight\nencoder.layer.7.attention.self.value.bias\nencoder.layer.7.attention.output.dense.weight\nencoder.layer.7.attention.output.dense.bias\nencoder.layer.7.attention.output.LayerNorm.weight\nencoder.layer.7.attention.output.LayerNorm.bias\nencoder.layer.7.intermediate.dense.weight\nencoder.layer.7.intermediate.dense.bias\nencoder.layer.7.output.dense.weight\nencoder.layer.7.output.dense.bias\nencoder.layer.7.output.LayerNorm.weight\nencoder.layer.7.output.LayerNorm.bias\nencoder.layer.8.attention.self.query.weight\nencoder.layer.8.attention.self.query.bias\nencoder.layer.8.attention.self.key.weight\nencoder.layer.8.attention.self.key.bias\nencoder.layer.8.attention.self.value.weight\nencoder.layer.8.attention.self.value.bias\nencoder.layer.8.attention.output.dense.weight\nencoder.layer.8.attention.output.dense.bias\nencoder.layer.8.attention.output.LayerNorm.weight\nencoder.layer.8.attention.output.LayerNorm.bias\nencoder.layer.8.intermediate.dense.weight\nencoder.layer.8.intermediate.dense.bias\nencoder.layer.8.output.dense.weight\nencoder.layer.8.output.dense.bias\nencoder.layer.8.output.LayerNorm.weight\nencoder.layer.8.output.LayerNorm.bias\nencoder.layer.9.attention.self.query.weight\nencoder.layer.9.attention.self.query.bias\nencoder.layer.9.attention.self.key.weight\nencoder.layer.9.attention.self.key.bias\nencoder.layer.9.attention.self.value.weight\nencoder.layer.9.attention.self.value.bias\nencoder.layer.9.attention.output.dense.weight\nencoder.layer.9.attention.output.dense.bias\nencoder.layer.9.attention.output.LayerNorm.weight\nencoder.layer.9.attention.output.LayerNorm.bias\nencoder.layer.9.intermediate.dense.weight\nencoder.layer.9.intermediate.dense.bias\nencoder.layer.9.output.dense.weight\nencoder.layer.9.output.dense.bias\nencoder.layer.9.output.LayerNorm.weight\nencoder.layer.9.output.LayerNorm.bias\nencoder.layer.10.attention.self.query.weight\nencoder.layer.10.attention.self.query.bias\nencoder.layer.10.attention.self.key.weight\nencoder.layer.10.attention.self.key.bias\nencoder.layer.10.attention.self.value.weight\nencoder.layer.10.attention.self.value.bias\nencoder.layer.10.attention.output.dense.weight\nencoder.layer.10.attention.output.dense.bias\nencoder.layer.10.attention.output.LayerNorm.weight\nencoder.layer.10.attention.output.LayerNorm.bias\nencoder.layer.10.intermediate.dense.weight\nencoder.layer.10.intermediate.dense.bias\nencoder.layer.10.output.dense.weight\nencoder.layer.10.output.dense.bias\nencoder.layer.10.output.LayerNorm.weight\nencoder.layer.10.output.LayerNorm.bias\nencoder.layer.11.attention.self.query.weight\nencoder.layer.11.attention.self.query.bias\nencoder.layer.11.attention.self.key.weight\nencoder.layer.11.attention.self.key.bias\nencoder.layer.11.attention.self.value.weight\nencoder.layer.11.attention.self.value.bias\nencoder.layer.11.attention.output.dense.weight\nencoder.layer.11.attention.output.dense.bias\nencoder.layer.11.attention.output.LayerNorm.weight\nencoder.layer.11.attention.output.LayerNorm.bias\nencoder.layer.11.intermediate.dense.weight\nencoder.layer.11.intermediate.dense.bias\nencoder.layer.11.output.dense.weight\nencoder.layer.11.output.dense.bias\nencoder.layer.11.output.LayerNorm.weight\nencoder.layer.11.output.LayerNorm.bias\npooler.dense.weight\npooler.dense.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 0.1927, -0.1140,  0.1545,  ..., -0.1682,  0.0765,  0.1716],\n",
       "        [ 0.2937, -0.0188,  0.3508,  ..., -0.1271,  0.1640,  0.1288],\n",
       "        [ 0.2748, -0.0108,  0.1434,  ..., -0.2278,  0.1751,  0.1524],\n",
       "        [ 0.3211, -0.1236,  0.1505,  ..., -0.3515,  0.0855,  0.1090]],\n",
       "       device='cuda:0', grad_fn=<TanhBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "outputs.pooler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PreTrainedType.BertMultiLingual)\n",
    "model = BertModel.from_pretrained(PreTrainedType.BertMultiLingual)\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101, 31178,   117, 15127, 17835, 10124, 21610, 10112,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.0569,  0.0820,  0.0835,  ...,  0.2801, -0.1775,  0.2176],\n",
       "         [-0.1544,  0.0196,  0.2836,  ...,  0.1256, -0.6682, -0.3504],\n",
       "         [-0.3150, -0.3662,  0.1486,  ..., -0.4080, -0.1579,  0.5193],\n",
       "         ...,\n",
       "         [ 0.2656, -0.3016, -0.5070,  ...,  0.3957, -0.2573, -0.0307],\n",
       "         [ 0.0419, -0.2688, -0.0519,  ...,  0.0905, -0.2808,  0.4754],\n",
       "         [-0.0482, -0.0653,  0.5319,  ...,  0.2482, -0.2556,  0.2320]]],\n",
       "       grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[ 0.3043,  0.0656,  0.2868, -0.1854, -0.1371,  0.5640,  0.2290,  0.2035,\n",
       "         -0.4755,  0.4321, -0.1011, -0.2794, -0.2156, -0.1278,  0.2104, -0.2177,\n",
       "          0.7091, -0.0092,  0.1960, -0.4344, -1.0000, -0.1208, -0.3457, -0.2106,\n",
       "         -0.3447,  0.1666, -0.2626,  0.0763,  0.1979, -0.1930,  0.1113, -1.0000,\n",
       "          0.5719,  0.7092,  0.2417, -0.0967,  0.2205,  0.2705,  0.2206, -0.3866,\n",
       "         -0.2654, -0.0908, -0.1782,  0.2449, -0.1834, -0.2459, -0.2140,  0.2404,\n",
       "         -0.3426,  0.1179,  0.1057,  0.3010,  0.5157,  0.3317,  0.2399,  0.2175,\n",
       "          0.2240,  0.2683,  0.4356, -0.2385, -0.0456,  0.3629,  0.1646, -0.1851,\n",
       "         -0.2624, -0.4005,  0.0805, -0.0520,  0.6177, -0.0836, -0.2512, -0.4851,\n",
       "         -0.1985,  0.1649,  0.0985, -0.2923,  0.3634,  0.3320,  0.0153, -0.2104,\n",
       "         -0.4136, -0.5783, -0.1684,  0.1689, -0.2240,  0.2801,  0.3826, -0.3551,\n",
       "          0.0998, -0.0355,  0.2569,  0.5724, -0.3112,  0.4229, -0.1496, -0.2456,\n",
       "         -0.8750, -0.1602, -0.0548, -0.4673, -0.2646,  0.2305, -0.4092, -0.3055,\n",
       "         -0.2764, -0.3610,  0.1436,  0.2881, -0.2208,  0.4187,  0.0986, -0.4792,\n",
       "         -0.2417,  0.0353, -0.2504,  0.9848, -0.3839,  0.2432, -0.1470, -0.2025,\n",
       "         -0.6647,  1.0000,  0.0622, -0.2237,  0.0261,  0.1622, -0.5495,  0.1246,\n",
       "          0.3651,  0.2482,  0.1935, -0.1074, -0.1687, -0.3699, -0.8547, -0.3043,\n",
       "         -0.1884,  0.4046, -0.4027, -0.1811,  0.0900,  0.5893,  0.2102, -0.1112,\n",
       "         -0.1918, -0.1395,  0.3999, -0.2856,  1.0000,  0.6956, -0.1502, -0.1968,\n",
       "          0.6121, -0.7360, -0.3531, -0.3941, -0.3081, -0.5198,  0.2791,  0.2303,\n",
       "          0.1062, -0.0927, -0.2173, -0.2701,  0.3556, -0.6930, -0.2094,  0.3175,\n",
       "          0.3865,  0.1932, -0.1435,  0.3576,  0.2295, -0.3738, -0.1364,  0.2904,\n",
       "          0.1059,  0.0604, -0.1614, -0.2196,  0.2279, -0.1517, -0.5941,  0.0728,\n",
       "         -0.0954, -0.5920,  0.0589,  0.0948, -0.2265,  0.3219, -0.1867,  0.2559,\n",
       "         -0.3689,  0.3069,  0.3368,  0.1718, -0.4846,  0.2885,  0.3194,  0.4060,\n",
       "          0.2755,  0.0925,  0.0257,  0.1835, -0.1216, -0.6045,  0.3194,  0.1986,\n",
       "          0.4257, -0.1904, -0.4402, -0.2187,  0.6396,  0.2475, -0.2939,  0.2539,\n",
       "          0.1675, -0.2328, -0.0925,  0.2611, -0.2596, -0.4859, -0.3483, -0.2302,\n",
       "         -0.0218,  0.2362,  0.1635,  0.2058,  0.1731, -0.2866, -0.0898, -0.2424,\n",
       "          0.0438,  0.3219,  0.0068,  0.8517, -0.2035,  0.2091, -0.5923, -0.1909,\n",
       "          0.4550, -0.2016,  0.1757,  0.9762,  0.1937, -0.2815,  0.2105,  0.1888,\n",
       "          0.2187, -0.3279,  0.0148, -0.6671,  0.6836,  0.3191,  0.2665, -1.0000,\n",
       "          0.3311,  0.2132,  0.4592,  0.2411,  0.2918,  0.1789,  0.3330,  0.9217,\n",
       "         -0.4782, -0.4738, -0.4140, -0.1910, -0.5208, -0.2714, -0.1918, -0.3573,\n",
       "         -0.2288, -0.0516, -0.1774,  0.2291,  0.2815, -0.9969,  0.8845,  0.1438,\n",
       "         -0.1304, -0.0756,  0.2130, -1.0000,  0.2516, -0.0776, -0.3065,  0.3590,\n",
       "         -0.5855, -0.2876,  0.1530,  0.5008,  0.2514,  0.2050,  0.1557,  0.5015,\n",
       "         -0.1558,  0.0615,  0.3031, -0.0457,  0.6967,  0.0381,  0.0611,  0.4680,\n",
       "         -0.0678,  0.3198, -0.2556,  0.2682,  0.4544,  0.2222,  0.0633, -0.3741,\n",
       "          0.1673, -0.8122,  0.1023, -0.2940, -0.1162, -0.0601,  0.1025, -0.2548,\n",
       "         -0.3321,  0.0409, -0.4062,  1.0000,  0.1403, -0.3016, -0.3427,  0.5821,\n",
       "          0.5614, -0.3352, -0.6459,  0.0327,  0.6283,  0.4142,  0.2222,  0.0780,\n",
       "         -0.2878,  0.3074, -0.1637, -0.2060, -0.0842, -0.4224,  0.2835, -0.1880,\n",
       "         -0.2443,  0.2380, -0.1876, -0.1785, -0.7875,  0.2686,  0.1177,  0.0872,\n",
       "          0.1662,  0.1306, -0.3397,  0.5739,  0.3910, -0.2600, -0.3565, -0.2448,\n",
       "         -0.2191,  0.1073, -0.2423, -0.3978,  0.1823, -0.7387,  0.2329,  0.0347,\n",
       "         -0.2625, -0.2611,  0.3339, -1.0000, -0.2223,  0.2753, -0.3422,  0.1848,\n",
       "         -0.3319, -0.1927,  0.1918,  0.2353,  0.0145,  0.1221, -0.4057,  0.2373,\n",
       "         -0.0698,  0.0989,  0.8409,  0.6095,  0.2038, -0.2398,  0.0613, -0.6216,\n",
       "         -0.2184,  0.3083,  0.1392, -0.2819,  0.2566,  0.2974,  0.2085, -0.1320,\n",
       "          0.3083, -0.1026, -0.1864,  0.2943, -0.0022, -0.2316, -0.2110,  0.3194,\n",
       "         -0.5709,  0.3343,  0.1407,  0.4103,  0.1866,  0.2766, -0.2241, -0.0205,\n",
       "         -0.2018, -0.0177, -0.2899, -0.2581, -0.1638,  1.0000,  0.2474,  0.4263,\n",
       "         -0.4723,  0.2702,  0.4071, -0.3153,  0.1808,  0.2557,  0.1144, -0.1909,\n",
       "          0.1146,  0.0321,  0.2879,  0.3203,  0.3278,  0.5471, -0.3652,  0.7609,\n",
       "         -0.2042, -0.3800, -0.9974,  0.2021,  0.3233, -0.4707, -0.6077,  0.2039,\n",
       "         -0.2474, -0.0280, -0.2594,  0.1050,  0.2305, -0.2195,  0.4418, -0.2800,\n",
       "          1.0000, -0.0257,  0.1084,  0.3168,  0.1780, -0.3480, -0.1121,  0.0415,\n",
       "          0.3279, -0.1086,  0.1498, -0.9578,  0.3457,  0.1222,  0.2943, -0.0874,\n",
       "          0.3505, -0.4280,  0.3109, -0.0171, -0.1010, -0.3313,  0.3362, -0.3452,\n",
       "          0.4997, -0.1310,  0.2467, -0.4295,  0.2869, -0.1318,  0.4260, -0.1889,\n",
       "          0.2150, -0.2183, -0.2943, -0.2494,  0.0836, -0.4898,  1.0000,  0.0421,\n",
       "          0.2415, -0.2906,  0.2648, -0.1888,  0.3747,  0.7778, -0.2762,  0.2276,\n",
       "          0.3803, -0.6896,  0.1929, -0.0527, -0.7243, -0.2473,  0.9679,  0.1282,\n",
       "          0.5007,  0.3933,  0.3361,  0.1759, -0.0918,  0.1504,  0.9244,  0.1940,\n",
       "          0.1417,  0.1647, -0.1562, -0.3758, -0.2218,  1.0000,  1.0000, -0.0516,\n",
       "          0.3961, -0.3864, -0.2542, -0.1881,  0.2163,  0.1820,  0.2830, -0.0210,\n",
       "         -0.0722, -0.4064, -0.1839, -0.0924, -0.0754, -0.0775,  0.0786, -0.3259,\n",
       "          0.5661,  0.4157,  0.0554,  0.4771,  0.2689,  0.2565, -0.0690, -0.2386,\n",
       "          0.5382, -0.2233, -0.0624, -0.3624,  0.0269, -1.0000, -0.2140, -0.1509,\n",
       "         -0.2286,  0.6007,  0.1375,  0.0469, -0.3019, -0.0717, -0.3982,  0.2483,\n",
       "          0.2432,  0.0329, -0.2231, -0.4392,  0.3868, -0.5131,  0.2162, -0.4382,\n",
       "         -0.3031, -0.6810, -0.2271, -0.2154,  0.3583, -0.1915, -0.1791,  0.2861,\n",
       "          0.3678,  0.2617, -0.3804,  0.3307, -0.3277,  0.0426,  0.3504,  0.2636,\n",
       "          0.1601, -0.3008, -0.3315, -0.1171, -0.1958, -0.2352,  0.3120, -0.3604,\n",
       "          0.2353, -0.2525,  0.1616, -0.2502,  0.0130,  0.2569,  0.5073, -0.2590,\n",
       "          0.5715,  0.3930, -0.0944,  0.4905,  0.0304, -0.3571, -0.2306,  1.0000,\n",
       "          0.5284,  0.1349,  0.1715, -0.0265,  0.3843,  0.1403,  0.5061, -0.2338,\n",
       "          0.7803, -0.2514,  0.3416,  0.1551,  0.2806,  0.0447,  0.2339,  0.4760,\n",
       "          0.7493,  0.2612,  0.3379,  0.3301,  0.2064,  0.3858,  0.3059,  0.2761,\n",
       "          0.3591,  0.3884, -0.2024,  0.3250, -0.0463, -0.2144, -0.0464, -0.2190,\n",
       "         -0.2074,  0.1099, -0.1694, -0.2152, -0.1738,  0.3717, -0.2383,  0.4053,\n",
       "         -0.0803, -0.2693,  0.5765, -0.5285,  0.3224, -0.1825,  0.1612, -0.8612,\n",
       "          0.1892, -0.1753, -0.5548, -0.1963, -0.4683,  0.1478,  0.2193, -0.2491,\n",
       "          0.2611, -0.2966,  0.3088, -0.2402, -0.1985,  0.0290, -1.0000,  0.1140,\n",
       "          0.1509, -0.3366,  0.1799,  0.0772,  0.1558,  0.1888, -0.1982, -0.2855,\n",
       "         -0.1396,  0.3264, -0.3373,  0.0493,  0.2954, -0.4042, -0.1802,  0.0425,\n",
       "         -0.1153,  0.1527,  0.3800, -0.4013,  0.2971, -0.3889,  0.2704,  0.0563,\n",
       "          0.2403, -0.4251, -0.2945,  0.3285, -0.5407, -0.4589, -0.1432,  0.0696,\n",
       "         -0.1831,  0.1566,  0.1953, -0.1126,  0.4028, -0.2402,  0.3593, -0.2983,\n",
       "          0.4513, -0.8391, -0.2651, -0.4160, -0.1078,  0.2266,  0.3165,  0.0803,\n",
       "          0.3080, -0.0270,  0.2792, -0.0789,  0.1682,  0.2307, -0.1635,  0.0978,\n",
       "         -0.2047,  0.3130, -0.4171,  0.1603, -0.9942, -0.3603,  0.0932,  0.3686,\n",
       "          0.3737, -0.2416, -0.0927, -0.3871, -0.0990,  0.2526,  0.3764,  0.2848,\n",
       "          0.2850,  0.3245, -0.2492,  0.0116,  0.7094, -0.3519, -0.1154,  0.4541,\n",
       "          0.2360,  0.8350,  0.3362,  0.3888,  0.1266, -0.2992,  0.3614,  0.3006]],\n",
       "       grad_fn=<TanhBackward>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Load Tokenizer...\tdone!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = load_tokenizer(\n",
    "    type=PreProcessType.Base\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Apply Tokenization...\tdone!\n"
     ]
    }
   ],
   "source": [
    "dataset_raw, labels = load_data(path=Config.Train)\n",
    "dataset_tokenized = apply_tokenization(\n",
    "    dataset=dataset_raw, tokenizer=tokenizer, method=PreProcessType.Base\n",
    ")\n",
    "dataset = REDataset(tokenized_dataset=dataset_tokenized, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForWholeWordMask(\n",
    "        tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataCollatorForWholeWordMask(tokenizer=PreTrainedTokenizer(name_or_path='bert-base-multilingual-cased', vocab_size=119547, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}), mlm=True, mlm_probability=0.15)"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(**TrainArgs.Base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': tensor([   101,  50266,  11489,   9405,  24974,  24683,   9477,  90578,   9625,\n",
       "         119376,  12692,  45725,   9651,  99183,  10459,   9376,  42771,  70186,\n",
       "           9167,  15001,  11261,  41605,    113,  12001,  57836,    114,   9590,\n",
       "           9706,  28396,    113,  13796,  19986,    114,   8843,  22634,    117,\n",
       "           9638,   9376,  42771,  22879,   9651,  99183,  10459,   9684,  46520,\n",
       "          11513,   9641, 119298,  11018,   9251,  11261,   9405,  24974, 118800,\n",
       "          27792,  16139,    119,    102,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]),\n",
       " 'labels': tensor(17)}"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': tensor([   101,  50266,  11489,   9405,  24974,  24683,   9477,  90578,   9625,\n",
       "         119376,  12692,  45725,   9651,  99183,  10459,   9376,  42771,  70186,\n",
       "           9167,  15001,  11261,  41605,    113,  12001,  57836,    114,   9590,\n",
       "           9706,  28396,    113,  13796,  19986,    114,   8843,  22634,    117,\n",
       "           9638,   9376,  42771,  22879,   9651,  99183,  10459,   9684,  46520,\n",
       "          11513,   9641, 119298,  11018,   9251,  11261,   9405,  24974, 118800,\n",
       "          27792,  16139,    119,    102,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]),\n",
       " 'labels': tensor(17)}"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "label = dataset['labels'], dataset['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = data_collator(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'[CLS] 용병 공격수 [MASK] [MASK] [MASK] [MASK] [MASK] 초 활약한 강수일의 침체, 시즌 중반에 영입한 세르비아 출신 [MASK] [MASK] 미드필더 오그넨 코로만의 부상 [MASK] 부진의 원인으로 지적되던 [MASK] 인천은 시즌 [MASK] 4경기에서 3승 1패를 거두며 막판 승점 [MASK] [MASK] [MASK] [MASK] [MASK] 정규리그 순위 5위로 플레이오프 [SEP]'"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "tokenizer.decode(temp['input_ids'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': tensor([   101,  50266,  11489,   9405,  24974,  24683,   9477,  90578,   9625,\n",
       "         119376,  12692,  45725,   9651,  99183,  10459,   9376,  42771,  70186,\n",
       "           9167,  15001,  11261,  41605,    113,  12001,  57836,    114,   9590,\n",
       "           9706,  28396,    113,  13796,  19986,    114,   8843,  22634,    117,\n",
       "           9638,   9376,  42771,  22879,   9651,  99183,  10459,   9684,  46520,\n",
       "          11513,   9641, 119298,  11018,   9251,  11261,   9405,  24974, 118800,\n",
       "          27792,  16139,    119,    102,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "              0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]),\n",
       " 'labels': tensor(17)}"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'[UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] 브랜드들은 [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] 일컫는 말로 [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]'"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "tokenizer.decode(temp['labels'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Expected input batch_size (32) to match target batch_size (3200).",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model_path, trial)\u001b[0m\n\u001b[1;32m    886\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m                     \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_total_flos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1248\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1275\u001b[0m         \u001b[0mSubclass\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0moverride\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcustom\u001b[0m \u001b[0mbehavior\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m         \"\"\"\n\u001b[0;32m-> 1277\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1278\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1511\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1512\u001b[0m                 \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1513\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1515\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 948\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2420\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2421\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2422\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2214\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2215\u001b[0m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[0;32m-> 2216\u001b[0;31m                          .format(input.size(0), target.size(0)))\n\u001b[0m\u001b[1;32m   2217\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2218\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (32) to match target batch_size (3200)."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(**TrainArgs.Base)\n",
    "\n",
    "data_collaor = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset\n",
    ")"
   ]
  }
 ]
}