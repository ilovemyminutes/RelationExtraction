{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python377jvsc74a57bd098b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f",
   "display_name": "Python 3.7.7 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline, BertForMaskedLM, BertTokenizer, BertConfig, BertForPreTraining, BertTokenizer, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "    \n",
    "sys.path.insert(0, '../')\n",
    "from config import Config, PreTrainedType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertForMaskedLM.from_pretrained(PreTrainedType.BertMultiLingual)\n",
    "tokenizer = AutoTokenizer.from_pretrained(PreTrainedType.BertMultiLingual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['이', '##순', '##신은', '조선', '중', '##기의', '무신', '##이다']\n{'input_ids': [2, 706, 1155, 7559, 2000, 754, 2605, 13160, 1895, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "example = '이순신은 조선 중기의 무신이다'\n",
    "print(tokenizer.tokenize(example))\n",
    "print(tokenizer(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp_fill = pipeline('fill-mask', top_k=5, model=model, tokenizer=tokenizer)\n",
    "# nlp_fill('Martin is living in [MASK].')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir my_data\n",
    "\n",
    "# # wiki corpu\n",
    "# !curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1zib1GI8Q5wV08TgYBa2GagqNh4jyfXZz\" > /dev/null\n",
    "# !curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1zib1GI8Q5wV08TgYBa2GagqNh4jyfXZz\" -o my_data/wiki_20190620_small.txt\n",
    "\n",
    "\n",
    "# # korean corpus\n",
    "# !curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1_F5fziHjUM-jKr5Pwcx1we6g_J2o70kZ\" > /dev/null\n",
    "# !curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1_F5fziHjUM-jKr5Pwcx1we6g_J2o70kZ\" -o my_data/wiki_20190620.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize empty tokenizer\n",
    "wp_tokenizer = BertWordPieceTokenizer(\n",
    "    clean_text=True, # whitespace 문자를 제거(\\t, \\n, \\r, '')\n",
    "    handle_chinese_chars=True,\n",
    "    strip_accents=False, # [CamelCase] -> [Camel, Case]\n",
    "    lowercase=False # Hello -> hello\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "wp_tokenizer.train(\n",
    "    files=\"my_data/wiki_20190620_small.txt\",\n",
    "    vocab_size=20000,   # vocab size 를 지정해줄 수 있습니다.\n",
    "    min_frequency=2,\n",
    "    show_progress=True,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
    "    wordpieces_prefix=\"##\"\n",
    ")"
   ]
  },
  {
   "source": [
    "## Train BERT"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer(\n",
    "    vocab_file=\"./wordPieceTokenizer/my_tokenizer-vocab.txt\",\n",
    "    max_len=128,\n",
    "    do_lower_case=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['이', '##순', '##신은', '조선', '중', '##기의', '무신', '##이다']\n"
     ]
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({'mask_token': '[MASK]'})\n",
    "print(tokenizer.tokenize(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig(\n",
    "    vocab_size=20000,\n",
    "    hidden_size=512,\n",
    "    num_hidden_layers=12,\n",
    "    num_attention_heads=8,\n",
    "    intermediate_size=3072, # transformer 내 FFN 사이즈\n",
    "    hidden_act='gelu',\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    max_position_embeddings=128, # 최대 임베딩 사이즈. 최대 몇 개의 토큰까지를 이별긍로 받을지 결정. default. 512\n",
    "    type_vocab_size=2, # default\n",
    "    pad_token_id=0, # default\n",
    "    position_embedding_type='absolute' # default\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "61278754"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "model = BertForPreTraining(config=config)\n",
    "model.num_parameters()"
   ]
  },
  {
   "source": [
    "## Data Collection for BERT"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "from filelock import FileLock\n",
    "from typing import Dict, List, Optional\n",
    "from torch.utils.data import Dataset\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "from transformers.utils import logging\n",
    "\n",
    "logger = logging.get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDatasetForNextSentencePrediction(Dataset):\n",
    "    \"\"\"\n",
    "    This will be superseded by a framework-agnostic approach soon.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        file_path: str,\n",
    "        block_size: int,\n",
    "        overwrite_cache=False,\n",
    "        short_seq_probability=0.1,\n",
    "        nsp_probability=0.5,\n",
    "    ):\n",
    "        # 여기 부분은 학습 데이터를 caching하는 부분입니다 :-)\n",
    "        assert os.path.isfile(file_path), f\"Input file path {file_path} not found\"\n",
    "\n",
    "        self.block_size = block_size - tokenizer.num_special_tokens_to_add(pair=True)\n",
    "        self.short_seq_probability = short_seq_probability\n",
    "        self.nsp_probability = nsp_probability\n",
    "\n",
    "        directory, filename = os.path.split(file_path)\n",
    "        cached_features_file = os.path.join(\n",
    "            directory,\n",
    "            \"cached_nsp_{}_{}_{}\".format(\n",
    "                tokenizer.__class__.__name__,\n",
    "                str(block_size),\n",
    "                filename,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        lock_path = cached_features_file + \".lock\"\n",
    "\n",
    "        # Input file format:\n",
    "        # (1) One sentence per line. These should ideally be actual sentences, not\n",
    "        # entire paragraphs or arbitrary spans of text. (Because we use the\n",
    "        # sentence boundaries for the \"next sentence prediction\" task).\n",
    "        # (2) Blank lines between documents. Document boundaries are needed so\n",
    "        # that the \"next sentence prediction\" task doesn't span between documents.\n",
    "        #\n",
    "        # Example:\n",
    "        # I am very happy.\n",
    "        # Here is the second sentence.\n",
    "        #\n",
    "        # A new document.\n",
    "\n",
    "        with FileLock(lock_path):\n",
    "            if os.path.exists(cached_features_file) and not overwrite_cache:\n",
    "                start = time.time()\n",
    "                with open(cached_features_file, \"rb\") as handle:\n",
    "                    self.examples = pickle.load(handle)\n",
    "                logger.info(\n",
    "                    f\"Loading features from cached file {cached_features_file} [took %.3f s]\", time.time() - start\n",
    "                )\n",
    "            else:\n",
    "                logger.info(f\"Creating features from dataset file at {directory}\")\n",
    "                # 여기서부터 본격적으로 dataset을 만듭니다.\n",
    "                self.documents = [[]]\n",
    "                with open(file_path, encoding=\"utf-8\") as f:\n",
    "                    while True: # 일단 문장을 읽고\n",
    "                        line = f.readline()\n",
    "                        if not line:\n",
    "                            break\n",
    "                        line = line.strip()\n",
    "\n",
    "                        # 이중 띄어쓰기가 발견된다면, 나왔던 문장들을 모아 하나의 문서로 묶어버립니다.\n",
    "                        # 즉, 문단 단위로 데이터를 저장합니다.\n",
    "                        if not line and len(self.documents[-1]) != 0:\n",
    "                            self.documents.append([])\n",
    "                        tokens = tokenizer.tokenize(line)\n",
    "                        tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
    "                        if tokens:\n",
    "                            self.documents[-1].append(tokens)\n",
    "                # 이제 코퍼스 전체를 읽고, 문서 데이터를 생성했습니다! :-)\n",
    "                logger.info(f\"Creating examples from {len(self.documents)} documents.\")\n",
    "                self.examples = []\n",
    "                # 본격적으로 학습을 위한 데이터로 변형시켜볼까요?\n",
    "                for doc_index, document in enumerate(self.documents):\n",
    "                    self.create_examples_from_document(document, doc_index) # 함수로 가봅시다.\n",
    "\n",
    "                start = time.time()\n",
    "                with open(cached_features_file, \"wb\") as handle:\n",
    "                    pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                logger.info(\n",
    "                    \"Saving features into cached file %s [took %.3f s]\", cached_features_file, time.time() - start\n",
    "                )\n",
    "\n",
    "    def create_examples_from_document(self, document: List[List[int]], doc_index: int):\n",
    "        \"\"\"Creates examples for a single document.\"\"\"\n",
    "        # 문장의 앞, 뒤에 [CLS], [SEP] token이 부착되기 때문에, 내가 지정한 size에서 2 만큼 빼줍니다.\n",
    "        # 예를 들어 128 token 만큼만 학습 가능한 model을 선언했다면, 학습 데이터로부터는 최대 126 token만 가져오게 됩니다.\n",
    "        max_num_tokens = self.block_size - self.tokenizer.num_special_tokens_to_add(pair=True)\n",
    "\n",
    "        # We *usually* want to fill up the entire sequence since we are padding\n",
    "        # to `block_size` anyways, so short sequences are generally wasted\n",
    "        # computation. However, we *sometimes*\n",
    "        # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter\n",
    "        # sequences to minimize the mismatch between pretraining and fine-tuning.\n",
    "        # The `target_seq_length` is just a rough target however, whereas\n",
    "        # `block_size` is a hard limit.\n",
    "\n",
    "        # 여기가 재밌는 부분인데요!\n",
    "        # 위에서 설명했듯이, 학습 데이터는 126 token(128-2)을 채워서 만들어지는게 목적입니다.\n",
    "        # 하지만 나중에 BERT를 사용할 때, 126 token 이내의 짧은 문장을 테스트하는 경우도 분명 많을 것입니다 :-)\n",
    "        # 그래서 short_seq_probability 만큼의 데이터에서는 2-126 사이의 random 값으로 학습 데이터를 만들게 됩니다.\n",
    "        target_seq_length = max_num_tokens\n",
    "        if random.random() < self.short_seq_probability:\n",
    "            target_seq_length = random.randint(2, max_num_tokens)\n",
    "\n",
    "        current_chunk = []  # a buffer stored current working segments\n",
    "        current_length = 0\n",
    "        i = 0\n",
    "\n",
    "        # 데이터 구축의 단위는 document 입니다\n",
    "        # 이 때, 무조건 문장_1[SEP]문장_2 이렇게 만들어지는 것이 아니라,\n",
    "        # 126 token을 꽉 채울 수 있게 문장_1+문장_2[SEP]문장_3+문장_4 형태로 만들어질 수 있습니다.\n",
    "        while i < len(document):\n",
    "            segment = document[i]\n",
    "            current_chunk.append(segment)\n",
    "            current_length += len(segment)\n",
    "            if i == len(document) - 1 or current_length >= target_seq_length:\n",
    "                if current_chunk:\n",
    "                    # `a_end` is how many segments from `current_chunk` go into the `A`\n",
    "                    # (first) sentence.\n",
    "                    a_end = 1\n",
    "                    # 여기서 문장_1+문장_2 가 이루어졌을 때, 길이를 random하게 짤라버립니다 :-)\n",
    "                    if len(current_chunk) >= 2:\n",
    "                        a_end = random.randint(1, len(current_chunk) - 1)\n",
    "                    tokens_a = []\n",
    "                    for j in range(a_end):\n",
    "                        tokens_a.extend(current_chunk[j])\n",
    "                    # 이제 [SEP] 뒷 부분인 segmentB를 살펴볼까요?\n",
    "                    tokens_b = []\n",
    "                    # 50%의 확률로 랜덤하게 다른 문장을 선택하거나, 다음 문장을 학습데이터로 만듭니다.\n",
    "                    if len(current_chunk) == 1 or random.random() < self.nsp_probability:\n",
    "                        is_random_next = True\n",
    "                        target_b_length = target_seq_length - len(tokens_a)\n",
    "\n",
    "                        # This should rarely go for more than one iteration for large\n",
    "                        # corpora. However, just to be careful, we try to make sure that\n",
    "                        # the random document is not the same as the document\n",
    "                        # we're processing.\n",
    "                        for _ in range(10):\n",
    "                            random_document_index = random.randint(0, len(self.documents) - 1)\n",
    "                            if random_document_index != doc_index:\n",
    "                                break\n",
    "                        # 여기서 랜덤하게 선택합니다 :-)\n",
    "                        random_document = self.documents[random_document_index]\n",
    "                        random_start = random.randint(0, len(random_document) - 1)\n",
    "                        for j in range(random_start, len(random_document)):\n",
    "                            tokens_b.extend(random_document[j])\n",
    "                            if len(tokens_b) >= target_b_length:\n",
    "                                break\n",
    "                        # We didn't actually use these segments so we \"put them back\" so\n",
    "                        # they don't go to waste.\n",
    "                        num_unused_segments = len(current_chunk) - a_end\n",
    "                        i -= num_unused_segments\n",
    "                    # Actual next\n",
    "                    else:\n",
    "                        is_random_next = False\n",
    "                        for j in range(a_end, len(current_chunk)):\n",
    "                            tokens_b.extend(current_chunk[j])\n",
    "\n",
    "                    # 이제 126 token을 넘는다면 truncation을 해야합니다.\n",
    "                    # 이 때, 126 token 이내로 들어온다면 행위를 멈추고,\n",
    "                    # 만약 126 token을 넘는다면, segmentA와 segmentB에서 랜덤하게 하나씩 제거합니다.\n",
    "                    def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens):\n",
    "                        \"\"\"Truncates a pair of sequences to a maximum sequence length.\"\"\"\n",
    "                        while True:\n",
    "                            total_length = len(tokens_a) + len(tokens_b)\n",
    "                            if total_length <= max_num_tokens:\n",
    "                                break\n",
    "                            trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n",
    "                            assert len(trunc_tokens) >= 1\n",
    "                            # We want to sometimes truncate from the front and sometimes from the\n",
    "                            # back to add more randomness and avoid biases.\n",
    "                            if random.random() < 0.5:\n",
    "                                del trunc_tokens[0]\n",
    "                            else:\n",
    "                                trunc_tokens.pop()\n",
    "\n",
    "                    truncate_seq_pair(tokens_a, tokens_b, max_num_tokens)\n",
    "\n",
    "                    assert len(tokens_a) >= 1\n",
    "                    assert len(tokens_b) >= 1\n",
    "\n",
    "                    # add special tokens\n",
    "                    input_ids = self.tokenizer.build_inputs_with_special_tokens(tokens_a, tokens_b)\n",
    "                    # add token type ids, 0 for sentence a, 1 for sentence b\n",
    "                    token_type_ids = self.tokenizer.create_token_type_ids_from_sequences(tokens_a, tokens_b)\n",
    "                    \n",
    "                    # 드디어 아래 항목에 대한 데이터셋이 만들어졌습니다! :-)\n",
    "                    # 즉, segmentA[SEP]segmentB, [0, 0, .., 0, 1, 1, ..., 1], NSP 데이터가 만들어진 것입니다 :-)\n",
    "                    # 그럼 다음은.. 이 데이터에 [MASK] 를 씌워야겠죠?\n",
    "                    example = {\n",
    "                        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "                        \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "                        \"next_sentence_label\": torch.tensor(1 if is_random_next else 0, dtype=torch.long),\n",
    "                    }\n",
    "\n",
    "                    self.examples.append(example)\n",
    "\n",
    "                current_chunk = []\n",
    "                current_length = 0\n",
    "\n",
    "            i += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.examples[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextDatasetForNextSentencePrediction(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='./my_data/wiki_20190620_small.txt',\n",
    "    block_size=128,\n",
    "    overwrite_cache=False,\n",
    "    short_seq_probability=0.1,\n",
    "    nsp_probability=0.5,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(    # [MASK] 를 씌우는 것은 저희가 구현하지 않아도 됩니다! :-)\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': tensor([   101,   9672,  36240,  12605,   9551,    107,   9706,  22458,    107,\n",
       "           9786,  21876,   9689,  25503, 106340,   9311,  16323,  21928,   9768,\n",
       "          25387,  23545,  11303,  48506,  70672,  30919,    119,   9706,  22458,\n",
       "           9786,  21876,  11018,   9678,  12508,  16985,  16323,   9430,  21876,\n",
       "           9786,  21614,  45725,   9944,  56645,  12030,  12605,   9246,  10622,\n",
       "          11489,  88921,    119,    102,  32537,  78686,   9641,  12609,    119,\n",
       "          11087,  10954,  23545,   9960,  17360,   9069,  92454,   9576,  50450,\n",
       "          36251,  18347,   9136, 119312,    217,   9279,  18227,  33727,   8843,\n",
       "         118698,  25685,   9089,  10622,   9339,  17706,    119,  21555,   9353,\n",
       "          66923,    107,   9136, 119312,   9027,  14646,    107,   9202,   9524,\n",
       "          26737,  32855,    119,  10828,  10954,   9678,  12508,  16985,   9689,\n",
       "           9414,  14279,   9637,  14279,   9428,  41521,  11489,   8983,  18471,\n",
       "          35506,  16439,   8924,   9428,  41521,  11287,   9365,  16605,  18471,\n",
       "          41521,   9573,  59724,   9645, 119230,  17594,  37909,    102]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1]),\n",
       " 'next_sentence_label': tensor(0)}"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'input_ids': tensor([   101,   9672,  36240,  12605,   9551,    107,   9706,  22458,    107,\n          9786,  21876,   9689,  25503, 106340,   9311,  16323,  21928,   9768,\n         25387,  23545,  11303,  48506,  70672,  30919,    119,   9706,  22458,\n          9786,  21876,  11018,   9678,  12508,  16985,  16323,   9430,  21876,\n          9786,  21614,  45725,   9944,  56645,  12030,  12605,   9246,  10622,\n         11489,  88921,    119,    102,  32537,  78686,   9641,  12609,    119,\n         11087,  10954,  23545,   9960,  17360,   9069,  92454,   9576,  50450,\n         36251,  18347,   9136, 119312,    217,   9279,  18227,  33727,   8843,\n        118698,  25685,   9089,  10622,   9339,  17706,    119,  21555,   9353,\n         66923,    107,   9136, 119312,   9027,  14646,    107,   9202,   9524,\n         26737,  32855,    119,  10828,  10954,   9678,  12508,  16985,   9689,\n          9414,  14279,   9637,  14279,   9428,  41521,  11489,   8983,  18471,\n         35506,  16439,   8924,   9428,  41521,  11287,   9365,  16605,  18471,\n         41521,   9573,  59724,   9645, 119230,  17594,  37909,    102]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1]), 'next_sentence_label': tensor(0)}\n"
     ]
    }
   ],
   "source": [
    "for example in dataset.examples[0:1]:\n",
    "    print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'input_ids': tensor([[  101,  9672, 36240,  ..., 17594, 37909,   102],\n        [  101, 32537, 12490,  ..., 12490,   119,   102],\n        [  101, 21890, 70347,  ..., 10003, 30005,   102],\n        ...,\n        [  101,   103,  8885,  ...,   103,  9694,   102],\n        [  101,   103,   117,  ...,  8932, 21611,   102],\n        [  101,  9272, 22333,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 1, 1, 1],\n        [0, 0, 0,  ..., 1, 1, 1],\n        [0, 0, 0,  ..., 1, 1, 1],\n        ...,\n        [0, 0, 0,  ..., 1, 1, 1],\n        [0, 0, 0,  ..., 1, 1, 1],\n        [0, 0, 0,  ..., 0, 0, 0]]), 'next_sentence_label': tensor([0, 0, 0,  ..., 1, 0, 0]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1],\n        ...,\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 1, 1, 1],\n        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n        [ -100,  -100,  -100,  ...,  -100,  -100,  -100],\n        ...,\n        [ -100,  9701,  -100,  ..., 27023,  -100,  -100],\n        [ -100,  9701,  -100,  ...,  -100,  -100,  -100],\n        [ -100,  -100,  -100,  ...,  -100,  -100,  -100]])}\n"
     ]
    }
   ],
   "source": [
    "print(data_collator(dataset.examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n         -100,  -100,  5379,  -100,  -100,  2407,    16,  -100,  -100,  -100,\n         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  3666,\n           16,  -100,  -100,  -100,  -100,  -100,  -100,   174,  -100,  -100,\n         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n           94,   438,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n         -100,  -100,  -100, 17663,  -100,  -100,  -100,  -100,  -100,  -100,\n         -100,  -100,  -100,  1212,  -100,  4860,  -100,  -100,   174,  -100,\n         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  6530,  -100,\n         -100,  -100,  -100,  -100,    16,  -100,  -100,  -100,  -100,  -100,\n         -100, 17629,  -100,  -100,  -100])\n"
     ]
    }
   ],
   "source": [
    "print(data_collator(dataset.examples)['labels'][0]) # -100이 뭐지? 아무런 변형하지 않은 부분인 것 같긴 하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[CLS] 제임스 얼 [MASK] 지미 \" 카터 주니 [MASK] 민주당 출신 미국 39번째 대통령 이다. 지미 카터는 [MASK] 섬터 카 [MASK]티 플레인스 마을에 [MASK] 태어났다. 조지아 공과대학교를 졸업하였다 [MASK] [MASK] 후 해군 [MASK] 들어가 전 [MASK] · 원자력 · 잠수함의 승무 총 일하였다 [MASK] [SEP] [MASK] 예편하였고 이후 땅콩 · [MASK]화 등을 [UNK] 많은 돈을 벌었다. 그의 별명이 \" 땅콩 농부 \" [MASK] 알려졌다. [MASK] 조지아 [MASK] [MASK]원 의원 선거에서 낙선하나 그 선거가 부정선거 였음을 입증하게 되어 당선되고, 1966년 조지아 주 지사 [MASK] 낙선하지만 1970년 조지아 주 지사를 [MASK]했다. 대통령이 [MASK] 전 조지아주 상원의원을 [MASK] 연임했으며 [SEP]\n"
     ]
    }
   ],
   "source": [
    "sample = data_collator(dataset.examples)\n",
    "encode_sample = sample['input_ids'][0].tolist()\n",
    "label_sample = sample['labels'][0].tolist()\n",
    "print(tokenizer.decode(encode_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input ID [4]\nLabel [16249]\nDecoded [MASK]\n"
     ]
    }
   ],
   "source": [
    "idx = 19\n",
    "print('Input ID', [encode_sample[idx]])\n",
    "print('Label', [label_sample[idx]])\n",
    "print('Decoded', tokenizer.decode([encode_sample[idx]]))"
   ]
  },
  {
   "source": [
    "Mask Input ID: 4  \n",
    "Mask Label: 5, 2489, 16249 <- 해당 위치에 원래 어떤 레이블이 있었는지. -100읽 경우에는 replace 또는 masking이 되지 않은 부분임"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'[CLS] [MASK] 얼 \" 지미 \" 카터 주니어는 민주당 출신 미국 39번째 대통령 이다 [MASK] 지미 [MASK] 조지아주 섬터 카운티 플레인스 마을에서 태어났다. 조지아 공과대학교를 졸업하였다. 그 후 해군에 들어가 전함 · 원자력 · 잠수함의 승무원으로 일하였다 [MASK] [SEP]위로 [MASK]편하였고 이후 땅콩 · 면화 등을 [UNK] 많은 돈을 벌었다. 그의 별명이 \" 땅콩 농부 [MASK] 로 알려졌다. 1962년 조지아 [MASK] 상원 의원 [MASK] 낙선하나 그 [MASK] [MASK] DN음을 입증럭 되어 당선되고, 1966년 조지아 주 지사 선거에 낙선하지만 [MASK] 조지아 주 지사를 역임 [MASK]. 대통령이 [MASK] 전 [MASK] 상원의원을 두번 연임했으며 [SEP]'"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "tokenizer.decode(data_collator(dataset.examples)['input_ids'][0].tolist())"
   ]
  },
  {
   "source": [
    "## Trainer를 활용한 학습"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='model_output',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=32,\n",
    "    save_steps=1000, # 1000 스텝마다 모델을 저장하겠다\n",
    "    save_total_limit=2, # 마지막 두 스텝 iteration에 대한 모델만 저장하고 나머지는 삭제하겠다. 지나치게 많은 모델이 저장되지 않도록 하기 위해 사용\n",
    "    logging_steps=100\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n        </style>\n      \n      <progress value='820' max='820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [820/820 02:51, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>8.430500</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>7.695400</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>7.529500</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>7.471800</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>7.445700</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>7.268400</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>7.211400</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>7.186700</td>\n    </tr>\n  </tbody>\n</table><p>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['이', '##순', '##신은', '조선', '중', '##기의', '무신', '##이다']"
      ]
     },
     "metadata": {},
     "execution_count": 98
    }
   ],
   "source": [
    "tokenizer.tokenize(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_fill = pipeline('fill-mask', top_k=5, model=m)"
   ]
  }
 ]
}