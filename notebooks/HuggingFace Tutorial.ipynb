{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python377jvsc74a57bd098b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer\n",
    "from torch import nn\n",
    "MODEL_NAME = 'bert-base-multilingual-cased'"
   ]
  },
  {
   "source": [
    "'bert-base-multilingual-cased'의 경우 BertTokenizerFast 토크나이저 클래스를 활용함"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'transformers.models.bert.modeling_bert.BertModel'>\n<class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(type(model))\n",
    "print(type(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "пункты 22593\n",
      "julkaisi 45839\n",
      "portail 106318\n",
      "##ds 13268\n",
      "samom 61677\n",
      "stanja 104065\n",
      "##weld 93423\n",
      "প্রথম 21716\n",
      "119547\n"
     ]
    }
   ],
   "source": [
    "for idx, i in enumerate(tokenizer.vocab):\n",
    "    if idx <=7:\n",
    "        print(i, tokenizer.vocab[i]) # 텍스트, 인코딩ID\n",
    "    else:\n",
    "        break\n",
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '이순신은 조건 중기의 무신이다.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input_ids tensor([[   101,   9638, 119064,  25387,  10892,   9678,  71439,   9694,  46874,\n           9294,  25387,  11925,    119,    102]])\ntoken_type_ids tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\nattention_mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "tokenized_input_text = tokenizer(text, return_tensors='pt') # 파이토치 텐서로 리턴\n",
    "for key, value in tokenized_input_text.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "source": [
    "`input_ids`: 토크나이징 결과 각 토큰의 vocab ID가 담긴 텐서  \n",
    "`token_type_ids`: 각 토큰이 어떤 문장에 포함되는 지 ID가 담긴 텐서  \n",
    "`attention_mask`: 패딩 토큰이 포함되어 있을 경우 해당 attention value는 0\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[   101,   9638, 119064,  25387,  10892,   9678,  71439,   9694,  46874,\n",
       "           9294,  25387,  11925,    119,    102]])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "tokenized_input_text.input_ids # <=> tokenized_input_text['input_ids']"
   ]
  },
  {
   "source": [
    "`tokenizer.tokenize(text)`: 입력된 텍스트의 토크나이징 결과를 리턴"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Tokenize, Encode, Decode"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tokenize: ['이', '##순', '##신', '##은', '조', '##건', '중', '##기의', '무', '##신', '##이다', '.']\nencode: [101, 9638, 119064, 25387, 10892, 9678, 71439, 9694, 46874, 9294, 25387, 11925, 119, 102]\ndecode: [CLS] 이순신은 조건 중기의 무신이다. [SEP]\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenizer.tokenize(text)\n",
    "input_ids = tokenizer.encode(text) # 알아서 스페셜 토큰을 부착한 모습(101-[CLS], 102-[SEP])\n",
    "decoded_ids = tokenizer.decode(input_ids)\n",
    "print(f'tokenize: {tokenized_text}')\n",
    "print(f'encode: {input_ids}')\n",
    "print(f'decode: {decoded_ids}') # 알아서 스페셜 토큰을 부착한 모습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tokenize: ['이', '##순', '##신', '##은', '조', '##건', '중', '##기의', '무', '##신', '##이다', '.']\nencode: [9638, 119064, 25387, 10892, 9678, 71439, 9694, 46874, 9294, 25387, 11925, 119]\ndecode: 이순신은 조건 중기의 무신이다.\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenizer.tokenize(text, add_special_tokens=False)\n",
    "input_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "decoded_ids = tokenizer.decode(input_ids)\n",
    "print(f'tokenize: {tokenized_text}')\n",
    "print(f'encode: {input_ids}')\n",
    "print(f'decode: {decoded_ids}') # 스페셜 토큰을 부착하지 않을 수도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tokenize: ['이', '##순', '##신', '##은', '조']\ninput_ids [9638, 119064, 25387, 10892, 9678]\n이순신은 조\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenizer.tokenize(\n",
    "    text,\n",
    "    add_special_tokens=False,\n",
    "    max_length=5, # 최대 길이 설정\n",
    "    truncation=True\n",
    "    )\n",
    "print('tokenize:', tokenized_text)\n",
    "\n",
    "input_ids = tokenizer.encode(\n",
    "    text,\n",
    "    add_special_tokens=False,\n",
    "    max_length=5, # 최대 길이 설정\n",
    "    truncation=True\n",
    "    )\n",
    "print('input_ids', input_ids)\n",
    "print(tokenizer.decode(input_ids))"
   ]
  },
  {
   "source": [
    "## Padding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[PAD]\n0\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.pad_token)\n",
    "print(tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tokenize ['이', '##순', '##신', '##은', '조', '##건', '중', '##기의', '무', '##신', '##이다', '.', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenizer.tokenize(\n",
    "    text,\n",
    "    add_special_tokens=False,\n",
    "    max_length=20, # 최대 길이 설정\n",
    "    padding='max_length'\n",
    "    )\n",
    "print('tokenize', tokenized_text) # 길이에 맞춰 [PAD] 토큰이 추가된 모습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input_ids [9638, 119064, 25387, 10892, 9678, 71439, 9694, 46874, 9294, 25387, 11925, 119, 0, 0, 0, 0, 0, 0, 0, 0]\ndecoded 이순신은 조건 중기의 무신이다. [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\n",
    "    text,\n",
    "    add_special_tokens=False,\n",
    "    max_length=20, # 최대 길이 설정\n",
    "    padding='max_length'\n",
    "    )\n",
    "print('input_ids', input_ids) # [PAD] 토큰의 인코딩 ID 0이 추가된 모습\n",
    "print('decoded', tokenizer.decode(input_ids))"
   ]
  },
  {
   "source": [
    "## Update New Token"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tokenize ['머', '##리', '##카', '##락', '##이', '[UNK]', '나', '##와', '##서', '우', '##리가', '청', '##소', '다', '해', '##야', '##돼', '##요']\ninput_ids [101, 9265, 12692, 24206, 107693, 10739, 100, 8982, 12638, 12424, 9604, 44130, 9751, 22333, 9056, 9960, 21711, 118798, 48549, 102]\ndecoded [CLS] 머리카락이 [UNK] 나와서 우리가 청소 다 해야돼요 [SEP]\n"
     ]
    }
   ],
   "source": [
    "text = '머리카락이 켸쇽 나와서 우리가 청소 다 해야돼요'\n",
    "\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print('tokenize', tokenized_text)\n",
    "\n",
    "input_ids = tokenizer.encode(text)\n",
    "print('input_ids', input_ids)\n",
    "print('decoded', tokenizer.decode(input_ids))"
   ]
  },
  {
   "source": [
    "위처럼 '켸쇽'과 같은 vocabulary에 기록되지 않은 토큰은 `[UNK]` 토큰으로 인코딩. 이러한 토큰이 많을 수록 문장은 본연의 의미를 잃어갈 수밖에 없음 => 새로운 토큰 추가의 필요성"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "added_token_num = tokenizer.add_tokens(['켸쇽'])\n",
    "print(added_token_num) # vocab에 새롭게 추가된 토큰 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['머', '##리', '##카', '##락', '##이', '켸쇽', '나', '##와', '##서', '우', '##리가', '청', '##소', '다', '해', '##야', '##돼', '##요']\ninput_ids [9265, 12692, 24206, 107693, 10739, 119547, 8982, 12638, 12424, 9604, 44130, 9751, 22333, 9056, 9960, 21711, 118798, 48549]\ndecoded_ids 머리카락이 켸쇽 나와서 우리가 청소 다 해야돼요\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenizer.tokenize(text, add_special_tokens=False)\n",
    "print(tokenized_text)\n",
    "\n",
    "input_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "decoded_ids = tokenizer.decode(input_ids)\n",
    "\n",
    "print('input_ids', input_ids)\n",
    "print('decoded_ids', decoded_ids) # vocab이 업데이트되어 '켸쇽'의 인코딩이 [UNK]로 되지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "119547"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "source": [
    "위키트리 데이터만을 가지고 vocabulary가 구성되어 있으니, 도메인에 맞게 vocabulary를 업데이트해줄 필요가 있음!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Special Tokens"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tokenized ['[', 'J', '##H', '##KO', ']', '이', '##순', '##신', '##은', '조선', '중', '##기의', '무', '##신', '##이다', '[', '/', 'J', '##H', '##KO', ']']\n"
     ]
    }
   ],
   "source": [
    "# [JHKO]라는 스페셜 토큰을 추가해보고 싶은데\n",
    "text = \"[JHKO]이순신은 조선 중기의 무신이다[/JHKO]\"\n",
    "\n",
    "tokenized_text = tokenizer.tokenize(text, add_special_tokens=False)\n",
    "print('tokenized', tokenized_text) # 일반적인 토큰으로 인식되어 토크나이징이 의도대로 되지 않는다"
   ]
  },
  {
   "source": [
    "이럴 땐 `add_special_tokens` 메서드를 통해 스페셜 토큰을 추가"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_token_num += tokenizer.add_special_tokens({'additional_special_tokens': ['[JHKO]', '[/JHKO]']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tokenize ['[JHKO]', '이', '##순', '##신', '##은', '조선', '중', '##기의', '무', '##신', '##이다', '[/JHKO]']\ninput_ids [119548, 9638, 119064, 25387, 10892, 59906, 9694, 46874, 9294, 25387, 11925, 119549]\ndecoded_ids [JHKO] 이순신은 조선 중기의 무신이다 [/JHKO]\ndecoded_ids_skipped 이순신은 조선 중기의 무신이다\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenizer.tokenize(text ,add_special_tokens=False)\n",
    "print('tokenize', tokenized_text)\n",
    "\n",
    "input_ids = tokenizer.encode(text ,add_special_tokens=False)\n",
    "decoded_ids = tokenizer.decode(input_ids)\n",
    "decoded_ids_skipped = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "\n",
    "print('input_ids', input_ids) # [JHKO] 스페셜 토큰이 119548의 ID로 맵핑된 모습\n",
    "print('decoded_ids', decoded_ids)\n",
    "print('decoded_ids_skipped', decoded_ids_skipped) # 스페셜 토큰이 제거된 모습"
   ]
  },
  {
   "source": [
    "## 토큰이 새롭게 추가됐다면 신경써야할 부분"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "이미 pretrained 된 모델은 기존 vocab size에 맞춰져 있기 때문에, 토큰을 새로 추가했을 경우 별도의 모델 교정이 필요"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Single segment token, str ['[CLS]', '이', '##순', '##신', '##은', '조선', '중', '##기의', '무', '##신', '##이다', '[SEP]']\nSingle segment token, int [101, 9638, 119064, 25387, 10892, 59906, 9694, 46874, 9294, 25387, 11925, 102]\nSingle segment type [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "single_seg_input = tokenizer('이순신은 조선 중기의 무신이다')\n",
    "\n",
    "# [CLS] 토큰이 부여된 모습\n",
    "print('Single segment token, str', tokenizer.convert_ids_to_tokens(single_seg_input['input_ids']))\n",
    "print('Single segment token, int', single_seg_input['input_ids'])\n",
    "print('Single segment type', single_seg_input['token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "multi segment token, str ['[CLS]', '이', '##순', '##신', '##은', '조선', '중', '##기의', '무', '##신', '##이다', '[SEP]', '그는', '임', '##진', '##왜', '##란', '##을', '승', '##리로', '이', '##끌', '##었다', '[SEP]']\nmulti segment token, int [101, 9638, 119064, 25387, 10892, 59906, 9694, 46874, 9294, 25387, 11925, 102, 17889, 9644, 18623, 119164, 49919, 10622, 9484, 100434, 9638, 118705, 17706, 102]\nmulti segment type [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "multi_seg_input = tokenizer('이순신은 조선 중기의 무신이다', '그는 임진왜란을 승리로 이끌었다')\n",
    "\n",
    "# [CLS], [SEP] 토큰이 부여된 모습\n",
    "print('multi segment token, str', tokenizer.convert_ids_to_tokens(multi_seg_input['input_ids']))\n",
    "print('multi segment token, int', multi_seg_input['input_ids'])\n",
    "print('multi segment type', multi_seg_input['token_type_ids'])"
   ]
  },
  {
   "source": [
    "## BERT 사용해보기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['이', '##순', '##신', '##은', '[MASK]', '중', '##기의', '무', '##신', '##이다']\n"
     ]
    }
   ],
   "source": [
    "text = '이순신은 [MASK] 중기의 무신이다'\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'sequence': '[CLS] 이순신은 조선 중기의 무신이다 [SEP]',\n",
       "  'score': 0.8562734723091125,\n",
       "  'token': 59906,\n",
       "  'token_str': '조선'},\n",
       " {'sequence': '[CLS] 이순신은 청 중기의 무신이다 [SEP]',\n",
       "  'score': 0.050685591995716095,\n",
       "  'token': 9751,\n",
       "  'token_str': '청'},\n",
       " {'sequence': '[CLS] 이순신은 전 중기의 무신이다 [SEP]',\n",
       "  'score': 0.019383328035473824,\n",
       "  'token': 9665,\n",
       "  'token_str': '전'},\n",
       " {'sequence': '[CLS] 이순신은기 중기의 무신이다 [SEP]',\n",
       "  'score': 0.007541158702224493,\n",
       "  'token': 12310,\n",
       "  'token_str': '##기'},\n",
       " {'sequence': '[CLS] 이순신은기의 중기의 무신이다 [SEP]',\n",
       "  'score': 0.002962720114737749,\n",
       "  'token': 46874,\n",
       "  'token_str': '##기의'}]"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlp_fill = pipeline('fill-mask', model=MODEL_NAME)\n",
    "nlp_fill(text)"
   ]
  },
  {
   "source": [
    "모델의 출력 결과도 확인 가능"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input_ids tensor([[   101,   9638, 119064,  25387,  10892,  59906,   9694,  46874,   9294,\n          25387,  11925,    119,    102]])\ntoken_type_ids tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\nattention_mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\nshape of last_hidden_state torch.Size([1, 13, 768])\nshape of pooler_outputs torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "tokens_pt = tokenizer(\"이순신은 조선 중기의 무신이다.\", return_tensors='pt')\n",
    "\n",
    "for key, value in tokens_pt.items():\n",
    "    print(key, value)\n",
    "\n",
    "outputs = model(**tokens_pt) # input_ids, token_type_ids, attention_mask를 모두 입력하여 inference\n",
    "last_hidden_state = outputs.last_hidden_state # 모든 hidden state를 담은 텐서\n",
    "pooler_output = outputs.pooler_output # [CLS]에 대한 final hidden state\n",
    "\n",
    "print('shape of last_hidden_state', last_hidden_state.shape)\n",
    "print('shape of pooler_outputs', pooler_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Embedding(119547, 768, padding_idx=0)\n",
      "# of new tokens 3\n",
      "Embedding(119550, 768)\n"
     ]
    }
   ],
   "source": [
    "# 기존 입력 임베딩 차원 수\r\n",
    "print(model.get_input_embeddings())\r\n",
    "\r\n",
    "# 새로 추가된 토큰 수\r\n",
    "print('# of new tokens', added_token_num)\r\n",
    "\r\n",
    "# 모델 입력 차원 수정\r\n",
    "model.resize_token_embeddings(tokenizer.vocab_size + added_token_num)\r\n",
    "print(model.get_input_embeddings()) # 차원이 늘어난 모습\r\n"
   ]
  },
  {
   "source": [
    "## `[CLS]` 토큰을 활용해 문장 간 유사도 측정 가능"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = tokenizer('오늘 하루는 어떻게 보내셨나요?', return_tensors='pt')\n",
    "sent2 = tokenizer('오늘은 어떤 하루를 보내셨나요?', return_tensors='pt')\n",
    "sent3 = tokenizer('이순신은 조선 중기의 무신이다.', return_tensors='pt')\n",
    "sent4 = tokenizer('머리카락이 켸쇽 나와서 우리가 청소 다 해야돼요', return_tensors='pt')\n",
    "\n",
    "outputs = model(**sent1)\n",
    "pooler_output1 = outputs.pooler_output\n",
    "\n",
    "outputs = model(**sent2)\n",
    "pooler_output2 = outputs.pooler_output\n",
    "\n",
    "outputs = model(**sent3)\n",
    "pooler_output3 = outputs.pooler_output\n",
    "\n",
    "outputs = model(**sent4)\n",
    "pooler_output4 = outputs.pooler_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0.9896], grad_fn=<DivBackward0>)\ntensor([0.5918], grad_fn=<DivBackward0>)\ntensor([0.6075], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cos_sim = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "print(cos_sim(pooler_output1, pooler_output2)) # 문장 간 유사도가 높게 측정된 모습\n",
    "print(cos_sim(pooler_output1, pooler_output3)) # 다른 맥락으로 유사도가 낮게 측정된 모습\n",
    "print(cos_sim(pooler_output2, pooler_output3)) "
   ]
  }
 ]
}