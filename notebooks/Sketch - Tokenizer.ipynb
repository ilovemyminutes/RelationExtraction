{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python377jvsc74a57bd098b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f",
   "display_name": "Python 3.7.7 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "from dataset import load_data, REDataset\n",
    "from config import Config, PreTrainedType, PreProcessType\n",
    "from tokenization import SpecialToken\n",
    "from utils import load_pickle\n",
    "from models import VanillaBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Load raw data...\tpreprocessing for 'Base'...\tdone!\n",
      "Load Tokenizer...\tdone!\n",
      "Apply Tokenization...\tdone!\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD = \"../saved_models/VanillaBert_bert-base-multilingual-cased_20210418160912/VanillaBert_bert-base-multilingual-cased_ep(02)acc(0.4878)loss(0.0048)id(20210418160912).pth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Load raw data...\tpreprocessing for 'EntitySeparation'...\tdone!\n",
      "Load Tokenizer...\tdone!\n",
      "Apply Tokenization...\tdone!\n"
     ]
    }
   ],
   "source": [
    "data = REDataset(preprocess_type=PreProcessType.ES,device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[100, 102, 0, 101, 103]"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "data.tokenizer.all_special_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '랜',\n",
       " '##드',\n",
       " '##로',\n",
       " '##버',\n",
       " '[SEP]',\n",
       " '자',\n",
       " '##동차',\n",
       " '[SEP]',\n",
       " '영국',\n",
       " '##에서',\n",
       " '사',\n",
       " '##용',\n",
       " '##되는',\n",
       " '스',\n",
       " '##포츠',\n",
       " '유',\n",
       " '##틸',\n",
       " '##리',\n",
       " '##티',\n",
       " '자',\n",
       " '##동차',\n",
       " '##의',\n",
       " '브',\n",
       " '##랜드',\n",
       " '##로는',\n",
       " '랜',\n",
       " '##드',\n",
       " '##로',\n",
       " '##버',\n",
       " '(',\n",
       " 'Land',\n",
       " 'Rover',\n",
       " ')',\n",
       " '와',\n",
       " '지',\n",
       " '##프',\n",
       " '(',\n",
       " 'Je',\n",
       " '##ep',\n",
       " ')',\n",
       " '가',\n",
       " '있으며',\n",
       " ',',\n",
       " '이',\n",
       " '브',\n",
       " '##랜드',\n",
       " '##들은',\n",
       " '자',\n",
       " '##동차',\n",
       " '##의',\n",
       " '종',\n",
       " '##류',\n",
       " '##를',\n",
       " '일',\n",
       " '##컫',\n",
       " '##는',\n",
       " '말',\n",
       " '##로',\n",
       " '사',\n",
       " '##용',\n",
       " '##되',\n",
       " '##기도',\n",
       " '한다',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]']"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "data.tokenizer.convert_ids_to_tokens(data[0][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-f2a543ce2583>, line 49)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-f2a543ce2583>\"\u001b[0;36m, line \u001b[0;32m49\u001b[0m\n\u001b[0;31m    if\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class REDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str = Config.Train,\n",
    "        preprocess_type: str = PreProcessType.Base,\n",
    "        device: str = Config.Device,\n",
    "    ):\n",
    "        self.tokenizer = load_tokenizer(type=tokenization_type)\n",
    "        self.enc = LabelEncoder()\n",
    "        raw = self._load_raw(root)\n",
    "        self.sentences = self._tokenize(raw)\n",
    "        self.labels = raw[\"label\"].tolist()\n",
    "        self.device = device\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[dict, torch.Tensor]:\n",
    "        sentence = {\n",
    "            key: torch.as_tensor(val[idx]).to(self.device)\n",
    "            for key, val in self.sentences.items()\n",
    "        }\n",
    "        label = torch.as_tensor(self.labels[idx]).to(self.device)\n",
    "        return sentence, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def _load_raw(self, root):\n",
    "        print(\"Load raw data...\", end=\"\\t\")\n",
    "        raw = pd.read_csv(root, sep=\"\\t\", header=None)\n",
    "        raw.columns = COLUMNS\n",
    "        raw = raw.drop(\"id\", axis=1)\n",
    "        raw[\"label\"] = raw[\"label\"].apply(lambda x: self.enc.transform(x))\n",
    "        print(\"done!\")\n",
    "        return raw\n",
    "\n",
    "    def _tokenize(self, data):\n",
    "        print(\"Apply Tokenization...\", end=\"\\t\")\n",
    "        data_tokenized = self.tokenizer(\n",
    "            data[\"relation_state\"].tolist(),\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            add_special_tokens=True,\n",
    "        )\n",
    "        print(\"done!\")\n",
    "        return data_tokenized\n",
    "\n",
    "    def _text_preprocess(self, data, preprocess_type):\n",
    "        if preprocess_type == PreProcessType.Base:\n",
    "            data['']\n",
    "            return data\n",
    "        elif preprocess_type == PreProcessType.ES:\n",
    "            return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\n",
    "    data['relation_state'],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    add_special_tokens=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PreTrainedType.BertMultiLingual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['num_tokens'] = data['relation_state'].apply(lambda x: len(tokenizer.tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['relation_state'] = data['e1'] + SpecialToken.SEP + data['e2'] + SpecialToken.SEP + data['relation_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': [101, 9167, 15001, 11261, 41605, 102, 9651, 99183, 102, 50266, 11489, 9405, 24974, 24683, 9477, 90578, 9625, 119376, 12692, 45725, 9651, 99183, 10459, 9376, 42771, 70186, 9167, 15001, 11261, 41605, 113, 12001, 57836, 114, 9590, 9706, 28396, 113, 13796, 19986, 114, 8843, 22634, 117, 9638, 9376, 42771, 22879, 9651, 99183, 10459, 9684, 46520, 11513, 9641, 119298, 11018, 9251, 11261, 9405, 24974, 118800, 27792, 16139, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "tokenizer(data['relation_state'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = list(map(lambda x: tokenizer(x), data['relation_state']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '랜',\n",
       " '##드',\n",
       " '##로',\n",
       " '##버',\n",
       " '[SEP]',\n",
       " '자',\n",
       " '##동차',\n",
       " '[SEP]',\n",
       " '영국',\n",
       " '##에서',\n",
       " '사',\n",
       " '##용',\n",
       " '##되는',\n",
       " '스',\n",
       " '##포츠',\n",
       " '유',\n",
       " '##틸',\n",
       " '##리',\n",
       " '##티',\n",
       " '자',\n",
       " '##동차',\n",
       " '##의',\n",
       " '브',\n",
       " '##랜드',\n",
       " '##로는',\n",
       " '랜',\n",
       " '##드',\n",
       " '##로',\n",
       " '##버',\n",
       " '(',\n",
       " 'Land',\n",
       " 'Rover',\n",
       " ')',\n",
       " '와',\n",
       " '지',\n",
       " '##프',\n",
       " '(',\n",
       " 'Je',\n",
       " '##ep',\n",
       " ')',\n",
       " '가',\n",
       " '있으며',\n",
       " ',',\n",
       " '이',\n",
       " '브',\n",
       " '##랜드',\n",
       " '##들은',\n",
       " '자',\n",
       " '##동차',\n",
       " '##의',\n",
       " '종',\n",
       " '##류',\n",
       " '##를',\n",
       " '일',\n",
       " '##컫',\n",
       " '##는',\n",
       " '말',\n",
       " '##로',\n",
       " '사',\n",
       " '##용',\n",
       " '##되',\n",
       " '##기도',\n",
       " '한다',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(output[0].input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                         relation_state        e1  e1_start  \\\n",
       "0     영국에서 사용되는 스포츠 유틸리티 자동차의 브랜드로는 랜드로버(Land Rover)...      랜드로버        30   \n",
       "1     선거에서 민주당은 해산 전 의석인 230석에 한참 못 미치는 57석(지역구 27석,...       민주당         5   \n",
       "2     유럽 축구 연맹(UEFA) 집행위원회는 2014년 1월 24일에 열린 회의를 통해 ...  유럽 축구 연맹         0   \n",
       "3     용병 공격수 챠디의 부진과 시즌 초 활약한 강수일의 침체, 시즌 중반에 영입한 세르...       강수일        24   \n",
       "4     람캄행 왕은 1237년에서 1247년 사이 수코타이의 왕 퍼쿤 씨 인트라팃과 쓰엉 ...       람캄행         0   \n",
       "...                                                 ...       ...       ...   \n",
       "8995  2002년 FIFA 월드컵 사우디아라비아와의 1차전에서 독일은 8-0으로 승리하였는...   사우디아라비아        15   \n",
       "8996  일본의 2대 메이커인 토요타와 닛산은 시장 점유율을 높이기 위한 신차 개발을 계속하...       토요타        12   \n",
       "8997  방호의의 손자 방덕룡(方德龍)은 1588년(선조 21년) 무과에 급제하고 낙안군수로...       방덕룡         8   \n",
       "8998  LG전자는 올해 초 국내시장에 출시한 2020년형 ‘LG 그램’ 시리즈를 이달부터 ...      LG전자         0   \n",
       "8999  전남도의회 안전건설소방위원회 차영수 의원(강진1)은 지난 14일 설 명절을 앞두고 ...       차영수        16   \n",
       "\n",
       "      e1_end              e2  e2_start  e2_end  label  num_tokens  \n",
       "0         33             자동차        19      21     17          56  \n",
       "1          7             27석        42      44      0          44  \n",
       "2          7            UEFA         9      12      6          50  \n",
       "3         26             공격수         3       5      2         108  \n",
       "4          2       퍼쿤 씨 인트라팃        32      40      8          32  \n",
       "...      ...             ...       ...     ...    ...         ...  \n",
       "8995      21           2002년         0       4      0          34  \n",
       "8996      14              일본         0       1      9          32  \n",
       "8997      10  선무원종공신(宣武原從功臣)        93     106      2          84  \n",
       "8998       3              북미        46      47      0          44  \n",
       "8999      18              의원        20      21      2          72  \n",
       "\n",
       "[9000 rows x 9 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>relation_state</th>\n      <th>e1</th>\n      <th>e1_start</th>\n      <th>e1_end</th>\n      <th>e2</th>\n      <th>e2_start</th>\n      <th>e2_end</th>\n      <th>label</th>\n      <th>num_tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>영국에서 사용되는 스포츠 유틸리티 자동차의 브랜드로는 랜드로버(Land Rover)...</td>\n      <td>랜드로버</td>\n      <td>30</td>\n      <td>33</td>\n      <td>자동차</td>\n      <td>19</td>\n      <td>21</td>\n      <td>17</td>\n      <td>56</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>선거에서 민주당은 해산 전 의석인 230석에 한참 못 미치는 57석(지역구 27석,...</td>\n      <td>민주당</td>\n      <td>5</td>\n      <td>7</td>\n      <td>27석</td>\n      <td>42</td>\n      <td>44</td>\n      <td>0</td>\n      <td>44</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>유럽 축구 연맹(UEFA) 집행위원회는 2014년 1월 24일에 열린 회의를 통해 ...</td>\n      <td>유럽 축구 연맹</td>\n      <td>0</td>\n      <td>7</td>\n      <td>UEFA</td>\n      <td>9</td>\n      <td>12</td>\n      <td>6</td>\n      <td>50</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>용병 공격수 챠디의 부진과 시즌 초 활약한 강수일의 침체, 시즌 중반에 영입한 세르...</td>\n      <td>강수일</td>\n      <td>24</td>\n      <td>26</td>\n      <td>공격수</td>\n      <td>3</td>\n      <td>5</td>\n      <td>2</td>\n      <td>108</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>람캄행 왕은 1237년에서 1247년 사이 수코타이의 왕 퍼쿤 씨 인트라팃과 쓰엉 ...</td>\n      <td>람캄행</td>\n      <td>0</td>\n      <td>2</td>\n      <td>퍼쿤 씨 인트라팃</td>\n      <td>32</td>\n      <td>40</td>\n      <td>8</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>8995</th>\n      <td>2002년 FIFA 월드컵 사우디아라비아와의 1차전에서 독일은 8-0으로 승리하였는...</td>\n      <td>사우디아라비아</td>\n      <td>15</td>\n      <td>21</td>\n      <td>2002년</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>8996</th>\n      <td>일본의 2대 메이커인 토요타와 닛산은 시장 점유율을 높이기 위한 신차 개발을 계속하...</td>\n      <td>토요타</td>\n      <td>12</td>\n      <td>14</td>\n      <td>일본</td>\n      <td>0</td>\n      <td>1</td>\n      <td>9</td>\n      <td>32</td>\n    </tr>\n    <tr>\n      <th>8997</th>\n      <td>방호의의 손자 방덕룡(方德龍)은 1588년(선조 21년) 무과에 급제하고 낙안군수로...</td>\n      <td>방덕룡</td>\n      <td>8</td>\n      <td>10</td>\n      <td>선무원종공신(宣武原從功臣)</td>\n      <td>93</td>\n      <td>106</td>\n      <td>2</td>\n      <td>84</td>\n    </tr>\n    <tr>\n      <th>8998</th>\n      <td>LG전자는 올해 초 국내시장에 출시한 2020년형 ‘LG 그램’ 시리즈를 이달부터 ...</td>\n      <td>LG전자</td>\n      <td>0</td>\n      <td>3</td>\n      <td>북미</td>\n      <td>46</td>\n      <td>47</td>\n      <td>0</td>\n      <td>44</td>\n    </tr>\n    <tr>\n      <th>8999</th>\n      <td>전남도의회 안전건설소방위원회 차영수 의원(강진1)은 지난 14일 설 명절을 앞두고 ...</td>\n      <td>차영수</td>\n      <td>16</td>\n      <td>18</td>\n      <td>의원</td>\n      <td>20</td>\n      <td>21</td>\n      <td>2</td>\n      <td>72</td>\n    </tr>\n  </tbody>\n</table>\n<p>9000 rows × 9 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "tokenizer.unk_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='bert-base-multilingual-cased', vocab_size=119547, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "tokenizer()"
   ]
  }
 ]
}