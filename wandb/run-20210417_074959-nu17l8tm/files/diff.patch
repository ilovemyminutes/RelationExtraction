diff --git a/config.py b/config.py
index ce2d546..e73f883 100644
--- a/config.py
+++ b/config.py
@@ -64,6 +64,7 @@ class PreProcessType:
 
 @dataclass
 class ModelType:
+    VanillaBert: str = 'VanillaBert'
     Base: str = "BertModel"
     SequenceClf: str = "BertForSequenceClassification"
 
diff --git a/models.py b/models.py
index 779849b..b54c1ed 100644
--- a/models.py
+++ b/models.py
@@ -1,6 +1,8 @@
 import torch
+from torch import nn
 from transformers import BertModel, BertConfig, BertForSequenceClassification
 from config import ModelType, Config, ModelType, PreTrainedType
+from dataset import REDataset, split_train_test_loader
 
 
 def load_model(
@@ -8,6 +10,7 @@ def load_model(
     pretrained_type: str = PreTrainedType.BertMultiLingual,
     num_classes: int = Config.NumClasses,
     load_state_dict: str = None,
+    pooler_idx: int = 0 # get last hidden state from CLS
 ):
     print("Load Model...", end="\t")
     # make BERT configuration
@@ -21,6 +24,8 @@ def load_model(
         model = BertForSequenceClassification.from_pretrained(
             pretrained_type, config=bert_config
         )
+    elif model_type == ModelType.VanillaBert:
+        model = VanillaBert(model_type=model_type, pretrained_type=pretrained_type, num_labels=num_classes, pooler_idx=pooler_idx)
     else:
         raise NotImplementedError()
 
@@ -30,3 +35,47 @@ def load_model(
 
     print("done!")
     return model
+
+class VanillaBert(nn.Module):
+    def __init__(self, model_type:str=ModelType.SequenceClf, pretrained_type: str=PreTrainedType.BertMultiLingual, num_labels: int=Config.NumClasses, pooler_idx: int='cls'):
+        super(VanillaBert, self).__init__()
+        # idx: index of hidden state to extract from output hidden states. It is CLS hidden states for index 0.
+        self.idx = 0 if pooler_idx in ['cls', 0] else pooler_idx 
+        self.backbone = self.load_bert(model_type=model_type, pretrained_type=pretrained_type, num_labels=num_labels)
+        self.layernorm = nn.LayerNorm(768) # 768: output length of BERT, or backbone
+        self.dropout = nn.Dropout()
+        self.relu = nn.ReLU()
+        self.linear = nn.Linear(in_features=768, out_features=num_labels)
+
+    def forward(self, input_ids, token_type_ids, attention_mask):
+        x = self.backbone(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)
+        x = x.last_hidden_state[:, self.idx, :]
+        x = self.layernorm(x)
+        x = self.dropout(x)
+        x = self.relu(x)
+        output = self.linear(x)
+        return output
+
+    @staticmethod
+    def load_bert(model_type, pretrained_type, num_labels):
+        bert_config = BertConfig.from_pretrained(pretrained_type)
+        bert_config.num_labels = num_labels
+
+        if model_type == ModelType.SequenceClf:    
+            model = BertForSequenceClassification.from_pretrained(pretrained_type, config=bert_config)
+            model = model.bert
+
+        elif model_type == ModelType.Base:
+            raise NotImplementedError()
+
+        return model
+
+
+if __name__ == '__main__':
+    dataset = REDataset()
+    train_loader, valid_loader = split_train_test_loader(dataset)
+    model = VanillaBert().cuda()
+    for sents, labels in train_loader:
+        break
+    model(**sents)
+    
\ No newline at end of file
diff --git a/train.py b/train.py
index 0f8c2f8..e0d8b59 100644
--- a/train.py
+++ b/train.py
@@ -22,6 +22,7 @@ def train(
     model_type: str = ModelType.SequenceClf,
     pretrained_type: str = PreTrainedType.BertMultiLingual,
     num_classes: int = Config.NumClasses,
+    pooler_idx: int = 0, 
     load_state_dict: str = None,
     data_root: str = Config.Train,
     tokenization_type: str = PreProcessType.Base,
@@ -57,7 +58,7 @@ def train(
         )
 
     # load model
-    model = load_model(model_type, pretrained_type, num_classes, load_state_dict)
+    model = load_model(model_type, pretrained_type, num_classes, load_state_dict, pooler_idx)
     model.to(device)
     model.train()
 
@@ -89,6 +90,9 @@ def train(
                 outputs = model(**sentences).logits
             elif model_type == ModelType.Base:
                 outputs = model(**sentences).pooler_output
+            else:
+                outputs = model(**sentences)
+
             loss = criterion(outputs, labels)
             total_loss += loss.item()
 
@@ -241,11 +245,12 @@ if __name__ == "__main__":
     LOAD_STATE_DICT = None
     
     parser = argparse.ArgumentParser()
-    parser.add_argument("--model-type", type=str, default=ModelType.Base)
+    parser.add_argument("--model-type", type=str, default=ModelType.VanillaBert)
     parser.add_argument(
         "--pretrained-type", type=str, default=PreTrainedType.BertMultiLingual
     )
-    parser.add_argument("--num-classes", type=str, default=Config.NumClasses)
+    parser.add_argument("--num-classes", type=int, default=Config.NumClasses)
+    parser.add_argument("--poolder-idx", type=int, default=0)
     parser.add_argument("--load-state-dict", type=str, default=LOAD_STATE_DICT)
     parser.add_argument("--data-root", type=str, default=Config.Train)
     parser.add_argument("--tokenization-type", type=str, default=PreProcessType.Base)
diff --git a/wandb/latest-run b/wandb/latest-run
index 53379a8..35b50c1 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20210417_054421-qhvltodw
\ No newline at end of file
+run-20210417_074959-nu17l8tm
\ No newline at end of file
