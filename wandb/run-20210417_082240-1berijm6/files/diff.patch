diff --git a/config.py b/config.py
index ce2d546..e73f883 100644
--- a/config.py
+++ b/config.py
@@ -64,6 +64,7 @@ class TokenizationType:
 
 @dataclass
 class ModelType:
+    VanillaBert: str = 'VanillaBert'
     Base: str = "BertModel"
     SequenceClf: str = "BertForSequenceClassification"
 
diff --git a/inference.py b/inference.py
index 7cb038c..e1bb6d4 100644
--- a/inference.py
+++ b/inference.py
@@ -1,22 +1,18 @@
-from transformers import (
-    AutoTokenizer,
-    BertForSequenceClassification,
-    Trainer,
-    TrainingArguments,
-    BertConfig,
-    BertTokenizer,
-)
-from torch.utils.data import DataLoader
-from load_data import *
-import pandas as pd
-import torch
+import argparse
 import pickle as pickle
 import numpy as np
-import argparse
-from dataset import load_test_dataset
+import pandas as pd
+
+import torch
+from torch.utils.data import DataLoader
+from dataset import REDataset
+from config import Config, TokenizationType
+
 
 
 def inference(model, tokenized_sent, device):
+    dataset = REDataset(root=Config.Test, tokenization_type=tokenization_type, device=device
+        device: str = Config.Device,)
     dataloader = DataLoader(tokenized_sent, batch_size=40, shuffle=False)
     model.eval()
     output_pred = []
diff --git a/models.py b/models.py
index 62a9073..e39904a 100644
--- a/models.py
+++ b/models.py
@@ -25,12 +25,7 @@ def load_model(
             pretrained_type, config=bert_config
         )
     elif model_type == ModelType.VanillaBert:
-        model = VanillaBert(
-            model_type=model_type,
-            pretrained_type=pretrained_type,
-            num_labels=num_classes,
-            pooler_idx=pooler_idx,
-        )
+        model = VanillaBert(model_type=ModelType.SequenceClf, pretrained_type=pretrained_type, num_labels=num_classes, pooler_idx=pooler_idx)
     else:
         raise NotImplementedError()
 
@@ -45,10 +40,10 @@ def load_model(
 class VanillaBert(nn.Module):
     def __init__(
         self,
-        model_type: str = ModelType.SequenceClf,
-        pretrained_type: str = PreTrainedType.BertMultiLingual,
+        model_type: str,
+        pretrained_type: str,
         num_labels: int = Config.NumClasses,
-        pooler_idx: int = "cls",
+        pooler_idx: int = 0,
     ):
         super(VanillaBert, self).__init__()
         # idx: index of hidden state to extract from output hidden states. It is CLS hidden states for index 0.
diff --git a/train.py b/train.py
index 0f8c2f8..dd3435c 100644
--- a/train.py
+++ b/train.py
@@ -19,9 +19,10 @@ warnings.filterwarnings("ignore")
 TOTAL_SAMPLES = 9000
 
 def train(
-    model_type: str = ModelType.SequenceClf,
+    model_type: str = ModelType.VanillaBert,
     pretrained_type: str = PreTrainedType.BertMultiLingual,
     num_classes: int = Config.NumClasses,
+    pooler_idx: int = 0, 
     load_state_dict: str = None,
     data_root: str = Config.Train,
     tokenization_type: str = TokenizationType.Base,
@@ -57,7 +58,7 @@ def train(
         )
 
     # load model
-    model = load_model(model_type, pretrained_type, num_classes, load_state_dict)
+    model = load_model(model_type, pretrained_type, num_classes, load_state_dict, pooler_idx)
     model.to(device)
     model.train()
 
@@ -89,6 +90,9 @@ def train(
                 outputs = model(**sentences).logits
             elif model_type == ModelType.Base:
                 outputs = model(**sentences).pooler_output
+            else:
+                outputs = model(**sentences)
+
             loss = criterion(outputs, labels)
             total_loss += loss.item()
 
@@ -214,6 +218,9 @@ def validate(model, model_type, valid_loader, criterion):
                 outputs = model(**sentences).logits
             elif model_type == ModelType.Base:
                 outputs = model(**sentences).pooler_output
+            else:
+                outputs = model(**sentences)
+                
             loss = criterion(outputs, labels)
             total_loss += loss.item()
 
@@ -241,11 +248,12 @@ if __name__ == "__main__":
     LOAD_STATE_DICT = None
     
     parser = argparse.ArgumentParser()
-    parser.add_argument("--model-type", type=str, default=ModelType.Base)
+    parser.add_argument("--model-type", type=str, default=ModelType.VanillaBert)
     parser.add_argument(
         "--pretrained-type", type=str, default=PreTrainedType.BertMultiLingual
     )
-    parser.add_argument("--num-classes", type=str, default=Config.NumClasses)
+    parser.add_argument("--num-classes", type=int, default=Config.NumClasses)
+    parser.add_argument("--pooler-idx", type=int, default=0)
     parser.add_argument("--load-state-dict", type=str, default=LOAD_STATE_DICT)
     parser.add_argument("--data-root", type=str, default=Config.Train)
     parser.add_argument("--tokenization-type", type=str, default=TokenizationType.Base)
diff --git a/wandb/latest-run b/wandb/latest-run
index 53379a8..62dab4a 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20210417_054421-qhvltodw
\ No newline at end of file
+run-20210417_082240-1berijm6
\ No newline at end of file
