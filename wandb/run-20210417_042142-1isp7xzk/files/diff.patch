diff --git a/config.py b/config.py
index 522a3d0..befa2bd 100644
--- a/config.py
+++ b/config.py
@@ -6,7 +6,6 @@ import torch
 TRAIN = "./input/data/train/train.tsv"
 TEST = "./input/data/test/test.tsv"
 LABEL = "./input/data/label_type.pkl"
-SAVEPATH = "./saved_models"
 LOGS = "./logs"
 
 DOT = "."
@@ -20,12 +19,11 @@ class Config:
 
     Train: str = TRAIN if os.path.isfile(TRAIN) else DOT + TRAIN
     Test: str = TEST if os.path.isfile(TEST) else DOT + TEST
-    ValidSize: float = 0.2
+    ValidSize: float = 0.1
     Label: str = LABEL if os.path.isfile(LABEL) else DOT + LABEL
-    SavePath: str = SAVEPATH if os.path.isfile(SAVEPATH) else DOT + SAVEPATH
     Logs: str = LOGS if os.path.isfile(LOGS) else DOT + LOGS
     NumClasses: int = 42
-    Epochs: int = 20
+    Epochs: int = 10
 
     Batch8:int = 8
     Batch16: int = 16
diff --git a/train.py b/train.py
index 32c2b91..0ad71fd 100644
--- a/train.py
+++ b/train.py
@@ -60,9 +60,16 @@ def train(
     optimizer = get_optimizer(model=model, type=optim_type, lr=lr)
     if lr_scheduler is not None:
         scheduler = get_scheduler(type=lr_scheduler, optimizer=optimizer)
+    
+    # make checkpoint directory to save model during train
+    checkpoint_dir = f"{model_type}_{pretrained_type}_{TIMESTAMP}"
+    if checkpoint_dir not in os.listdir(save_path):
+        os.mkdir(os.path.join(save_path, checkpoint_dir))
+    save_path = os.path.join(save_path, checkpoint_dir)
 
     # train phase
     best_acc = 0
+    best_loss = 999
 
     for epoch in range(epochs):
         print(f"Epoch: {epoch}")
@@ -103,17 +110,17 @@ def train(
                     {
                         f"First EP Train ACC": train_eval["accuracy"],
                         f"First EP Train F1": train_eval["f1"],
-                        f"First EP Train PRC": train_eval["precision"],
-                        f"First EP Train REC": train_eval["recall"],
+                        # f"First EP Train PRC": train_eval["precision"],
+                        # f"First EP Train REC": train_eval["recall"],
                         f"First EP Train Loss": train_loss,
                     }
-                )
+                ) 
 
             if idx != 0 and idx % VALID_CYCLE == 0:
                 valid_eval, valid_loss = validate(
                     model=model, valid_loader=valid_loader, criterion=criterion
                 )
-                verbose(phase="Valid", eval=train_eval, loss=train_loss)
+                verbose(phase="Valid", eval=valid_eval, loss=valid_loss)
                 verbose(phase="Train", eval=train_eval, loss=train_loss)
 
                 if epoch == 0:
@@ -121,8 +128,8 @@ def train(
                         {
                             f"First EP Valid ACC": train_eval["accuracy"],
                             f"First EP Valid F1": train_eval["f1"],
-                            f"First EP Valid PRC": train_eval["precision"],
-                            f"First EP Valid REC": train_eval["recall"],
+                            # f"First EP Valid PRC": train_eval["precision"],
+                            # f"First EP Valid REC": train_eval["recall"],
                             f"First EP Valid Loss": train_loss,
                         }
                     )
@@ -134,21 +141,31 @@ def train(
                 "Valid ACC": valid_eval["accuracy"],
                 "Train F1": train_eval["f1"],
                 "Valid F1": valid_eval["f1"],
-                "Train PRC": train_eval["precision"],
-                "Valid PRC": valid_eval["precision"],
-                "Train REC": train_eval["recall"],
-                "Valid REC": valid_eval["recall"],
+                # "Train PRC": train_eval["precision"],
+                # "Valid PRC": valid_eval["precision"],
+                # "Train REC": train_eval["recall"],
+                # "Valid REC": valid_eval["recall"],
                 "Train Loss": train_loss,
                 "Valid Loss": valid_loss,
             }
         )
 
-        if save_path and valid_eval["accuracy"] >= best_acc:
-            name = f"{model_type}_{pretrained_type}_ep({epoch:0>2d})acc({valid_eval['accuracy']:.4f})id({TIMESTAMP}).pth"
+        # Checkpoint Condition
+            # 1. Better Accuracy
+            # 2. Better Loss if accuracy is the same as before
+
+        if save_path and valid_eval["accuracy"] > best_acc:
+            name = f"{model_type}_{pretrained_type}_ep({epoch:0>2d})acc({valid_eval['accuracy']:.4f})loss({valid_loss})id({TIMESTAMP}).pth"
             best_acc = valid_eval["accuracy"]
+            best_loss = valid_loss
+            torch.save(model.state_dict(), os.path.join(save_path, name))
+            print(f'Model saved: {os.path.join(save_path, name)}')
+
+        elif save_path and valid_eval["accuracy"] == best_acc and best_loss > valid_loss:
+            name = f"{model_type}_{pretrained_type}_ep({epoch:0>2d})acc({valid_eval['accuracy']:.4f})loss({valid_loss})id({TIMESTAMP}).pth"
+            best_loss = valid_loss
             torch.save(model.state_dict(), os.path.join(save_path, name))
             print(f'Model saved: {os.path.join(save_path, name)}')
-            
 
 
 def validate(model, valid_loader, criterion):
@@ -193,9 +210,10 @@ def verbose(phase: str, eval: dict, loss: float):
 
 if __name__ == "__main__":
     LOAD_STATE_DICT = None
+    TIMESTAMP = get_timestamp()
 
     parser = argparse.ArgumentParser()
-    parser.add_argument("--model-type", type=str, default=ModelType.SequenceClf)
+    parser.add_argument("--model-type", type=str, default=ModelType.Base)
     parser.add_argument(
         "--pretrained-type", type=str, default=PreTrainedType.BertMultiLingual
     )
@@ -217,7 +235,6 @@ if __name__ == "__main__":
 
     # register logs to wandb
     args = parser.parse_args()
-    TIMESTAMP = get_timestamp()
     name = args.model_type + "_" + args.pretrained_type + "_" + TIMESTAMP
     run = wandb.init(project="pstage-klue", name=name, reinit=True)
     wandb.config.update(args)
diff --git a/wandb/latest-run b/wandb/latest-run
index 25c39ef..ac16c2a 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20210416_190945-14py9lif
\ No newline at end of file
+run-20210417_042142-1isp7xzk
\ No newline at end of file
