diff --git a/dataset.py b/dataset.py
index 8dedae0..b73cf33 100644
--- a/dataset.py
+++ b/dataset.py
@@ -1,4 +1,5 @@
 import random
+from tqdm import tqdm
 from typing import Tuple, Dict
 import pandas as pd
 import torch
@@ -9,7 +10,10 @@ from transformers.utils import logging
 logger = logging.get_logger(__name__)
 from config import Config, PreProcessType
 from utils import load_pickle
-from tokenization import load_tokenizer, tokenize
+from tokenization import (
+    load_tokenizer,
+    tokenize
+)
 from preprocessing import preprocess_text
 
 
@@ -26,6 +30,54 @@ COLUMNS = [
 ]
 
 
+class REDataset_fix(Dataset):
+    def __init__(
+        self,
+        root: str = Config.Train,
+        preprocess_type: str = PreProcessType.EM,
+        device: str = Config.Device,
+    ):
+        self.data = self._load_data(root, preprocess_type=preprocess_type)
+        self.labels = self.data["label"].tolist()
+        self.tokenizer = load_tokenizer(type=preprocess_type)
+        self.inputs = tokenize(
+            self.data, self.tokenizer, preprocess_type
+        )
+        self.device = device
+
+    def __getitem__(self, idx) -> Tuple[Dict[str, torch.Tensor], torch.Tensor]:
+        """모델에 입력할 데이터 생성시, device 상황에 따라 CPU 또는 GPU에 할당한 채로 return"""
+        sentence = {
+            key: torch.as_tensor(val[idx]).to(self.device)  # device 할당
+            for key, val in self.inputs.items()
+        }
+        label = torch.as_tensor(self.labels[idx]).to(self.device)  # device 할당
+        return sentence, label
+
+    def __len__(self):
+        return len(self.labels)
+
+    @staticmethod
+    def _load_data(root: str, preprocess_type: str) -> pd.DataFrame:
+
+        # load raw data
+        print("Load raw data...", end="\t")
+        enc = LabelEncoder()
+        raw = pd.read_csv(root, sep="\t", header=None)
+        raw.columns = COLUMNS
+        raw = raw.drop("id", axis=1)
+        raw["label"] = raw["label"].apply(lambda x: enc.transform(x))
+
+        # preprocessing
+        print(f"preprocessing for '{preprocess_type}'...", end="\t")
+        data = preprocess_text(raw, method=preprocess_type)
+        print("done!")
+
+        return data
+
+    
+
+
 class REDataset(Dataset):
     def __init__(
         self,
@@ -48,7 +100,7 @@ class REDataset(Dataset):
         for key in sentence.keys():
             sentence[key] = sentence[key].to(self.device)
 
-        label = torch.as_tensor(self.labels[idx]).to(self.device)  
+        label = torch.as_tensor(self.labels[idx]).to(self.device)
         return sentence, label
 
     def __len__(self):
@@ -177,5 +229,5 @@ class LabelEncoder:
 # just for debug
 if __name__ == "__main__":
     config_dataset = dict(root=Config.Train, preprocess_type=PreProcessType.EM)
-    dataset = REDataset(**config_dataset)
+    dataset = REDataset_fix(**config_dataset)
     # train_loader, valid_loader = split_train_test_loader(dataset)
diff --git a/notebooks/Data Augmentation.ipynb b/notebooks/Data Augmentation.ipynb
index 553ae37..8f53a29 100644
--- a/notebooks/Data Augmentation.ipynb	
+++ b/notebooks/Data Augmentation.ipynb	
@@ -23,7 +23,7 @@
  "cells": [
   {
    "cell_type": "code",
-   "execution_count": 1,
+   "execution_count": 42,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -48,7 +48,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 2,
+   "execution_count": 43,
    "metadata": {},
    "outputs": [
     {
@@ -77,7 +77,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 30,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -107,50 +107,68 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 27,
+   "execution_count": 28,
+   "metadata": {},
+   "outputs": [
+    {
+     "output_type": "stream",
+     "name": "stdout",
+     "text": [
+      "CPU times: user 3h 40min 16s, sys: 11min 20s, total: 3h 51min 37s\nWall time: 2h 52min 57s\n"
+     ]
+    }
+   ],
+   "source": [
+    "%%time\n",
+    "data['relation_state'] = data['relation_state'].apply(lambda x: spacing(x))"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 35,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "data['relation_state'] = data.apply(lambda x: x['relation_state'].replace(TEMP_E1, x['e1']), axis=1)\n",
+    "data['relation_state'] = data.apply(lambda x: x['relation_state'].replace(TEMP_E2, x['e2']), axis=1)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 37,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "data.to_csv('external_preprocessed_v1.csv')"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 39,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "sample = data.iloc[0, :]"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 41,
    "metadata": {},
    "outputs": [
     {
      "output_type": "execute_result",
      "data": {
       "text/plain": [
-       "                        id                                     relation_state  \\\n",
-       "0         from train_csv 5  카터는영희와이스라엘을조정하여,캠프데이비드에서철수대통령과메나헴베긴수상과함께중동평화를위...   \n",
-       "1         from train_csv 8  카터는이집트와영희을조정하여,캠프데이비드에서안와르사다트대통령과메나헴베긴수상과함께중동평...   \n",
-       "2         from train_csv 9  카터는영희와이스라엘을조정하여,캠프데이비드에서안와르사다트대통령과메나헴베긴수상과함께중동...   \n",
-       "3        from train_csv 11  카터는이집트와영희을조정하여,캠프데이비드에서안와르사다트대통령과철수수상과함께중동평화를위...   \n",
-       "4        from train_csv 65  선거전까지각축전을벌인유력한후보는15대대통령선거에나와김대중에게패배한제1야당철수의대표'...   \n",
-       "...                    ...                                                ...   \n",
-       "229606  from dev_csv 99215                           2.\"영희\"(철수작사/계동균작곡)–03:24   \n",
-       "229607  from dev_csv 99216                            9.\"영희\"(철수작사/철수작곡)-02:06   \n",
-       "229608  from dev_csv 99217  2009년,블런트는장마크발레가감독을맡고철수가각본을쓴《영희》에서빅토리아여왕역으로출연하였다.   \n",
-       "229609  from dev_csv 99218  2008년저드애퍼토가제작하고세스로건과철수가각본을맡은코미디영화《영희》와무술영화《겟썸》...   \n",
-       "229610  from dev_csv 99219  영화《라이터를켜라》,《영희》의철수,《그해여름》의김은희작가가공동으로집필하고,조현탁PD...   \n",
-       "\n",
-       "               e1  e1_start  e1_end          e2  e2_start  e2_end       label  \n",
-       "0         안와르 사다트        38      44         이집트         5       7  인물:출신성분/국적  \n",
-       "1       캠프데이비드 협정        78      86        이스라엘        11      14      단체:구성원  \n",
-       "2       캠프데이비드 협정        78      86         이집트         5       7      단체:구성원  \n",
-       "3          메나헴 베긴        52      57        이스라엘        11      14  인물:출신성분/국적  \n",
-       "4            한나라당        63      66         이회창        75      77      단체:구성원  \n",
-       "...           ...       ...     ...         ...       ...     ...         ...  \n",
-       "229606        박건호        11      13        사슴여인         4       7       인물:제작  \n",
-       "229607         장덕        14      15     안녕히 계세요         4      10       인물:제작  \n",
-       "229608    줄리언 펠로스        27      33      영 빅토리아        43      48       인물:제작  \n",
-       "229609    에번 골드버그        26      32  파인애플 익스프레스        50      59       인물:제작  \n",
-       "229610        장항준        23      25      귀신이 산다        14      19       인물:제작  \n",
-       "\n",
-       "[229611 rows x 9 columns]"
-      ],
-      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>relation_state</th>\n      <th>e1</th>\n      <th>e1_start</th>\n      <th>e1_end</th>\n      <th>e2</th>\n      <th>e2_start</th>\n      <th>e2_end</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>from train_csv 5</td>\n      <td>카터는영희와이스라엘을조정하여,캠프데이비드에서철수대통령과메나헴베긴수상과함께중동평화를위...</td>\n      <td>안와르 사다트</td>\n      <td>38</td>\n      <td>44</td>\n      <td>이집트</td>\n      <td>5</td>\n      <td>7</td>\n      <td>인물:출신성분/국적</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>from train_csv 8</td>\n      <td>카터는이집트와영희을조정하여,캠프데이비드에서안와르사다트대통령과메나헴베긴수상과함께중동평...</td>\n      <td>캠프데이비드 협정</td>\n      <td>78</td>\n      <td>86</td>\n      <td>이스라엘</td>\n      <td>11</td>\n      <td>14</td>\n      <td>단체:구성원</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>from train_csv 9</td>\n      <td>카터는영희와이스라엘을조정하여,캠프데이비드에서안와르사다트대통령과메나헴베긴수상과함께중동...</td>\n      <td>캠프데이비드 협정</td>\n      <td>78</td>\n      <td>86</td>\n      <td>이집트</td>\n      <td>5</td>\n      <td>7</td>\n      <td>단체:구성원</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>from train_csv 11</td>\n      <td>카터는이집트와영희을조정하여,캠프데이비드에서안와르사다트대통령과철수수상과함께중동평화를위...</td>\n      <td>메나헴 베긴</td>\n      <td>52</td>\n      <td>57</td>\n      <td>이스라엘</td>\n      <td>11</td>\n      <td>14</td>\n      <td>인물:출신성분/국적</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>from train_csv 65</td>\n      <td>선거전까지각축전을벌인유력한후보는15대대통령선거에나와김대중에게패배한제1야당철수의대표'...</td>\n      <td>한나라당</td>\n      <td>63</td>\n      <td>66</td>\n      <td>이회창</td>\n      <td>75</td>\n      <td>77</td>\n      <td>단체:구성원</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>229606</th>\n      <td>from dev_csv 99215</td>\n      <td>2.\"영희\"(철수작사/계동균작곡)–03:24</td>\n      <td>박건호</td>\n      <td>11</td>\n      <td>13</td>\n      <td>사슴여인</td>\n      <td>4</td>\n      <td>7</td>\n      <td>인물:제작</td>\n    </tr>\n    <tr>\n      <th>229607</th>\n      <td>from dev_csv 99216</td>\n      <td>9.\"영희\"(철수작사/철수작곡)-02:06</td>\n      <td>장덕</td>\n      <td>14</td>\n      <td>15</td>\n      <td>안녕히 계세요</td>\n      <td>4</td>\n      <td>10</td>\n      <td>인물:제작</td>\n    </tr>\n    <tr>\n      <th>229608</th>\n      <td>from dev_csv 99217</td>\n      <td>2009년,블런트는장마크발레가감독을맡고철수가각본을쓴《영희》에서빅토리아여왕역으로출연하였다.</td>\n      <td>줄리언 펠로스</td>\n      <td>27</td>\n      <td>33</td>\n      <td>영 빅토리아</td>\n      <td>43</td>\n      <td>48</td>\n      <td>인물:제작</td>\n    </tr>\n    <tr>\n      <th>229609</th>\n      <td>from dev_csv 99218</td>\n      <td>2008년저드애퍼토가제작하고세스로건과철수가각본을맡은코미디영화《영희》와무술영화《겟썸》...</td>\n      <td>에번 골드버그</td>\n      <td>26</td>\n      <td>32</td>\n      <td>파인애플 익스프레스</td>\n      <td>50</td>\n      <td>59</td>\n      <td>인물:제작</td>\n    </tr>\n    <tr>\n      <th>229610</th>\n      <td>from dev_csv 99219</td>\n      <td>영화《라이터를켜라》,《영희》의철수,《그해여름》의김은희작가가공동으로집필하고,조현탁PD...</td>\n      <td>장항준</td>\n      <td>23</td>\n      <td>25</td>\n      <td>귀신이 산다</td>\n      <td>14</td>\n      <td>19</td>\n      <td>인물:제작</td>\n    </tr>\n  </tbody>\n</table>\n<p>229611 rows × 9 columns</p>\n</div>"
+       "31"
+      ]
      },
      "metadata": {},
-     "execution_count": 27
+     "execution_count": 41
     }
    ],
    "source": [
-    "%%time\n",
-    "data['relation_state'] = data['relation_state'].apply(lambda x: spacing(x))"
+    "sample['relation_state'].index(sample['e1'])"
    ]
   }
  ]
diff --git a/notebooks/Debug - Model Inference.ipynb b/notebooks/Debug - Model Inference.ipynb
deleted file mode 100644
index 632741e..0000000
--- a/notebooks/Debug - Model Inference.ipynb	
+++ /dev/null
@@ -1,551 +0,0 @@
-{
- "metadata": {
-  "language_info": {
-   "codemirror_mode": {
-    "name": "ipython",
-    "version": 3
-   },
-   "file_extension": ".py",
-   "mimetype": "text/x-python",
-   "name": "python",
-   "nbconvert_exporter": "python",
-   "pygments_lexer": "ipython3",
-   "version": "3.7.7-final"
-  },
-  "orig_nbformat": 2,
-  "kernelspec": {
-   "name": "python377jvsc74a57bd098b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f",
-   "display_name": "Python 3.7.7 64-bit ('base': conda)"
-  }
- },
- "nbformat": 4,
- "nbformat_minor": 2,
- "cells": [
-  {
-   "cell_type": "code",
-   "execution_count": 30,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "import torch\n",
-    "from torch.nn import functional as F\n",
-    "from torch import optim\n",
-    "from torch import nn\n",
-    "from torch.utils.data import DataLoader\n",
-    "import sys\n",
-    "sys.path.insert(0, '../')\n",
-    "\n",
-    "from config import *\n",
-    "from transformers import BertModel, BertConfig, BertForSequenceClassification\n",
-    "from models import load_model, VanillaBert\n",
-    "from dataset import REDataset, split_train_test_loader\n",
-    "from utils import set_seed\n",
-    "from criterions import *\n",
-    "from optimizers import *"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 31,
-   "metadata": {},
-   "outputs": [
-    {
-     "output_type": "stream",
-     "name": "stdout",
-     "text": [
-      "Load raw data...\tpreprocessing for 'Base'...\tdone!\n",
-      "Load Tokenizer...\tdone!\n",
-      "Apply Tokenization...\tdone!\n"
-     ]
-    }
-   ],
-   "source": [
-    "data = REDataset()"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 34,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "train_loader, valid_loader = split_train_test_loader(data, 0.2)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 36,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "for sents, labels in train_loader:\n",
-    "    break"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 38,
-   "metadata": {},
-   "outputs": [
-    {
-     "output_type": "execute_result",
-     "data": {
-      "text/plain": [
-       "tensor([ 0,  0,  0,  4, 25,  9,  0, 22,  0,  0,  2,  3,  4,  2,  0, 10,  2,  0,\n",
-       "         0,  0,  3,  0,  2,  6, 10,  7,  0,  0,  2, 10,  0,  6],\n",
-       "       device='cuda:0')"
-      ]
-     },
-     "metadata": {},
-     "execution_count": 38
-    }
-   ],
-   "source": [
-    "labels"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 14,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "LOAD = \"../saved_models/VanillaBert_bert-base-multilingual-cased_20210418164452/VanillaBert_bert-base-multilingual-cased_ep(02)acc(0.4878)loss(0.0048)id(20210418164452).pth\"\n"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 19,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "model_type = ModelType.VanillaBert\n",
-    "pretrained_type = PreTrainedType.MultiLingual\n",
-    "num_classes=Config.NumClasses\n",
-    "pooler_idx=0\n",
-    "load_state_dict=None\n",
-    "data_root=Config.Train\n",
-    "preprocess_type=PreProcessType.ES\n",
-    "epochs=Config.Epochs\n",
-    "valid_size=Config.ValidSize\n",
-    "train_batch_size=Config.Batch8\n",
-    "valid_batch_size=512\n",
-    "optim_type=Optimizer.Adam\n",
-    "loss_type=Loss.CE\n",
-    "lr=Config.LRSlower\n",
-    "lr_scheduler=Optimizer.CosineScheduler\n",
-    "device = Config.Device"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 24,
-   "metadata": {},
-   "outputs": [
-    {
-     "output_type": "stream",
-     "name": "stdout",
-     "text": [
-      "Load raw data...\tpreprocessing for 'EntitySeparation'...\tdone!\n",
-      "Load Tokenizer...\tdone!\n",
-      "Apply Tokenization...\tdone!\n",
-      "Load Model...\tSome weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
-      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
-      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
-      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
-      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
-      "done!\n"
-     ]
-    },
-    {
-     "output_type": "execute_result",
-     "data": {
-      "text/plain": [
-       "VanillaBert(\n",
-       "  (backbone): BertModel(\n",
-       "    (embeddings): BertEmbeddings(\n",
-       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
-       "      (position_embeddings): Embedding(512, 768)\n",
-       "      (token_type_embeddings): Embedding(2, 768)\n",
-       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "      (dropout): Dropout(p=0.1, inplace=False)\n",
-       "    )\n",
-       "    (encoder): BertEncoder(\n",
-       "      (layer): ModuleList(\n",
-       "        (0): BertLayer(\n",
-       "          (attention): BertAttention(\n",
-       "            (self): BertSelfAttention(\n",
-       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "            (output): BertSelfOutput(\n",
-       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "          )\n",
-       "          (intermediate): BertIntermediate(\n",
-       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "          )\n",
-       "          (output): BertOutput(\n",
-       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "            (dropout): Dropout(p=0.1, inplace=False)\n",
-       "          )\n",
-       "        )\n",
-       "        (1): BertLayer(\n",
-       "          (attention): BertAttention(\n",
-       "            (self): BertSelfAttention(\n",
-       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "            (output): BertSelfOutput(\n",
-       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "          )\n",
-       "          (intermediate): BertIntermediate(\n",
-       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "          )\n",
-       "          (output): BertOutput(\n",
-       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "            (dropout): Dropout(p=0.1, inplace=False)\n",
-       "          )\n",
-       "        )\n",
-       "        (2): BertLayer(\n",
-       "          (attention): BertAttention(\n",
-       "            (self): BertSelfAttention(\n",
-       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "            (output): BertSelfOutput(\n",
-       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "          )\n",
-       "          (intermediate): BertIntermediate(\n",
-       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "          )\n",
-       "          (output): BertOutput(\n",
-       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "            (dropout): Dropout(p=0.1, inplace=False)\n",
-       "          )\n",
-       "        )\n",
-       "        (3): BertLayer(\n",
-       "          (attention): BertAttention(\n",
-       "            (self): BertSelfAttention(\n",
-       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "            (output): BertSelfOutput(\n",
-       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "          )\n",
-       "          (intermediate): BertIntermediate(\n",
-       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "          )\n",
-       "          (output): BertOutput(\n",
-       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "            (dropout): Dropout(p=0.1, inplace=False)\n",
-       "          )\n",
-       "        )\n",
-       "        (4): BertLayer(\n",
-       "          (attention): BertAttention(\n",
-       "            (self): BertSelfAttention(\n",
-       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "            (output): BertSelfOutput(\n",
-       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "          )\n",
-       "          (intermediate): BertIntermediate(\n",
-       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "          )\n",
-       "          (output): BertOutput(\n",
-       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "            (dropout): Dropout(p=0.1, inplace=False)\n",
-       "          )\n",
-       "        )\n",
-       "        (5): BertLayer(\n",
-       "          (attention): BertAttention(\n",
-       "            (self): BertSelfAttention(\n",
-       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "            (output): BertSelfOutput(\n",
-       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "          )\n",
-       "          (intermediate): BertIntermediate(\n",
-       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "          )\n",
-       "          (output): BertOutput(\n",
-       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "            (dropout): Dropout(p=0.1, inplace=False)\n",
-       "          )\n",
-       "        )\n",
-       "        (6): BertLayer(\n",
-       "          (attention): BertAttention(\n",
-       "            (self): BertSelfAttention(\n",
-       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "            (output): BertSelfOutput(\n",
-       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "          )\n",
-       "          (intermediate): BertIntermediate(\n",
-       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "          )\n",
-       "          (output): BertOutput(\n",
-       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "            (dropout): Dropout(p=0.1, inplace=False)\n",
-       "          )\n",
-       "        )\n",
-       "        (7): BertLayer(\n",
-       "          (attention): BertAttention(\n",
-       "            (self): BertSelfAttention(\n",
-       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "            (output): BertSelfOutput(\n",
-       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "          )\n",
-       "          (intermediate): BertIntermediate(\n",
-       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "          )\n",
-       "          (output): BertOutput(\n",
-       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "            (dropout): Dropout(p=0.1, inplace=False)\n",
-       "          )\n",
-       "        )\n",
-       "        (8): BertLayer(\n",
-       "          (attention): BertAttention(\n",
-       "            (self): BertSelfAttention(\n",
-       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "            (output): BertSelfOutput(\n",
-       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "          )\n",
-       "          (intermediate): BertIntermediate(\n",
-       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "          )\n",
-       "          (output): BertOutput(\n",
-       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "            (dropout): Dropout(p=0.1, inplace=False)\n",
-       "          )\n",
-       "        )\n",
-       "        (9): BertLayer(\n",
-       "          (attention): BertAttention(\n",
-       "            (self): BertSelfAttention(\n",
-       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "            (output): BertSelfOutput(\n",
-       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "          )\n",
-       "          (intermediate): BertIntermediate(\n",
-       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "          )\n",
-       "          (output): BertOutput(\n",
-       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "            (dropout): Dropout(p=0.1, inplace=False)\n",
-       "          )\n",
-       "        )\n",
-       "        (10): BertLayer(\n",
-       "          (attention): BertAttention(\n",
-       "            (self): BertSelfAttention(\n",
-       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "            (output): BertSelfOutput(\n",
-       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "          )\n",
-       "          (intermediate): BertIntermediate(\n",
-       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "          )\n",
-       "          (output): BertOutput(\n",
-       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "            (dropout): Dropout(p=0.1, inplace=False)\n",
-       "          )\n",
-       "        )\n",
-       "        (11): BertLayer(\n",
-       "          (attention): BertAttention(\n",
-       "            (self): BertSelfAttention(\n",
-       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "            (output): BertSelfOutput(\n",
-       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "          )\n",
-       "          (intermediate): BertIntermediate(\n",
-       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "          )\n",
-       "          (output): BertOutput(\n",
-       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "            (dropout): Dropout(p=0.1, inplace=False)\n",
-       "          )\n",
-       "        )\n",
-       "      )\n",
-       "    )\n",
-       "    (pooler): BertPooler(\n",
-       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
-       "      (activation): Tanh()\n",
-       "    )\n",
-       "  )\n",
-       "  (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "  (dropout): Dropout(p=0.5, inplace=False)\n",
-       "  (relu): ReLU()\n",
-       "  (linear): Linear(in_features=768, out_features=42, bias=True)\n",
-       ")"
-      ]
-     },
-     "metadata": {},
-     "execution_count": 24
-    }
-   ],
-   "source": [
-    "dataset = REDataset(root=Config.Train, preprocess_type=preprocess_type, device=device)\n",
-    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, drop_last=True)\n",
-    "\n",
-    "# load model\n",
-    "model = load_model(model_type, pretrained_type, num_classes, load_state_dict, pooler_idx)\n",
-    "model.to(device)\n",
-    "model.train()"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 25,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "# load criterion, optimizer, scheduler\n",
-    "criterion = get_criterion(type=loss_type)\n",
-    "optimizer = get_optimizer(model=model, type=optim_type, lr=lr)\n",
-    "if lr_scheduler is not None:\n",
-    "    scheduler = get_scheduler(type=lr_scheduler, optimizer=optimizer)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "# make checkpoint directory to save model during train\n",
-    "checkpoint_dir = f\"{model_type}_{pretrained_type}_{TIMESTAMP}\"\n",
-    "if checkpoint_dir not in os.listdir(save_path):\n",
-    "    os.mkdir(os.path.join(save_path, checkpoint_dir))\n",
-    "save_path = os.path.join(save_path, checkpoint_dir)\n",
-    "\n",
-    "# train phase\n",
-    "best_acc = 0\n",
-    "best_loss = 999\n",
-    "\n",
-    "for epoch in range(epochs):\n",
-    "    print(f\"Epoch: {epoch}\")\n",
-    "\n",
-    "    pred_list = []\n",
-    "    true_list = []\n",
-    "    total_loss = 0\n",
-    "\n",
-    "    for idx, (sentences, labels) in tqdm(enumerate(train_loader), desc=\"[Train]\"):\n",
-    "        if model_type == ModelType.SequenceClf:\n",
-    "            outputs = model(**sentences).logits\n",
-    "        elif model_type == ModelType.Base:\n",
-    "            outputs = model(**sentences).pooler_output\n",
-    "        else:\n",
-    "            outputs = model(**sentences)\n",
-    "\n",
-    "        loss = criterion(outputs, labels)\n",
-    "        total_loss += loss.item()\n",
-    "\n",
-    "        # backpropagation\n",
-    "        optimizer.zero_grad()\n",
-    "        loss.backward()\n",
-    "        optimizer.step()\n",
-    "        \n",
-    "        if lr_scheduler is not None:\n",
-    "            scheduler.step()\n",
-    "\n",
-    "        # stack preds for evaluate\n",
-    "        _, preds = torch.max(outputs, dim=1)\n",
-    "        preds = preds.data.cpu().numpy()\n",
-    "        labels = labels.data.cpu().numpy()\n",
-    "\n",
-    "        pred_list.append(preds)\n",
-    "        true_list.append(labels)\n",
-    "\n",
-    "        pred_arr = np.hstack(pred_list)\n",
-    "        true_arr = np.hstack(true_list)\n",
-    "\n",
-    "        # evaluation phase\n",
-    "        train_eval = evaluate(y_true=true_arr, y_pred=pred_arr)  # ACC, F1, PRC, REC\n",
-    "        train_loss = total_loss / len(true_arr)"
-   ]
-  }
- ]
-}
\ No newline at end of file
diff --git a/notebooks/Entity Marker.ipynb b/notebooks/Entity Marker.ipynb
index add87de..6e72e4b 100644
--- a/notebooks/Entity Marker.ipynb	
+++ b/notebooks/Entity Marker.ipynb	
@@ -23,7 +23,7 @@
  "cells": [
   {
    "cell_type": "code",
-   "execution_count": 2,
+   "execution_count": 1,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -31,14 +31,15 @@
     "import sys\n",
     "import pandas as pd\n",
     "import torch\n",
-    "from torch.utils.data import Dataset\n",
+    "from torch.utils.data import Dataset, DataLoader\n",
     "from transformers import BertTokenizer\n",
     "sys.path.insert(0, '../')\n",
     "from typing import Tuple, Dict\n",
     "from config import Config, PreTrainedType, PreProcessType\n",
-    "from dataset import load_data, REDataset\n",
-    "from tokenization import load_tokenizer, tokenize\n",
-    "from tokenization import SpecialToken as ST\n",
+    "from dataset import load_data, REDataset, REDataset_fix\n",
+    "from tokenization import load_tokenizer, tokenize, make_additional_token_type_ids, find_sep_intervals, find_entity_intervals\n",
+    "from tokenization import SpecialToken\n",
+    "from models import load_model\n",
     "from preprocessing import preprocess_text\n",
     "\n",
     "tokenizer = BertTokenizer.from_pretrained(PreTrainedType.MultiLingual)"
@@ -46,109 +47,24 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 7,
-   "metadata": {},
-   "outputs": [
-    {
-     "output_type": "stream",
-     "name": "stdout",
-     "text": [
-      "Load Tokenizer...\tdone!\n"
-     ]
-    }
-   ],
-   "source": [
-    "tokenizer = load_tokenizer(PreProcessType.EM)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 10,
-   "metadata": {},
-   "outputs": [
-    {
-     "output_type": "execute_result",
-     "data": {
-      "text/plain": [
-       "{'input_ids': tensor([[   101,  50266,  11489,   9405,  24974,  24683,   9477,  90578,   9625,\n",
-       "         119376,  12692,  45725, 119549,   9651,  99183, 119550,   9637,   9376,\n",
-       "          42771,  70186, 119547,   9167,  15001,  11261,  41605, 119548,    113,\n",
-       "          12001,  57836,    114,   9590,   9706,  28396,    113,  13796,  19986,\n",
-       "            114,   8843,  22634,    117,   9638,   9376,  42771,  22879,   9651,\n",
-       "          99183,  10459,   9684,  46520,  11513,   9641, 119298,  11018,   9251,\n",
-       "          11261,   9405,  24974, 118800,  27792,  16139,    119,    102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
-       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
-       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
-       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
-       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
-      ]
-     },
-     "metadata": {},
-     "execution_count": 10
-    }
-   ],
-   "source": [
-    "sentence = '영국에서 사용되는 스포츠 유틸리티 [E2]자동차[/E2]의 브랜드로는 [E1]랜드로버[/E1](Land Rover)와 지프(Jeep)가 있으며, 이 브랜드들은 자동차의 종류를 일컫는 말로 사용되기도 한다.'\n",
-    "tokenize(sentence, tokenizer, PreProcessType.EM)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 5,
+   "execution_count": 24,
    "metadata": {},
    "outputs": [
     {
      "output_type": "stream",
      "name": "stdout",
      "text": [
-      "Load raw data...\tpreprocessing for 'EntityMarkerSeparationPositionEmbedding'...\tdone!\n",
-      "Load Tokenizer...\tdone!\n"
+      "Load raw data...\tpreprocessing for 'ESPositionEmbedding'...\tdone!\n",
+      "Load Tokenizer for ESPositionEmbedding...\tdone!\n",
+      "Update token_type_ids: 9000it [00:00, 33121.82it/s]\n"
      ]
     }
    ],
    "source": [
-    "data = REDataset(preprocess_type=PreProcessType.EMSP)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 2,
-   "metadata": {},
-   "outputs": [
-    {
-     "output_type": "execute_result",
-     "data": {
-      "text/plain": [
-       "                                      relation_state        e1  e1_start  \\\n",
-       "0  영국에서 사용되는 스포츠 유틸리티 자동차의 브랜드로는 랜드로버(Land Rover)...      랜드로버        30   \n",
-       "1  선거에서 민주당은 해산 전 의석인 230석에 한참 못 미치는 57석(지역구 27석,...       민주당         5   \n",
-       "2  유럽 축구 연맹(UEFA) 집행위원회는 2014년 1월 24일에 열린 회의를 통해 ...  유럽 축구 연맹         0   \n",
-       "\n",
-       "   e1_end    e2  e2_start  e2_end  label  \n",
-       "0      33   자동차        19      21     17  \n",
-       "1       7   27석        42      44      0  \n",
-       "2       7  UEFA         9      12      6  "
-      ],
-      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>relation_state</th>\n      <th>e1</th>\n      <th>e1_start</th>\n      <th>e1_end</th>\n      <th>e2</th>\n      <th>e2_start</th>\n      <th>e2_end</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>영국에서 사용되는 스포츠 유틸리티 자동차의 브랜드로는 랜드로버(Land Rover)...</td>\n      <td>랜드로버</td>\n      <td>30</td>\n      <td>33</td>\n      <td>자동차</td>\n      <td>19</td>\n      <td>21</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>선거에서 민주당은 해산 전 의석인 230석에 한참 못 미치는 57석(지역구 27석,...</td>\n      <td>민주당</td>\n      <td>5</td>\n      <td>7</td>\n      <td>27석</td>\n      <td>42</td>\n      <td>44</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>유럽 축구 연맹(UEFA) 집행위원회는 2014년 1월 24일에 열린 회의를 통해 ...</td>\n      <td>유럽 축구 연맹</td>\n      <td>0</td>\n      <td>7</td>\n      <td>UEFA</td>\n      <td>9</td>\n      <td>12</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
-     },
-     "metadata": {},
-     "execution_count": 2
-    }
-   ],
-   "source": [
-    "raw = load_data(Config.Train)\n",
-    "raw.head(3)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 24,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "class foo:\n",
-    "    def __repr__(self):\n",
-    "        return 'asdf'"
+    "data = REDataset_fix(preprocess_type=PreProcessType.ESP, device='cpu')\n",
+    "loader = DataLoader(data, batch_size=2)\n",
+    "for sents, labels in loader:\n",
+    "    break"
    ]
   },
   {
@@ -157,7 +73,9 @@
    "metadata": {},
    "outputs": [],
    "source": [
-    "temp = foo()"
+    "input_ids = sents['input_ids']\n",
+    "token_type_ids = sents['token_type_ids']\n",
+    "attention_mask = sents['attention_mask']"
    ]
   },
   {
@@ -169,7 +87,7 @@
      "output_type": "execute_result",
      "data": {
       "text/plain": [
-       "asdf"
+       "Embedding(119547, 768)"
       ]
      },
      "metadata": {},
@@ -177,163 +95,68 @@
     }
    ],
    "source": [
-    "temp"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 23,
-   "metadata": {},
-   "outputs": [
-    {
-     "output_type": "stream",
-     "name": "stdout",
-     "text": [
-      "asdf\n"
-     ]
-    }
-   ],
-   "source": [
-    "print(temp)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 4,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "data = preprocess_text(raw, method=PreProcessType.EMSP)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 9,
-   "metadata": {},
-   "outputs": [
-    {
-     "output_type": "stream",
-     "name": "stdout",
-     "text": [
-      "Load Tokenizer...\tdone!\n"
-     ]
-    }
-   ],
-   "source": [
-    "tokenizer = load_tokenizer(type=PreProcessType.EMSP)"
+    "model.resize_token_embeddings(len(data.tokenizer))"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 30,
    "metadata": {},
-   "outputs": [],
-   "source": [
-    "sample = data['input'].tolist()[0]\n",
-    "tokenized = tokenizer.tokenize(sample)\n",
-    "input = tokenizer(sample)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 41,
-   "metadata": {},
    "outputs": [
     {
      "output_type": "execute_result",
      "data": {
       "text/plain": [
-       "[4, 7]"
+       "odict_values([tensor(3.7427, grad_fn=<NllLossBackward>), tensor([[ 0.1835, -0.0588, -0.0764, -0.3866,  0.1130, -0.2831, -0.0367,  0.0009,\n",
+       "         -0.1457, -0.0048, -0.2346,  0.1587, -0.2430,  0.0714, -0.1243, -0.5032,\n",
+       "          0.1647, -0.1711, -0.0693, -0.0375, -0.1153, -0.1138,  0.2672,  0.0356,\n",
+       "         -0.0488, -0.2763, -0.0240, -0.0481, -0.2864, -0.1340, -0.1421,  0.1442,\n",
+       "          0.3489,  0.4172,  0.3577,  0.3389,  0.2836,  0.0617,  0.0819,  0.1563,\n",
+       "         -0.2427,  0.3150],\n",
+       "        [ 0.1903, -0.0945,  0.0325, -0.3274,  0.0269, -0.1795, -0.0289, -0.0411,\n",
+       "         -0.1599, -0.0465, -0.2820,  0.1646, -0.2633,  0.0852, -0.0416, -0.3936,\n",
+       "          0.1031, -0.1798, -0.1170, -0.0722, -0.0972, -0.1291,  0.2408,  0.0439,\n",
+       "         -0.1015, -0.2061,  0.0070, -0.0379, -0.1449, -0.1210, -0.0794,  0.0943,\n",
+       "          0.3620,  0.4173,  0.2751,  0.3539,  0.2218,  0.0313,  0.0721,  0.0639,\n",
+       "         -0.2290,  0.3660]], grad_fn=<AddmmBackward>)])"
       ]
      },
      "metadata": {},
-     "execution_count": 41
+     "execution_count": 30
     }
    ],
-   "source": []
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 42,
-   "metadata": {},
-   "outputs": [],
    "source": [
-    "entity_indices = {\n",
-    "            'e1': (tokenized.index(ST.E1Open), tokenized.index(ST.E1Close)),\n",
-    "            'e2': (tokenized.index(ST.E1Open), tokenized.index(ST.E1Close))\n",
-    "            }\n",
-    "last_sep_idx = [idx for idx, tok in enumerate(tokenized) if tok == ST.SEP].pop()"
+    "model(input_ids, token_type_ids, attention_mask, labels=labels).values()"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 27,
+   "execution_count": 28,
    "metadata": {},
    "outputs": [
     {
-     "output_type": "execute_result",
-     "data": {
-      "text/plain": [
-       "[4]"
-      ]
-     },
-     "metadata": {},
-     "execution_count": 27
-    }
-   ],
-   "source": [
-    "sep_indices"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 13,
-   "metadata": {},
-   "outputs": [
-    {
-     "output_type": "execute_result",
-     "data": {
-      "text/plain": [
-       "{'input_ids': [101, 9167, 15001, 11261, 41605, 102, 9651, 99183, 102, 50266, 11489, 9405, 24974, 24683, 9477, 90578, 9625, 119376, 12692, 45725, 119549, 9651, 99183, 119550, 9637, 9376, 42771, 70186, 119547, 9167, 15001, 11261, 41605, 119548, 113, 12001, 57836, 114, 9590, 9706, 28396, 113, 13796, 19986, 114, 8843, 22634, 117, 9638, 9376, 42771, 22879, 9651, 99183, 10459, 9684, 46520, 11513, 9641, 119298, 11018, 9251, 11261, 9405, 24974, 118800, 27792, 16139, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
-      ]
-     },
-     "metadata": {},
-     "execution_count": 13
-    }
-   ],
-   "source": [
-    "input"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 119,
-   "metadata": {},
-   "outputs": [
-    {
-     "output_type": "execute_result",
-     "data": {
-      "text/plain": [
-       "(30, 33, 19, 21)"
-      ]
-     },
-     "metadata": {},
-     "execution_count": 119
+     "output_type": "error",
+     "ename": "IndexError",
+     "evalue": "index out of range in self",
+     "traceback": [
+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
+      "\u001b[0;32m<ipython-input-28-5c6ca7e647fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
+      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1495\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1496\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1497\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1498\u001b[0m         )\n\u001b[1;32m   1499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 956\u001b[0;31m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    957\u001b[0m         )\n\u001b[1;32m    958\u001b[0m         encoder_outputs = self.encoder(\n",
+      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m         return F.embedding(\n\u001b[1;32m    125\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1812\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1813\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1814\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
+     ]
     }
    ],
    "source": [
-    "\n",
-    "e1_open, e1_close, e2_open, e2_close = sample[entity_cols].values\n",
-    "e1_open, e1_close, e2_open, e2_close"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 131,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "data = load_data(Config.Train)"
+    "model(**sents)"
    ]
   },
   {
@@ -341,237 +164,7 @@
    "execution_count": null,
    "metadata": {},
    "outputs": [],
-   "source": [
-    "data"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 138,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "data['input'] = data.apply(lambda x: attach_entities(x), axis=1)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 128,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "def attach_entities(row: pd.Series):\n",
-    "    \n",
-    "    sentence = row['relation_state']\n",
-    "    e1_open, e1_close, e2_open, e2_close = row[entity_cols].values\n",
-    "    output = ''\n",
-    "    if e1_open > e2_open:\n",
-    "        output += sentence[:e2_open]\n",
-    "        output += ST.E2Open\n",
-    "        output += sentence[e2_open:e2_close+1]\n",
-    "        output += ST.E2Close\n",
-    "        output += sentence[e2_close+1:e1_open]\n",
-    "        output += ST.E1Open\n",
-    "        output += sentence[e1_open:e1_close+1]\n",
-    "        output += ST.E1Close\n",
-    "        output += sentence[e1_close+1:]\n",
-    "    else:\n",
-    "        output += sentence[:e1_open]\n",
-    "        output += ST.E1Open\n",
-    "        output += sentence[e1_open:e1_close+1]\n",
-    "        output += ST.E1Close\n",
-    "        output += sentence[e1_close+1:e2_open]\n",
-    "        output += ST.E2Open\n",
-    "        output += sentence[e2_open:e2_close+1]\n",
-    "        output += ST.E2Close\n",
-    "        output += sentence[e2_close+1:]\n",
-    "    return output\n",
-    "    "
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 127,
-   "metadata": {},
-   "outputs": [
-    {
-     "output_type": "execute_result",
-     "data": {
-      "text/plain": [
-       "'영국에서 사용되는 스포츠 유틸리티 [E2]자동차[/E2]의 브랜드로는 [E1]랜드로버[/E1](Land Rover)와 지프(Jeep)가 있으며, 이 브랜드들은 자동차의 종류를 일컫는 말로 사용되기도 한다.'"
-      ]
-     },
-     "metadata": {},
-     "execution_count": 127
-    }
-   ],
-   "source": [
-    "attach_entities(sample)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 84,
-   "metadata": {},
-   "outputs": [
-    {
-     "output_type": "execute_result",
-     "data": {
-      "text/plain": [
-       "4"
-      ]
-     },
-     "metadata": {},
-     "execution_count": 84
-    }
-   ],
-   "source": [
-    "tokenizer = BertTokenizer.from_pretrained(PreTrainedType.MultiLingual)\n",
-    "tokenizer.add_special_tokens({'additional_special_tokens': [ST.E1Open, ST.E1Close, ST.E2Open, \n",
-    "ST.E2Close]})"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 85,
-   "metadata": {},
-   "outputs": [
-    {
-     "output_type": "execute_result",
-     "data": {
-      "text/plain": [
-       "PreTrainedTokenizer(name_or_path='bert-base-multilingual-cased', vocab_size=119547, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]', 'additional_special_tokens': ['[E1]', '[/E1]', '[E2]', '[/E2]']})"
-      ]
-     },
-     "metadata": {},
-     "execution_count": 85
-    }
-   ],
-   "source": [
-    "tokenizer"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 24,
-   "metadata": {},
-   "outputs": [
-    {
-     "output_type": "execute_result",
-     "data": {
-      "text/plain": [
-       "'랜드로버[SEP]영국에서 사용되는 스포츠 유틸리티 자동차의 브랜드로는 랜드로버(Land Rover)와 지프(Jeep)가 있으며, 이 브랜드들은 자동차의 종류를 일컫는 말로 사용되기도 한다.[SEP]민주당[SEP]선거에서 민주당은 해산 전 의석인 230석에 한참 못 미치는 57석(지역구 27석, 비례대표 30석)을 획득하는 데 그쳤다.'"
-      ]
-     },
-     "metadata": {},
-     "execution_count": 24
-    }
-   ],
-   "source": [
-    "sample = raw['e1'][0] + '[SEP]' + raw['relation_state'][0]\n",
-    "sample2 = raw['e1'][1] + '[SEP]' + raw['relation_state'][1]\n",
-    "sample + '[SEP]' + sample2"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 87,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "sample = raw.iloc[0, :]"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 88,
-   "metadata": {},
-   "outputs": [
-    {
-     "output_type": "execute_result",
-     "data": {
-      "text/plain": [
-       "relation_state    영국에서 사용되는 스포츠 유틸리티 자동차의 브랜드로는 랜드로버(Land Rover)...\n",
-       "e1                                                             랜드로버\n",
-       "e1_start                                                         30\n",
-       "e1_end                                                           33\n",
-       "e2                                                              자동차\n",
-       "e2_start                                                         19\n",
-       "e2_end                                                           21\n",
-       "label                                                            17\n",
-       "Name: 0, dtype: object"
-      ]
-     },
-     "metadata": {},
-     "execution_count": 88
-    }
-   ],
-   "source": [
-    "sample"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 89,
-   "metadata": {},
-   "outputs": [
-    {
-     "output_type": "execute_result",
-     "data": {
-      "text/plain": [
-       "(30, 33, 19, 21)"
-      ]
-     },
-     "metadata": {},
-     "execution_count": 89
-    }
-   ],
-   "source": [
-    "rs = sample['relation_state']\n",
-    "e1_open_idx = sample['e1_start']\n",
-    "e1_close_idx = sample['e1_end']\n",
-    "e2_open_idx = sample['e2_start']\n",
-    "e2_close_idx = sample['e2_end']\n",
-    "e1_open_idx, e1_close_idx, e2_open_idx, e2_close_idx"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 25,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "tokened = tokenizer(sample + '[SEP]' + sample2)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 30,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "tokened['token_type_ids'] = [1 for _ in range(112)]"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 32,
-   "metadata": {},
-   "outputs": [
-    {
-     "output_type": "execute_result",
-     "data": {
-      "text/plain": [
-       "{'input_ids': [101, 9167, 15001, 11261, 41605, 102, 50266, 11489, 9405, 24974, 24683, 9477, 90578, 9625, 119376, 12692, 45725, 9651, 99183, 10459, 9376, 42771, 70186, 9167, 15001, 11261, 41605, 113, 12001, 57836, 114, 9590, 9706, 28396, 113, 13796, 19986, 114, 8843, 22634, 117, 9638, 9376, 42771, 22879, 9651, 99183, 10459, 9684, 46520, 11513, 9641, 119298, 11018, 9251, 11261, 9405, 24974, 118800, 27792, 16139, 119, 102, 9311, 16323, 21928, 102, 9428, 41521, 11489, 9311, 16323, 21928, 10892, 9960, 21386, 9665, 9637, 40958, 12030, 16888, 40958, 10530, 9954, 119251, 9290, 9309, 72087, 11817, 40958, 113, 58939, 17196, 10365, 40958, 117, 9379, 58762, 14423, 37824, 10244, 40958, 114, 9633, 9999, 118813, 12178, 9083, 8924, 90821, 119, 102], 'token_type_ids': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
-      ]
-     },
-     "metadata": {},
-     "execution_count": 32
-    }
-   ],
-   "source": [
-    "tokened"
-   ]
+   "source": []
   }
  ]
 }
\ No newline at end of file
diff --git a/tokenization.py b/tokenization.py
index 6c330a1..5b7c714 100644
--- a/tokenization.py
+++ b/tokenization.py
@@ -1,3 +1,6 @@
+from tqdm import tqdm
+import pandas as pd
+import torch
 from dataclasses import dataclass
 from transformers import BertTokenizer, AutoTokenizer
 from config import PreProcessType, PreTrainedType
@@ -5,8 +8,9 @@ from config import PreProcessType, PreTrainedType
 # 토큰화 결과 [CLS] 토큰이 가장 앞에 붙게 되기 떄문에
 # Entity Mark에 대한 임베딩 값을 조절하는 과정에서 인덱스를 OFFSET만큼 밀어주기 위해 사용
 OFFSET = 1
-ENTITY_SCORE = 2
-SEP_SCORE = 2
+ENTITY_SCORE = 1
+SEP_SCORE = 1
+MAX_LENGTH = 128
 
 
 @dataclass
@@ -26,64 +30,6 @@ class SpecialToken:
     E2Close: str = "[/E2]"
 
 
-def tokenize(sentence, tokenizer, type: str=PreProcessType.Base) -> dict:
-    outputs = tokenizer(
-        sentence,
-        return_tensors="pt",
-        padding="max_length",
-        truncation=True,
-        max_length=128,
-        add_special_tokens=True,
-    )
-    if type != PreProcessType.Base:
-        tokenized = tokenizer.tokenize(sentence)
-
-        if type == PreProcessType.EM:
-            # Add embedding value for entity marker tokens([E1], [/E1], [E2], [/E2])
-            entity_indices = find_entity_indices(tokenized)
-            for open, close in entity_indices.values():
-                outputs.token_type_ids[0][
-                    OFFSET + open : OFFSET + close + 1
-                ] += ENTITY_SCORE
-
-        elif type == PreProcessType.ESP:
-            # Add embedding value for separation token([SEP])
-            last_sep_idx = fine_sep_indices(tokenized).pop()
-            outputs.token_type_ids[0][OFFSET : last_sep_idx + 1] += SEP_SCORE
-            return outputs
-
-        elif type == PreProcessType.EMSP:
-            entity_indices = find_entity_indices(tokenized)
-            for (open, close) in entity_indices.values():
-                outputs.token_type_ids[0][
-                    OFFSET + open : OFFSET + close + 1
-                ] += ENTITY_SCORE
-
-            last_sep_idx = fine_sep_indices(tokenized).pop()
-            outputs.token_type_ids[0][OFFSET : last_sep_idx + 1] += SEP_SCORE
-
-    return outputs
-
-
-def find_entity_indices(tokenized: list) -> dict:
-    entity_indices = {
-        "e1": (
-            tokenized.index(SpecialToken.E1Open),
-            tokenized.index(SpecialToken.E1Close),
-        ),
-        "e2": (
-            tokenized.index(SpecialToken.E2Open),
-            tokenized.index(SpecialToken.E2Close),
-        ),
-    }
-    return entity_indices
-
-
-def fine_sep_indices(tokenized: list) -> list:
-    sep_indices = [idx for idx, tok in enumerate(tokenized) if tok == SpecialToken.SEP]
-    return sep_indices
-
-
 def load_tokenizer(type: str = PreProcessType.Base):
     """사전 학습된 tokenizer를 불러오는 함수
     Args
@@ -99,7 +45,7 @@ def load_tokenizer(type: str = PreProcessType.Base):
         tokenizer = BertTokenizer.from_pretrained(PreTrainedType.MultiLingual)
 
     # Entity Marker, Entity Marker Separator with Position Embedding
-    elif type in [PreProcessType.EM, PreProcessType.EMSP]:
+    elif type == PreProcessType.EM or type == PreProcessType.EMSP:
         tokenizer = BertTokenizer.from_pretrained(PreTrainedType.MultiLingual)
         tokenizer.add_special_tokens(
             {
@@ -117,7 +63,91 @@ def load_tokenizer(type: str = PreProcessType.Base):
     return tokenizer
 
 
+def tokenize(data: pd.DataFrame, tokenizer, type: str=PreProcessType.Base):
+    print("Apply Tokenization...", end="\t")
+    data_tokenized = tokenizer(
+        data["input"].tolist(),
+        return_tensors="pt",
+        padding=True,
+        truncation=True,
+        max_length=MAX_LENGTH,
+        add_special_tokens=True,
+    )
+    if type not in  [PreProcessType.Base, PreProcessType.ES]:
+        tokenized_decoded = data["input"].apply(lambda x: tokenizer.tokenize(x))
+
+        if type == PreProcessType.EM:
+            # entity marker
+            entity_intervals = tokenized_decoded.apply(
+                lambda x: find_entity_intervals(x)
+            ).tolist()
+            entity_interval_tensor = make_additional_token_type_ids(
+                entity_intervals, data_size=data.shape[0]
+            )
+            data_tokenized["token_type_ids"] += entity_interval_tensor.long()
+
+        elif type == PreProcessType.EMSP:
+            # entity marker
+            entity_intervals = tokenized_decoded.apply(
+                lambda x: find_entity_intervals(x)
+            ).tolist()
+            entity_interval_tensor = make_additional_token_type_ids(
+                entity_intervals, data_size=data.shape[0], type='entity'
+            )
+
+            # entity separation
+            sep_intervals = tokenized_decoded.apply(lambda x: find_sep_intervals(x)).tolist()
+            sep_interval_tensor = make_additional_token_type_ids(
+                sep_intervals, data_size=data.shape[0], type='sep'
+            )
+
+            data_tokenized["token_type_ids"] += entity_interval_tensor.long()
+            data_tokenized["token_type_ids"] += sep_interval_tensor.long()
+
+        elif type == PreProcessType.ESP:
+            # entity separation
+            sep_intervals = tokenized_decoded.apply(lambda x: find_sep_intervals(x)).tolist()
+            sep_interval_tensor = make_additional_token_type_ids(
+                sep_intervals, data_size=data.shape[0], type='sep'
+            )
+            data_tokenized["token_type_ids"] += sep_interval_tensor.long()
+
+    return data_tokenized
+
+
+def find_sep_intervals(tokenized: list) -> list:
+    sep_indices = tuple(idx for idx, tok in enumerate(tokenized) if tok == SpecialToken.SEP)
+    return sep_indices
+
+
+def find_entity_intervals(tokenized: list) -> dict:
+        entity_intervals = [
+            (tokenized.index(SpecialToken.E1Open), tokenized.index(SpecialToken.E1Close)),
+            (tokenized.index(SpecialToken.E2Open), tokenized.index(SpecialToken.E2Close))
+        ]
+        return entity_intervals
+
+def make_additional_token_type_ids(intervals: list, data_size: int, type: str='entity'):
+        n_rows = data_size
+        n_cols = MAX_LENGTH
+        additional_token_type_ids = torch.zeros(n_rows, n_cols)
+
+        if type == 'entity':
+            for idx, (e1, e2) in tqdm(enumerate(intervals), desc="Update token_type_ids"):
+                additional_token_type_ids[idx][OFFSET+e1[0]: OFFSET+e1[1]+1] += ENTITY_SCORE
+                additional_token_type_ids[idx][OFFSET+e2[0]: OFFSET+e2[1]+1] += ENTITY_SCORE
+
+        elif type == 'sep':
+            for idx, sep in tqdm(enumerate(intervals), desc="Update token_type_ids"):
+                last_sep = sep[-1]
+                additional_token_type_ids[idx][OFFSET: OFFSET+last_sep+1] += SEP_SCORE
+                additional_token_type_ids[idx][OFFSET: OFFSET+last_sep+1] += SEP_SCORE
+                
+        return additional_token_type_ids
+
+
+
 if __name__ == '__main__':
-    tokenizer = load_tokenizer(PreProcessType.EM)
+    tokenizer = load_tokenizer(PreProcessType.ES)
     sentence = '영국에서 사용되는 스포츠 유틸리티 [E2]자동차[/E2]의 브랜드로는 [E1]랜드로버[/E1](Land Rover)와 지프(Jeep)가 있으며, 이 브랜드들은 자동차의 종류를 일컫는 말로 사용되기도 한다.'
-    tokenize(sentence, tokenizer, PreProcessType.EM)
\ No newline at end of file
+    tokenize(sentence, tokenizer, PreProcessType.ES)
\ No newline at end of file
diff --git a/train.py b/train.py
index 1237fbc..3847850 100644
--- a/train.py
+++ b/train.py
@@ -8,14 +8,14 @@ from torch.utils.data import DataLoader
 import wandb
 from evaluation import evaluate
 from models import load_model
-from dataset import REDataset, split_train_test_loader
+from dataset import REDataset, REDataset_fix, split_train_test_loader
 from optimizers import get_optimizer, get_scheduler
 from criterions import get_criterion
 from utils import get_timestamp, get_timestamp, set_seed, verbose, ckpt_name, save_json
 from config import ModelType, Config, Optimizer, PreTrainedType, PreProcessType, Loss
 
 warnings.filterwarnings("ignore")
-
+# os.environ[ "TF_FORCE_GPU_ALLOW_GROWTH" ] = "true"
 
 TOTAL_SAMPLES = 9000
 
@@ -45,7 +45,7 @@ def train(
     set_seed(seed)
 
     # load data
-    dataset = REDataset(root=data_root, preprocess_type=preprocess_type, device=device)
+    dataset = REDataset_fix(root=data_root, preprocess_type=preprocess_type, device=device)
 
     # 학습 데이터를 분리하지 않을 경우
     if valid_size == 0:
@@ -68,6 +68,7 @@ def train(
     model = load_model(
         model_type, pretrained_type, num_classes, load_state_dict, pooler_idx, dropout
     )
+    model.resize_token_embeddings(len(dataset.tokenizer))
     model.to(device)
     model.train()
 
@@ -98,12 +99,14 @@ def train(
 
         for sentences, labels in tqdm(train_loader, desc="[Train]"):
             if model_type == ModelType.SequenceClf:
-                loss, outputs = model(**sentences, labels=labels).values()
+                input_ids = sentences['input_ids']
+                token_type_ids = sentences['token_type_ids']
+                attention_mask = sentences['attention_mask']
+                loss, outputs = model(input_ids, token_type_ids, attention_mask, labels=labels).values()
             else:
                 outputs = model(**sentences)
                 loss = criterion(outputs, labels)
                 
-
             total_loss += loss.item()
 
             # backpropagation
@@ -248,7 +251,10 @@ def validate(model, model_type, valid_loader, criterion):
     with torch.no_grad():
         for sentences, labels in tqdm(valid_loader, desc="[Valid]"):
             if model_type == ModelType.SequenceClf:
-                loss, outputs = model(**sentences, labels=labels).values()
+                input_ids = sentences['input_ids']
+                token_type_ids = sentences['token_type_ids']
+                attention_mask = sentences['attention_mask']
+                loss, outputs = model(input_ids, token_type_ids, attention_mask, labels=labels).values()
             else:
                 outputs = model(**sentences)
                 loss = criterion(outputs, labels)
@@ -288,7 +294,7 @@ if __name__ == "__main__":
     parser.add_argument("--dropout", type=float, default=0.8)
     parser.add_argument("--load-state-dict", type=str, default=LOAD_STATE_DICT)
     parser.add_argument("--data-root", type=str, default=Config.Train)
-    parser.add_argument("--preprocess-type", type=str, default=PreProcessType.EM)
+    parser.add_argument("--preprocess-type", type=str, default=PreProcessType.ESP)
     parser.add_argument("--epochs", type=int, default=Config.Epochs)
     parser.add_argument("--valid-size", type=int, default=Config.ValidSize)
     parser.add_argument("--train-batch-size", type=int, default=Config.Batch8)
diff --git a/wandb/latest-run b/wandb/latest-run
index 0566aeb..c30e059 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20210419_071305-2r3gvfg4
\ No newline at end of file
+run-20210419_135904-2c5m4ye1
\ No newline at end of file
