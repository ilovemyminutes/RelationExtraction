diff --git a/notebooks/Data Augmentation.ipynb b/notebooks/Data Augmentation.ipynb
index 553ae37..4f2a9f4 100644
--- a/notebooks/Data Augmentation.ipynb	
+++ b/notebooks/Data Augmentation.ipynb	
@@ -23,7 +23,7 @@
  "cells": [
   {
    "cell_type": "code",
-   "execution_count": 1,
+   "execution_count": 29,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -77,7 +77,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 30,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -107,50 +107,68 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 27,
+   "execution_count": 28,
+   "metadata": {},
+   "outputs": [
+    {
+     "output_type": "stream",
+     "name": "stdout",
+     "text": [
+      "CPU times: user 3h 40min 16s, sys: 11min 20s, total: 3h 51min 37s\nWall time: 2h 52min 57s\n"
+     ]
+    }
+   ],
+   "source": [
+    "%%time\n",
+    "data['relation_state'] = data['relation_state'].apply(lambda x: spacing(x))"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 35,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "data['relation_state'] = data.apply(lambda x: x['relation_state'].replace(TEMP_E1, x['e1']), axis=1)\n",
+    "data['relation_state'] = data.apply(lambda x: x['relation_state'].replace(TEMP_E2, x['e2']), axis=1)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 37,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "data.to_csv('external_preprocessed_v1.csv')"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 39,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "sample = data.iloc[0, :]"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 41,
    "metadata": {},
    "outputs": [
     {
      "output_type": "execute_result",
      "data": {
       "text/plain": [
-       "                        id                                     relation_state  \\\n",
-       "0         from train_csv 5  카터는영희와이스라엘을조정하여,캠프데이비드에서철수대통령과메나헴베긴수상과함께중동평화를위...   \n",
-       "1         from train_csv 8  카터는이집트와영희을조정하여,캠프데이비드에서안와르사다트대통령과메나헴베긴수상과함께중동평...   \n",
-       "2         from train_csv 9  카터는영희와이스라엘을조정하여,캠프데이비드에서안와르사다트대통령과메나헴베긴수상과함께중동...   \n",
-       "3        from train_csv 11  카터는이집트와영희을조정하여,캠프데이비드에서안와르사다트대통령과철수수상과함께중동평화를위...   \n",
-       "4        from train_csv 65  선거전까지각축전을벌인유력한후보는15대대통령선거에나와김대중에게패배한제1야당철수의대표'...   \n",
-       "...                    ...                                                ...   \n",
-       "229606  from dev_csv 99215                           2.\"영희\"(철수작사/계동균작곡)–03:24   \n",
-       "229607  from dev_csv 99216                            9.\"영희\"(철수작사/철수작곡)-02:06   \n",
-       "229608  from dev_csv 99217  2009년,블런트는장마크발레가감독을맡고철수가각본을쓴《영희》에서빅토리아여왕역으로출연하였다.   \n",
-       "229609  from dev_csv 99218  2008년저드애퍼토가제작하고세스로건과철수가각본을맡은코미디영화《영희》와무술영화《겟썸》...   \n",
-       "229610  from dev_csv 99219  영화《라이터를켜라》,《영희》의철수,《그해여름》의김은희작가가공동으로집필하고,조현탁PD...   \n",
-       "\n",
-       "               e1  e1_start  e1_end          e2  e2_start  e2_end       label  \n",
-       "0         안와르 사다트        38      44         이집트         5       7  인물:출신성분/국적  \n",
-       "1       캠프데이비드 협정        78      86        이스라엘        11      14      단체:구성원  \n",
-       "2       캠프데이비드 협정        78      86         이집트         5       7      단체:구성원  \n",
-       "3          메나헴 베긴        52      57        이스라엘        11      14  인물:출신성분/국적  \n",
-       "4            한나라당        63      66         이회창        75      77      단체:구성원  \n",
-       "...           ...       ...     ...         ...       ...     ...         ...  \n",
-       "229606        박건호        11      13        사슴여인         4       7       인물:제작  \n",
-       "229607         장덕        14      15     안녕히 계세요         4      10       인물:제작  \n",
-       "229608    줄리언 펠로스        27      33      영 빅토리아        43      48       인물:제작  \n",
-       "229609    에번 골드버그        26      32  파인애플 익스프레스        50      59       인물:제작  \n",
-       "229610        장항준        23      25      귀신이 산다        14      19       인물:제작  \n",
-       "\n",
-       "[229611 rows x 9 columns]"
-      ],
-      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>relation_state</th>\n      <th>e1</th>\n      <th>e1_start</th>\n      <th>e1_end</th>\n      <th>e2</th>\n      <th>e2_start</th>\n      <th>e2_end</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>from train_csv 5</td>\n      <td>카터는영희와이스라엘을조정하여,캠프데이비드에서철수대통령과메나헴베긴수상과함께중동평화를위...</td>\n      <td>안와르 사다트</td>\n      <td>38</td>\n      <td>44</td>\n      <td>이집트</td>\n      <td>5</td>\n      <td>7</td>\n      <td>인물:출신성분/국적</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>from train_csv 8</td>\n      <td>카터는이집트와영희을조정하여,캠프데이비드에서안와르사다트대통령과메나헴베긴수상과함께중동평...</td>\n      <td>캠프데이비드 협정</td>\n      <td>78</td>\n      <td>86</td>\n      <td>이스라엘</td>\n      <td>11</td>\n      <td>14</td>\n      <td>단체:구성원</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>from train_csv 9</td>\n      <td>카터는영희와이스라엘을조정하여,캠프데이비드에서안와르사다트대통령과메나헴베긴수상과함께중동...</td>\n      <td>캠프데이비드 협정</td>\n      <td>78</td>\n      <td>86</td>\n      <td>이집트</td>\n      <td>5</td>\n      <td>7</td>\n      <td>단체:구성원</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>from train_csv 11</td>\n      <td>카터는이집트와영희을조정하여,캠프데이비드에서안와르사다트대통령과철수수상과함께중동평화를위...</td>\n      <td>메나헴 베긴</td>\n      <td>52</td>\n      <td>57</td>\n      <td>이스라엘</td>\n      <td>11</td>\n      <td>14</td>\n      <td>인물:출신성분/국적</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>from train_csv 65</td>\n      <td>선거전까지각축전을벌인유력한후보는15대대통령선거에나와김대중에게패배한제1야당철수의대표'...</td>\n      <td>한나라당</td>\n      <td>63</td>\n      <td>66</td>\n      <td>이회창</td>\n      <td>75</td>\n      <td>77</td>\n      <td>단체:구성원</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>229606</th>\n      <td>from dev_csv 99215</td>\n      <td>2.\"영희\"(철수작사/계동균작곡)–03:24</td>\n      <td>박건호</td>\n      <td>11</td>\n      <td>13</td>\n      <td>사슴여인</td>\n      <td>4</td>\n      <td>7</td>\n      <td>인물:제작</td>\n    </tr>\n    <tr>\n      <th>229607</th>\n      <td>from dev_csv 99216</td>\n      <td>9.\"영희\"(철수작사/철수작곡)-02:06</td>\n      <td>장덕</td>\n      <td>14</td>\n      <td>15</td>\n      <td>안녕히 계세요</td>\n      <td>4</td>\n      <td>10</td>\n      <td>인물:제작</td>\n    </tr>\n    <tr>\n      <th>229608</th>\n      <td>from dev_csv 99217</td>\n      <td>2009년,블런트는장마크발레가감독을맡고철수가각본을쓴《영희》에서빅토리아여왕역으로출연하였다.</td>\n      <td>줄리언 펠로스</td>\n      <td>27</td>\n      <td>33</td>\n      <td>영 빅토리아</td>\n      <td>43</td>\n      <td>48</td>\n      <td>인물:제작</td>\n    </tr>\n    <tr>\n      <th>229609</th>\n      <td>from dev_csv 99218</td>\n      <td>2008년저드애퍼토가제작하고세스로건과철수가각본을맡은코미디영화《영희》와무술영화《겟썸》...</td>\n      <td>에번 골드버그</td>\n      <td>26</td>\n      <td>32</td>\n      <td>파인애플 익스프레스</td>\n      <td>50</td>\n      <td>59</td>\n      <td>인물:제작</td>\n    </tr>\n    <tr>\n      <th>229610</th>\n      <td>from dev_csv 99219</td>\n      <td>영화《라이터를켜라》,《영희》의철수,《그해여름》의김은희작가가공동으로집필하고,조현탁PD...</td>\n      <td>장항준</td>\n      <td>23</td>\n      <td>25</td>\n      <td>귀신이 산다</td>\n      <td>14</td>\n      <td>19</td>\n      <td>인물:제작</td>\n    </tr>\n  </tbody>\n</table>\n<p>229611 rows × 9 columns</p>\n</div>"
+       "31"
+      ]
      },
      "metadata": {},
-     "execution_count": 27
+     "execution_count": 41
     }
    ],
    "source": [
-    "%%time\n",
-    "data['relation_state'] = data['relation_state'].apply(lambda x: spacing(x))"
+    "sample['relation_state'].index(sample['e1'])"
    ]
   }
  ]
diff --git a/notebooks/Debug - Model Inference.ipynb b/notebooks/Debug - Model Inference.ipynb
deleted file mode 100644
index 632741e..0000000
--- a/notebooks/Debug - Model Inference.ipynb	
+++ /dev/null
@@ -1,551 +0,0 @@
-{
- "metadata": {
-  "language_info": {
-   "codemirror_mode": {
-    "name": "ipython",
-    "version": 3
-   },
-   "file_extension": ".py",
-   "mimetype": "text/x-python",
-   "name": "python",
-   "nbconvert_exporter": "python",
-   "pygments_lexer": "ipython3",
-   "version": "3.7.7-final"
-  },
-  "orig_nbformat": 2,
-  "kernelspec": {
-   "name": "python377jvsc74a57bd098b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f",
-   "display_name": "Python 3.7.7 64-bit ('base': conda)"
-  }
- },
- "nbformat": 4,
- "nbformat_minor": 2,
- "cells": [
-  {
-   "cell_type": "code",
-   "execution_count": 30,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "import torch\n",
-    "from torch.nn import functional as F\n",
-    "from torch import optim\n",
-    "from torch import nn\n",
-    "from torch.utils.data import DataLoader\n",
-    "import sys\n",
-    "sys.path.insert(0, '../')\n",
-    "\n",
-    "from config import *\n",
-    "from transformers import BertModel, BertConfig, BertForSequenceClassification\n",
-    "from models import load_model, VanillaBert\n",
-    "from dataset import REDataset, split_train_test_loader\n",
-    "from utils import set_seed\n",
-    "from criterions import *\n",
-    "from optimizers import *"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 31,
-   "metadata": {},
-   "outputs": [
-    {
-     "output_type": "stream",
-     "name": "stdout",
-     "text": [
-      "Load raw data...\tpreprocessing for 'Base'...\tdone!\n",
-      "Load Tokenizer...\tdone!\n",
-      "Apply Tokenization...\tdone!\n"
-     ]
-    }
-   ],
-   "source": [
-    "data = REDataset()"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 34,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "train_loader, valid_loader = split_train_test_loader(data, 0.2)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 36,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "for sents, labels in train_loader:\n",
-    "    break"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 38,
-   "metadata": {},
-   "outputs": [
-    {
-     "output_type": "execute_result",
-     "data": {
-      "text/plain": [
-       "tensor([ 0,  0,  0,  4, 25,  9,  0, 22,  0,  0,  2,  3,  4,  2,  0, 10,  2,  0,\n",
-       "         0,  0,  3,  0,  2,  6, 10,  7,  0,  0,  2, 10,  0,  6],\n",
-       "       device='cuda:0')"
-      ]
-     },
-     "metadata": {},
-     "execution_count": 38
-    }
-   ],
-   "source": [
-    "labels"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 14,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "LOAD = \"../saved_models/VanillaBert_bert-base-multilingual-cased_20210418164452/VanillaBert_bert-base-multilingual-cased_ep(02)acc(0.4878)loss(0.0048)id(20210418164452).pth\"\n"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 19,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "model_type = ModelType.VanillaBert\n",
-    "pretrained_type = PreTrainedType.MultiLingual\n",
-    "num_classes=Config.NumClasses\n",
-    "pooler_idx=0\n",
-    "load_state_dict=None\n",
-    "data_root=Config.Train\n",
-    "preprocess_type=PreProcessType.ES\n",
-    "epochs=Config.Epochs\n",
-    "valid_size=Config.ValidSize\n",
-    "train_batch_size=Config.Batch8\n",
-    "valid_batch_size=512\n",
-    "optim_type=Optimizer.Adam\n",
-    "loss_type=Loss.CE\n",
-    "lr=Config.LRSlower\n",
-    "lr_scheduler=Optimizer.CosineScheduler\n",
-    "device = Config.Device"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 24,
-   "metadata": {},
-   "outputs": [
-    {
-     "output_type": "stream",
-     "name": "stdout",
-     "text": [
-      "Load raw data...\tpreprocessing for 'EntitySeparation'...\tdone!\n",
-      "Load Tokenizer...\tdone!\n",
-      "Apply Tokenization...\tdone!\n",
-      "Load Model...\tSome weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
-      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
-      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
-      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
-      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
-      "done!\n"
-     ]
-    },
-    {
-     "output_type": "execute_result",
-     "data": {
-      "text/plain": [
-       "VanillaBert(\n",
-       "  (backbone): BertModel(\n",
-       "    (embeddings): BertEmbeddings(\n",
-       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
-       "      (position_embeddings): Embedding(512, 768)\n",
-       "      (token_type_embeddings): Embedding(2, 768)\n",
-       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "      (dropout): Dropout(p=0.1, inplace=False)\n",
-       "    )\n",
-       "    (encoder): BertEncoder(\n",
-       "      (layer): ModuleList(\n",
-       "        (0): BertLayer(\n",
-       "          (attention): BertAttention(\n",
-       "            (self): BertSelfAttention(\n",
-       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "            (output): BertSelfOutput(\n",
-       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "          )\n",
-       "          (intermediate): BertIntermediate(\n",
-       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "          )\n",
-       "          (output): BertOutput(\n",
-       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "            (dropout): Dropout(p=0.1, inplace=False)\n",
-       "          )\n",
-       "        )\n",
-       "        (1): BertLayer(\n",
-       "          (attention): BertAttention(\n",
-       "            (self): BertSelfAttention(\n",
-       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "            (output): BertSelfOutput(\n",
-       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "          )\n",
-       "          (intermediate): BertIntermediate(\n",
-       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "          )\n",
-       "          (output): BertOutput(\n",
-       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "            (dropout): Dropout(p=0.1, inplace=False)\n",
-       "          )\n",
-       "        )\n",
-       "        (2): BertLayer(\n",
-       "          (attention): BertAttention(\n",
-       "            (self): BertSelfAttention(\n",
-       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "            (output): BertSelfOutput(\n",
-       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "          )\n",
-       "          (intermediate): BertIntermediate(\n",
-       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "          )\n",
-       "          (output): BertOutput(\n",
-       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "            (dropout): Dropout(p=0.1, inplace=False)\n",
-       "          )\n",
-       "        )\n",
-       "        (3): BertLayer(\n",
-       "          (attention): BertAttention(\n",
-       "            (self): BertSelfAttention(\n",
-       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "            (output): BertSelfOutput(\n",
-       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "          )\n",
-       "          (intermediate): BertIntermediate(\n",
-       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "          )\n",
-       "          (output): BertOutput(\n",
-       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "            (dropout): Dropout(p=0.1, inplace=False)\n",
-       "          )\n",
-       "        )\n",
-       "        (4): BertLayer(\n",
-       "          (attention): BertAttention(\n",
-       "            (self): BertSelfAttention(\n",
-       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "            (output): BertSelfOutput(\n",
-       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "          )\n",
-       "          (intermediate): BertIntermediate(\n",
-       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "          )\n",
-       "          (output): BertOutput(\n",
-       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "            (dropout): Dropout(p=0.1, inplace=False)\n",
-       "          )\n",
-       "        )\n",
-       "        (5): BertLayer(\n",
-       "          (attention): BertAttention(\n",
-       "            (self): BertSelfAttention(\n",
-       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "            (output): BertSelfOutput(\n",
-       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "          )\n",
-       "          (intermediate): BertIntermediate(\n",
-       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "          )\n",
-       "          (output): BertOutput(\n",
-       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "            (dropout): Dropout(p=0.1, inplace=False)\n",
-       "          )\n",
-       "        )\n",
-       "        (6): BertLayer(\n",
-       "          (attention): BertAttention(\n",
-       "            (self): BertSelfAttention(\n",
-       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "            (output): BertSelfOutput(\n",
-       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "          )\n",
-       "          (intermediate): BertIntermediate(\n",
-       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "          )\n",
-       "          (output): BertOutput(\n",
-       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "            (dropout): Dropout(p=0.1, inplace=False)\n",
-       "          )\n",
-       "        )\n",
-       "        (7): BertLayer(\n",
-       "          (attention): BertAttention(\n",
-       "            (self): BertSelfAttention(\n",
-       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "            (output): BertSelfOutput(\n",
-       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "          )\n",
-       "          (intermediate): BertIntermediate(\n",
-       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "          )\n",
-       "          (output): BertOutput(\n",
-       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "            (dropout): Dropout(p=0.1, inplace=False)\n",
-       "          )\n",
-       "        )\n",
-       "        (8): BertLayer(\n",
-       "          (attention): BertAttention(\n",
-       "            (self): BertSelfAttention(\n",
-       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "            (output): BertSelfOutput(\n",
-       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "          )\n",
-       "          (intermediate): BertIntermediate(\n",
-       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "          )\n",
-       "          (output): BertOutput(\n",
-       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "            (dropout): Dropout(p=0.1, inplace=False)\n",
-       "          )\n",
-       "        )\n",
-       "        (9): BertLayer(\n",
-       "          (attention): BertAttention(\n",
-       "            (self): BertSelfAttention(\n",
-       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "            (output): BertSelfOutput(\n",
-       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "          )\n",
-       "          (intermediate): BertIntermediate(\n",
-       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "          )\n",
-       "          (output): BertOutput(\n",
-       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "            (dropout): Dropout(p=0.1, inplace=False)\n",
-       "          )\n",
-       "        )\n",
-       "        (10): BertLayer(\n",
-       "          (attention): BertAttention(\n",
-       "            (self): BertSelfAttention(\n",
-       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "            (output): BertSelfOutput(\n",
-       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "          )\n",
-       "          (intermediate): BertIntermediate(\n",
-       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "          )\n",
-       "          (output): BertOutput(\n",
-       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "            (dropout): Dropout(p=0.1, inplace=False)\n",
-       "          )\n",
-       "        )\n",
-       "        (11): BertLayer(\n",
-       "          (attention): BertAttention(\n",
-       "            (self): BertSelfAttention(\n",
-       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "            (output): BertSelfOutput(\n",
-       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
-       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "              (dropout): Dropout(p=0.1, inplace=False)\n",
-       "            )\n",
-       "          )\n",
-       "          (intermediate): BertIntermediate(\n",
-       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
-       "          )\n",
-       "          (output): BertOutput(\n",
-       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
-       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
-       "            (dropout): Dropout(p=0.1, inplace=False)\n",
-       "          )\n",
-       "        )\n",
-       "      )\n",
-       "    )\n",
-       "    (pooler): BertPooler(\n",
-       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
-       "      (activation): Tanh()\n",
-       "    )\n",
-       "  )\n",
-       "  (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
-       "  (dropout): Dropout(p=0.5, inplace=False)\n",
-       "  (relu): ReLU()\n",
-       "  (linear): Linear(in_features=768, out_features=42, bias=True)\n",
-       ")"
-      ]
-     },
-     "metadata": {},
-     "execution_count": 24
-    }
-   ],
-   "source": [
-    "dataset = REDataset(root=Config.Train, preprocess_type=preprocess_type, device=device)\n",
-    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, drop_last=True)\n",
-    "\n",
-    "# load model\n",
-    "model = load_model(model_type, pretrained_type, num_classes, load_state_dict, pooler_idx)\n",
-    "model.to(device)\n",
-    "model.train()"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 25,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "# load criterion, optimizer, scheduler\n",
-    "criterion = get_criterion(type=loss_type)\n",
-    "optimizer = get_optimizer(model=model, type=optim_type, lr=lr)\n",
-    "if lr_scheduler is not None:\n",
-    "    scheduler = get_scheduler(type=lr_scheduler, optimizer=optimizer)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "# make checkpoint directory to save model during train\n",
-    "checkpoint_dir = f\"{model_type}_{pretrained_type}_{TIMESTAMP}\"\n",
-    "if checkpoint_dir not in os.listdir(save_path):\n",
-    "    os.mkdir(os.path.join(save_path, checkpoint_dir))\n",
-    "save_path = os.path.join(save_path, checkpoint_dir)\n",
-    "\n",
-    "# train phase\n",
-    "best_acc = 0\n",
-    "best_loss = 999\n",
-    "\n",
-    "for epoch in range(epochs):\n",
-    "    print(f\"Epoch: {epoch}\")\n",
-    "\n",
-    "    pred_list = []\n",
-    "    true_list = []\n",
-    "    total_loss = 0\n",
-    "\n",
-    "    for idx, (sentences, labels) in tqdm(enumerate(train_loader), desc=\"[Train]\"):\n",
-    "        if model_type == ModelType.SequenceClf:\n",
-    "            outputs = model(**sentences).logits\n",
-    "        elif model_type == ModelType.Base:\n",
-    "            outputs = model(**sentences).pooler_output\n",
-    "        else:\n",
-    "            outputs = model(**sentences)\n",
-    "\n",
-    "        loss = criterion(outputs, labels)\n",
-    "        total_loss += loss.item()\n",
-    "\n",
-    "        # backpropagation\n",
-    "        optimizer.zero_grad()\n",
-    "        loss.backward()\n",
-    "        optimizer.step()\n",
-    "        \n",
-    "        if lr_scheduler is not None:\n",
-    "            scheduler.step()\n",
-    "\n",
-    "        # stack preds for evaluate\n",
-    "        _, preds = torch.max(outputs, dim=1)\n",
-    "        preds = preds.data.cpu().numpy()\n",
-    "        labels = labels.data.cpu().numpy()\n",
-    "\n",
-    "        pred_list.append(preds)\n",
-    "        true_list.append(labels)\n",
-    "\n",
-    "        pred_arr = np.hstack(pred_list)\n",
-    "        true_arr = np.hstack(true_list)\n",
-    "\n",
-    "        # evaluation phase\n",
-    "        train_eval = evaluate(y_true=true_arr, y_pred=pred_arr)  # ACC, F1, PRC, REC\n",
-    "        train_loss = total_loss / len(true_arr)"
-   ]
-  }
- ]
-}
\ No newline at end of file
diff --git a/notebooks/Entity Marker.ipynb b/notebooks/Entity Marker.ipynb
index add87de..b332719 100644
--- a/notebooks/Entity Marker.ipynb	
+++ b/notebooks/Entity Marker.ipynb	
@@ -23,7 +23,7 @@
  "cells": [
   {
    "cell_type": "code",
-   "execution_count": 2,
+   "execution_count": 1,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -31,7 +31,7 @@
     "import sys\n",
     "import pandas as pd\n",
     "import torch\n",
-    "from torch.utils.data import Dataset\n",
+    "from torch.utils.data import Dataset, DataLoader\n",
     "from transformers import BertTokenizer\n",
     "sys.path.insert(0, '../')\n",
     "from typing import Tuple, Dict\n",
@@ -46,14 +46,14 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 7,
+   "execution_count": 2,
    "metadata": {},
    "outputs": [
     {
      "output_type": "stream",
      "name": "stdout",
      "text": [
-      "Load Tokenizer...\tdone!\n"
+      "Load Tokenizer for EntityMarker...\tdone!\n"
      ]
     }
    ],
@@ -63,51 +63,68 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 10,
+   "execution_count": 3,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "sentence = '영국에서 사용되는 스포츠 유틸리티 [E2]자동차[/E2]의 브랜드로는 [E1]랜드로버[/E1](Land Rover)와 지프(Jeep)가 있으며, 이 브랜드들은 자동차의 종류를 일컫는 말로 사용되기도 한다.'"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 11,
    "metadata": {},
    "outputs": [
     {
      "output_type": "execute_result",
      "data": {
       "text/plain": [
-       "{'input_ids': tensor([[   101,  50266,  11489,   9405,  24974,  24683,   9477,  90578,   9625,\n",
-       "         119376,  12692,  45725, 119549,   9651,  99183, 119550,   9637,   9376,\n",
-       "          42771,  70186, 119547,   9167,  15001,  11261,  41605, 119548,    113,\n",
-       "          12001,  57836,    114,   9590,   9706,  28396,    113,  13796,  19986,\n",
-       "            114,   8843,  22634,    117,   9638,   9376,  42771,  22879,   9651,\n",
-       "          99183,  10459,   9684,  46520,  11513,   9641, 119298,  11018,   9251,\n",
-       "          11261,   9405,  24974, 118800,  27792,  16139,    119,    102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
-       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
-       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
-       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
-       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
+       "512"
       ]
      },
      "metadata": {},
-     "execution_count": 10
+     "execution_count": 11
     }
    ],
    "source": [
-    "sentence = '영국에서 사용되는 스포츠 유틸리티 [E2]자동차[/E2]의 브랜드로는 [E1]랜드로버[/E1](Land Rover)와 지프(Jeep)가 있으며, 이 브랜드들은 자동차의 종류를 일컫는 말로 사용되기도 한다.'\n",
-    "tokenize(sentence, tokenizer, PreProcessType.EM)"
+    "len(tokenizer(sentence, padding=\"max_length\")['input_ids'])"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 5,
+   "execution_count": 4,
    "metadata": {},
    "outputs": [
     {
      "output_type": "stream",
      "name": "stdout",
      "text": [
-      "Load raw data...\tpreprocessing for 'EntityMarkerSeparationPositionEmbedding'...\tdone!\n",
-      "Load Tokenizer...\tdone!\n"
+      "Load raw data...\tapply preprocess 'EntityMarker'...\tdone!\n",
+      "Load Tokenizer for EntityMarker...\tdone!\n"
      ]
     }
    ],
    "source": [
-    "data = REDataset(preprocess_type=PreProcessType.EMSP)"
+    "data = REDataset(preprocess_type=PreProcessType.EM)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 6,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "loader = DataLoader(data, batch_size=4)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 7,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "for sents, labels in loader:\n",
+    "    break"
    ]
   },
   {
diff --git a/tokenization.py b/tokenization.py
index 6c330a1..d07c627 100644
--- a/tokenization.py
+++ b/tokenization.py
@@ -35,6 +35,9 @@ def tokenize(sentence, tokenizer, type: str=PreProcessType.Base) -> dict:
         max_length=128,
         add_special_tokens=True,
     )
+    for key in outputs.keys():
+        outputs[key] = outputs[key].squeeze()
+
     if type != PreProcessType.Base:
         tokenized = tokenizer.tokenize(sentence)
 
@@ -42,25 +45,25 @@ def tokenize(sentence, tokenizer, type: str=PreProcessType.Base) -> dict:
             # Add embedding value for entity marker tokens([E1], [/E1], [E2], [/E2])
             entity_indices = find_entity_indices(tokenized)
             for open, close in entity_indices.values():
-                outputs.token_type_ids[0][
+                outputs.token_type_ids[
                     OFFSET + open : OFFSET + close + 1
                 ] += ENTITY_SCORE
 
         elif type == PreProcessType.ESP:
             # Add embedding value for separation token([SEP])
             last_sep_idx = fine_sep_indices(tokenized).pop()
-            outputs.token_type_ids[0][OFFSET : last_sep_idx + 1] += SEP_SCORE
+            outputs.token_type_ids[OFFSET : last_sep_idx + 1] += SEP_SCORE
             return outputs
 
         elif type == PreProcessType.EMSP:
             entity_indices = find_entity_indices(tokenized)
             for (open, close) in entity_indices.values():
-                outputs.token_type_ids[0][
+                outputs.token_type_ids[
                     OFFSET + open : OFFSET + close + 1
                 ] += ENTITY_SCORE
 
             last_sep_idx = fine_sep_indices(tokenized).pop()
-            outputs.token_type_ids[0][OFFSET : last_sep_idx + 1] += SEP_SCORE
+            outputs.token_type_ids[OFFSET : last_sep_idx + 1] += SEP_SCORE
 
     return outputs
 
diff --git a/train.py b/train.py
index 1237fbc..7af9198 100644
--- a/train.py
+++ b/train.py
@@ -98,6 +98,8 @@ def train(
 
         for sentences, labels in tqdm(train_loader, desc="[Train]"):
             if model_type == ModelType.SequenceClf:
+                for key in sentences.keys():
+                    sentences[key] = sentences[key].squeeze()
                 loss, outputs = model(**sentences, labels=labels).values()
             else:
                 outputs = model(**sentences)
diff --git a/wandb/latest-run b/wandb/latest-run
index 0566aeb..361bd29 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20210419_071305-2r3gvfg4
\ No newline at end of file
+run-20210419_073929-z5csch5s
\ No newline at end of file
