diff --git a/config.py b/config.py
index 957b5d5..a275bc3 100644
--- a/config.py
+++ b/config.py
@@ -84,3 +84,4 @@ class ModelType:
 class PreTrainedType:
     MultiLingual: str = "bert-base-multilingual-cased"
     BaseUncased: str = "bert-base-uncased"
+    KoELECTRAv3: str = "monologg/koelectra-base-v3-discriminator"
diff --git a/dataset.py b/dataset.py
index 696eb39..325d9f6 100644
--- a/dataset.py
+++ b/dataset.py
@@ -8,7 +8,7 @@ from torch.utils.data.sampler import SubsetRandomSampler
 from transformers.utils import logging
 
 logger = logging.get_logger(__name__)
-from config import Config, PreProcessType
+from config import Config, ModelType, PreProcessType
 from utils import load_pickle
 from tokenization import (
     load_tokenizer,
@@ -34,12 +34,13 @@ class REDataset(Dataset):
     def __init__(
         self,
         root: str = Config.Train,
+        model_type: str = ModelType.KoELECTRAv3,
         preprocess_type: str = PreProcessType.EM,
         device: str = Config.Device,
     ):
         self.data = self._load_data(root, preprocess_type=preprocess_type)
         self.labels = self.data["label"].tolist()
-        self.tokenizer = load_tokenizer(type=preprocess_type)
+        self.tokenizer = load_tokenizer(model_type=model_type, preprocess_type=preprocess_type)
         self.inputs = tokenize(
             self.data, self.tokenizer, preprocess_type
         )
diff --git a/models.py b/models.py
index 2f8e556..8d9a1be 100644
--- a/models.py
+++ b/models.py
@@ -1,6 +1,6 @@
 import torch
 from torch import nn
-from transformers import BertModel, BertConfig, BertForSequenceClassification, ElectraModel, ElectraTokenizer
+from transformers import BertModel, BertConfig, BertForSequenceClassification, ElectraModel, ElectraConfig
 from config import ModelType, Config, ModelType, PreTrainedType
 from dataset import REDataset, split_train_test_loader
 
@@ -48,7 +48,7 @@ def load_model(
             pooler_idx=pooler_idx
         )
     elif model_type == ModelType.KoELECTRAv3:
-        model = ElectraModel.from_pretrained("monologg/koelectra-small-v3-discriminator")
+        model = VanillaKoElectra(num_classes=num_classes)
 
     else:
         raise NotImplementedError()
@@ -61,6 +61,26 @@ def load_model(
     return model
 
 
+class VanillaKoElectra(nn.Module):
+    def __init__(self, num_classes):
+        super(VanillaKoElectra, self).__init__()
+        config = ElectraConfig.from_pretrained("monologg/koelectra-base-v3-discriminator")
+        self.backbone = ElectraModel.from_pretrained("monologg/koelectra-base-v3-discriminator", config=config)
+        self.linear = nn.Linear(in_features=768, out_features=num_classes)
+    
+    def forward(self, input_ids, token_type_ids, attention_mask):
+        x = self.backbone(
+            input_ids=input_ids,
+            token_type_ids=token_type_ids,
+            attention_mask=attention_mask
+            )
+        output = self.linear(x)
+        return output
+    
+    def resize_token_embeddings(self, new_num_tokens: int):
+        self.backbone.resize_token_embeddings(new_num_tokens)
+
+
 class VanillaBert_v2(nn.Module):
     def __init__(
         self,
diff --git a/notebooks/Sketch - Model.ipynb b/notebooks/Sketch - Model.ipynb
index 7cda36d..0bfc98a 100644
--- a/notebooks/Sketch - Model.ipynb	
+++ b/notebooks/Sketch - Model.ipynb	
@@ -23,7 +23,7 @@
  "cells": [
   {
    "cell_type": "code",
-   "execution_count": 33,
+   "execution_count": 1,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -31,7 +31,7 @@
     "import torch\n",
     "from torch.utils.data import DataLoader\n",
     "from torch import nn\n",
-    "from transformers import BertTokenizer, BertTokenizerFast, BertForSequenceClassification, BertModel, BertConfig\n",
+    "from transformers import BertTokenizer, BertTokenizerFast, BertForSequenceClassification, BertModel, BertConfig, ElectraModel, ElectraConfig, ElectraTokenizer\n",
     "from tokenizers import BertWordPieceTokenizer\n",
     "\n",
     "\n",
@@ -43,80 +43,124 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 6,
+   "execution_count": 20,
    "metadata": {},
    "outputs": [],
    "source": [
-    "config = BertConfig.from_pretrained(PreTrainedType.MultiLingual)\n",
-    "config.num_labels = 42"
+    "class VanillaKoElectra(nn.Module):\n",
+    "    def __init__(self, num_classes):\n",
+    "        super(VanillaKoElectra, self).__init__()\n",
+    "        config = ElectraConfig.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
+    "        self.backbone = ElectraModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\", config=config)\n",
+    "        self.linear = nn.Linear(in_features=768, out_features=num_classes)\n",
+    "    \n",
+    "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
+    "        x = self.backbone(\n",
+    "            input_ids=input_ids,\n",
+    "            token_type_ids=token_type_ids,\n",
+    "            attention_mask=attention_mask\n",
+    "            )\n",
+    "        output = self.linear(x)\n",
+    "        return x\n",
+    "    \n",
+    "    def resize_tokens_embeddings(self, new_num_tokens: int)"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 7,
+   "execution_count": 30,
    "metadata": {},
    "outputs": [
+    {
+     "output_type": "display_data",
+     "data": {
+      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=263326.0, style=ProgressStyle(descripti…",
+      "application/vnd.jupyter.widget-view+json": {
+       "version_major": 2,
+       "version_minor": 0,
+       "model_id": "fe913550254644a398eb96689fa08068"
+      }
+     },
+     "metadata": {}
+    },
     {
      "output_type": "stream",
-     "name": "stderr",
+     "name": "stdout",
      "text": [
-      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
-      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
-      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
-      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
-      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
+      "\n"
      ]
-    }
-   ],
-   "source": [
-    "model = BertForSequenceClassification.from_pretrained(PreTrainedType.MultiLingual, config=config)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 17,
-   "metadata": {},
-   "outputs": [
+    },
+    {
+     "output_type": "display_data",
+     "data": {
+      "text/plain": "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=61.0, style=ProgressStyle(description_w…",
+      "application/vnd.jupyter.widget-view+json": {
+       "version_major": 2,
+       "version_minor": 0,
+       "model_id": "4d673046a45447e1a228d3fb94eb9284"
+      }
+     },
+     "metadata": {}
+    },
     {
      "output_type": "stream",
      "name": "stdout",
      "text": [
-      "Load raw data...\tpreprocessing for 'Base'...\tdone!\n",
-      "Load Tokenizer...\tdone!\n",
-      "Apply Tokenization...\tdone!\n"
+      "\n"
      ]
     }
    ],
    "source": [
-    "dataset = REDataset(device='cpu')"
+    "tok = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 19,
+   "execution_count": 32,
    "metadata": {},
-   "outputs": [],
+   "outputs": [
+    {
+     "output_type": "execute_result",
+     "data": {
+      "text/plain": [
+       "2"
+      ]
+     },
+     "metadata": {},
+     "execution_count": 32
+    }
+   ],
    "source": [
-    "loader = DataLoader(dataset, batch_size=4)"
+    "tok.add_special_tokens({'additional_special_tokens': [\"[E1]\", \"[/E1]\"]})"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 20,
+   "execution_count": 34,
    "metadata": {},
-   "outputs": [],
+   "outputs": [
+    {
+     "output_type": "execute_result",
+     "data": {
+      "text/plain": [
+       "35002"
+      ]
+     },
+     "metadata": {},
+     "execution_count": 34
+    }
+   ],
    "source": [
-    "for sents, labels in loader:\n",
-    "    break"
+    "len(tok)"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 24,
+   "execution_count": 21,
    "metadata": {},
    "outputs": [],
    "source": [
-    "output = model.bert(**sents)"
+    "model = VanillaKoElectra(num_classes=42)"
    ]
   },
   {
@@ -125,147 +169,117 @@
    "metadata": {},
    "outputs": [],
    "source": [
-    "pooler = output.last_hidden_state[:, 0, :]"
+    "backbone = ElectraModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\", config=config)"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 30,
+   "execution_count": 29,
    "metadata": {},
    "outputs": [
     {
      "output_type": "execute_result",
      "data": {
       "text/plain": [
-       "torch.Size([4, 42])"
+       "Embedding(35000, 768, padding_idx=0)"
       ]
      },
      "metadata": {},
-     "execution_count": 30
+     "execution_count": 29
     }
    ],
    "source": [
-    "model.classifier(pooler).size()"
+    "backbone.resize_token_embeddings()"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 42,
+   "execution_count": 27,
    "metadata": {},
-   "outputs": [],
+   "outputs": [
+    {
+     "output_type": "error",
+     "ename": "ModuleAttributeError",
+     "evalue": "'VanillaKoElectra' object has no attribute 'resize_token_embeddings'",
+     "traceback": [
+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
+      "\u001b[0;32m<ipython-input-27-c1a5d89e94d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_token_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
+      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    770\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 772\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;31mModuleAttributeError\u001b[0m: 'VanillaKoElectra' object has no attribute 'resize_token_embeddings'"
+     ]
+    }
+   ],
    "source": [
-    "class VanillaBert_v2(nn.Module):\n",
-    "    def __init__(\n",
-    "        self,\n",
-    "        model_type: str = ModelType.SequenceClf,  # BertForSequenceClassification\n",
-    "        pretrained_type: str = PreTrainedType.MultiLingual,  # bert-base-multilingual-cased\n",
-    "        num_labels: int = Config.NumClasses,  # 42\n",
-    "        pooler_idx: int = 0\n",
-    "    ):\n",
-    "        super(VanillaBert_v2, self).__init__()\n",
-    "        bert = self.load_bert(\n",
-    "            model_type=model_type,\n",
-    "            pretrained_type=pretrained_type,\n",
-    "        )\n",
-    "        self.backbone = bert.bert\n",
-    "        self.dropout = bert.dropout\n",
-    "        self.clf = bert.classifier\n",
-    "        self.idx = 0 if pooler_idx == 0 else pooler_idx\n",
-    "\n",
-    "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
-    "        x = self.backbone(\n",
-    "            input_ids=input_ids,\n",
-    "            token_type_ids=token_type_ids,\n",
-    "            attention_mask=attention_mask,\n",
-    "        )\n",
-    "        x = x.last_hidden_state[:, self.idx, :]\n",
-    "        x = self.dropout(x)\n",
-    "        output = self.clf(x)\n",
-    "        return output\n",
-    "\n",
-    "    @staticmethod\n",
-    "    def load_bert(model_type, pretrained_type):\n",
-    "        config = BertConfig.from_pretrained(pretrained_type)\n",
-    "        config.num_labels = 42\n",
-    "        if model_type == ModelType.SequenceClf:\n",
-    "            model = BertForSequenceClassification.from_pretrained(pretrained_type, config=config)\n",
-    "        else:\n",
-    "            raise NotImplementedError()\n",
-    "\n",
-    "        return model"
+    "model.resize_token_embeddings()"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 43,
+   "execution_count": 22,
    "metadata": {},
    "outputs": [
     {
      "output_type": "stream",
-     "name": "stderr",
+     "name": "stdout",
      "text": [
-      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
-      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
-      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
-      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
-      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
+      "Load raw data...\tpreprocessing for 'EntityMarker'...\tdone!\n",
+      "Load Tokenizer for EntityMarker...\tdone!\n",
+      "Update token_type_ids: 9000it [00:00, 33603.24it/s]\n"
      ]
     }
    ],
    "source": [
-    "model = VanillaBert_v2()"
+    "data = REDataset(device='cpu')"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 23,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "loader = DataLoader(data, batch_size=2)"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 44,
+   "execution_count": 24,
    "metadata": {},
    "outputs": [],
    "source": [
-    "output = model(**sents)"
+    "for sents, labels in loader:\n",
+    "    break"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 45,
+   "execution_count": 25,
    "metadata": {},
    "outputs": [
     {
-     "output_type": "execute_result",
-     "data": {
-      "text/plain": [
-       "tensor([[-0.3085, -0.0310,  0.1683, -0.1096,  0.0322,  0.2899, -0.2387, -0.1071,\n",
-       "          0.1112, -0.3434, -0.0009, -0.3300, -0.0189, -0.0739,  0.0318, -0.1613,\n",
-       "          0.1797, -0.2363, -0.1003, -0.0215, -0.3221, -0.0310,  0.0659, -0.0153,\n",
-       "          0.1841,  0.1360,  0.0297, -0.2904,  0.0027, -0.1875, -0.1578, -0.0472,\n",
-       "          0.2544,  0.3674,  0.0224, -0.3766,  0.2296, -0.2980,  0.4823, -0.1805,\n",
-       "          0.1215,  0.0884],\n",
-       "        [-0.3178, -0.0420,  0.5670, -0.1458,  0.3103,  0.3062, -0.4243, -0.1841,\n",
-       "          0.0180, -0.4984,  0.0176, -0.2405,  0.0719, -0.3171,  0.0287, -0.3817,\n",
-       "          0.1676, -0.0818, -0.1787,  0.0245, -0.2646,  0.0456,  0.2499,  0.0211,\n",
-       "          0.5569,  0.2341, -0.1474, -0.2639, -0.1208, -0.1170, -0.2079,  0.0302,\n",
-       "         -0.0138,  0.3647,  0.0919, -0.4525,  0.3361, -0.0224,  0.3537, -0.3153,\n",
-       "         -0.0371,  0.0761],\n",
-       "        [-0.1305, -0.0609,  0.4915, -0.0239,  0.0627,  0.1599, -0.1920, -0.2792,\n",
-       "          0.1001, -0.3685,  0.0419, -0.2608, -0.0087, -0.1381, -0.0463, -0.2021,\n",
-       "          0.1034, -0.1794, -0.1384, -0.0544, -0.1763, -0.1280,  0.0806,  0.0696,\n",
-       "          0.3274,  0.1910,  0.0291, -0.2343, -0.0489, -0.2149, -0.0605, -0.1470,\n",
-       "          0.1708,  0.3493,  0.0546, -0.3818,  0.1025, -0.1922,  0.2025, -0.0817,\n",
-       "          0.0399,  0.0775],\n",
-       "        [-0.3324, -0.1546,  0.5358, -0.0141,  0.1724,  0.3538, -0.0277, -0.3849,\n",
-       "          0.2843, -0.2575, -0.0066, -0.0141, -0.0665, -0.2008,  0.0020, -0.3455,\n",
-       "          0.1137, -0.4120, -0.1799, -0.0884, -0.1748,  0.1247,  0.1147, -0.0386,\n",
-       "          0.1345,  0.1740, -0.1737, -0.1824, -0.2183, -0.0781,  0.0122,  0.1430,\n",
-       "          0.1853,  0.5613,  0.2649, -0.2512,  0.1894, -0.1758,  0.3394, -0.2700,\n",
-       "          0.0754,  0.1125]], grad_fn=<AddmmBackward>)"
-      ]
-     },
-     "metadata": {},
-     "execution_count": 45
+     "output_type": "error",
+     "ename": "IndexError",
+     "evalue": "index out of range in self",
+     "traceback": [
+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
+      "\u001b[0;32m<ipython-input-25-5c6ca7e647fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
+      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;32m<ipython-input-20-534edf0aa611>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             )\n\u001b[1;32m     14\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/electra/modeling_electra.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         hidden_states = self.embeddings(\n\u001b[0;32m--> 844\u001b[0;31m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m         )\n\u001b[1;32m    846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/electra/modeling_electra.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m         return F.embedding(\n\u001b[1;32m    125\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1812\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1813\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1814\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
+     ]
     }
    ],
    "source": [
-    "output"
+    "model(**sents)"
    ]
   }
  ]
diff --git a/tokenization.py b/tokenization.py
index 5b7c714..67c0e50 100644
--- a/tokenization.py
+++ b/tokenization.py
@@ -2,8 +2,8 @@ from tqdm import tqdm
 import pandas as pd
 import torch
 from dataclasses import dataclass
-from transformers import BertTokenizer, AutoTokenizer
-from config import PreProcessType, PreTrainedType
+from transformers import BertTokenizer, ElectraTokenizer
+from config import ModelType, PreProcessType, PreTrainedType
 
 # 토큰화 결과 [CLS] 토큰이 가장 앞에 붙게 되기 떄문에
 # Entity Mark에 대한 임베딩 값을 조절하는 과정에서 인덱스를 OFFSET만큼 밀어주기 위해 사용
@@ -30,7 +30,7 @@ class SpecialToken:
     E2Close: str = "[/E2]"
 
 
-def load_tokenizer(type: str = PreProcessType.Base):
+def load_tokenizer(model_type: str = ModelType.KoELECTRAv3, preprocess_type: str = PreProcessType.Base):
     """사전 학습된 tokenizer를 불러오는 함수
     Args
     ---
@@ -40,25 +40,47 @@ def load_tokenizer(type: str = PreProcessType.Base):
     ---
     - tokenizer(BertTokenizer): 사전 학습된 tokenizer
     """
-    print(f"Load Tokenizer for {type}...", end="\t")
-    if type in [PreProcessType.Base, PreProcessType.ES, PreProcessType.ESP]:
-        tokenizer = BertTokenizer.from_pretrained(PreTrainedType.MultiLingual)
-
-    # Entity Marker, Entity Marker Separator with Position Embedding
-    elif type == PreProcessType.EM or type == PreProcessType.EMSP:
-        tokenizer = BertTokenizer.from_pretrained(PreTrainedType.MultiLingual)
-        tokenizer.add_special_tokens(
-            {
-                "additional_special_tokens": [
-                    SpecialToken.E1Open,
-                    SpecialToken.E1Close,
-                    SpecialToken.E2Open,
-                    SpecialToken.E2Close,
-                ]
-            }
-        )
-    else:
-        raise NotImplementedError
+    print(f"Load Tokenizer for {preprocess_type}...", end="\t")
+    if model_type in [ModelType.SequenceClf, ModelType.VanillaBert, ModelType.VanillaBert_v2]:
+        if preprocess_type in [PreProcessType.Base, PreProcessType.ES, PreProcessType.ESP]:
+            tokenizer = BertTokenizer.from_pretrained(PreTrainedType.MultiLingual)
+
+        # Entity Marker, Entity Marker Separator with Position Embedding
+        elif preprocess_type in [PreProcessType.EM, PreProcessType.EMSP]:
+            tokenizer = BertTokenizer.from_pretrained(PreTrainedType.MultiLingual)
+            tokenizer.add_special_tokens(
+                {
+                    "additional_special_tokens": [
+                        SpecialToken.E1Open,
+                        SpecialToken.E1Close,
+                        SpecialToken.E2Open,
+                        SpecialToken.E2Close,
+                    ]
+                }
+            )
+        else:
+            raise NotImplementedError
+
+    elif model_type == ModelType.KoELECTRAv3:
+        if preprocess_type in [PreProcessType.Base, PreProcessType.ES, PreProcessType.ESP]:
+            tokenizer = ElectraTokenizer.from_pretrained(PreTrainedType.KoELECTRAv3)
+
+        # Entity Marker, Entity Marker Separator with Position Embedding
+        elif preprocess_type in [PreProcessType.EM, PreProcessType.EMSP]:
+            tokenizer = ElectraTokenizer.from_pretrained(PreTrainedType.KoELECTRAv3)
+            tokenizer.add_special_tokens(
+                {
+                    "additional_special_tokens": [
+                        SpecialToken.E1Open,
+                        SpecialToken.E1Close,
+                        SpecialToken.E2Open,
+                        SpecialToken.E2Close,
+                    ]
+                }
+            )
+        else:
+            raise NotImplementedError
+
     print("done!")
     return tokenizer
 
diff --git a/train.py b/train.py
index 3847850..bc1238b 100644
--- a/train.py
+++ b/train.py
@@ -8,7 +8,7 @@ from torch.utils.data import DataLoader
 import wandb
 from evaluation import evaluate
 from models import load_model
-from dataset import REDataset, REDataset_fix, split_train_test_loader
+from dataset import REDataset, REDataset, split_train_test_loader
 from optimizers import get_optimizer, get_scheduler
 from criterions import get_criterion
 from utils import get_timestamp, get_timestamp, set_seed, verbose, ckpt_name, save_json
@@ -45,7 +45,7 @@ def train(
     set_seed(seed)
 
     # load data
-    dataset = REDataset_fix(root=data_root, preprocess_type=preprocess_type, device=device)
+    dataset = REDataset(root=data_root, model_type=model_type, preprocess_type=preprocess_type, device=device)
 
     # 학습 데이터를 분리하지 않을 경우
     if valid_size == 0:
@@ -285,16 +285,16 @@ if __name__ == "__main__":
     LOAD_STATE_DICT = None
 
     parser = argparse.ArgumentParser()
-    parser.add_argument("--model-type", type=str, default=ModelType.SequenceClf)
+    parser.add_argument("--model-type", type=str, default=ModelType.KoELECTRAv3)
     parser.add_argument(
-        "--pretrained-type", type=str, default=PreTrainedType.MultiLingual
+        "--pretrained-type", type=str, default=PreTrainedType.KoELECTRAv3
     )
     parser.add_argument("--num-classes", type=int, default=Config.NumClasses)
     parser.add_argument("--pooler-idx", type=int, default=0)
     parser.add_argument("--dropout", type=float, default=0.8)
     parser.add_argument("--load-state-dict", type=str, default=LOAD_STATE_DICT)
     parser.add_argument("--data-root", type=str, default=Config.Train)
-    parser.add_argument("--preprocess-type", type=str, default=PreProcessType.ESP)
+    parser.add_argument("--preprocess-type", type=str, default=PreProcessType.ES)
     parser.add_argument("--epochs", type=int, default=Config.Epochs)
     parser.add_argument("--valid-size", type=int, default=Config.ValidSize)
     parser.add_argument("--train-batch-size", type=int, default=Config.Batch8)
@@ -308,7 +308,7 @@ if __name__ == "__main__":
     parser.add_argument("--save-path", type=str, default=Config.CheckPoint)
 
     args = parser.parse_args()
-    if args.model_type == ModelType.SequenceClf:
+    if args.model_type in [ModelType.SequenceClf, ModelType.KoELECTRAv3]:
         args.loss_type = Loss.CE
         args.dropout = None
 
diff --git a/wandb/latest-run b/wandb/latest-run
index c30e059..afa8db0 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20210419_135904-2c5m4ye1
\ No newline at end of file
+run-20210419_150041-3occxxna
\ No newline at end of file
