diff --git a/config.py b/config.py
index 3cdd441..ff3a021 100644
--- a/config.py
+++ b/config.py
@@ -24,7 +24,7 @@ class Config:
     Label: str = LABEL if os.path.isfile(LABEL) else DOT + LABEL
     Logs: str = LOGS if os.path.isfile(LOGS) else DOT + LOGS
     NumClasses: int = 42
-    Epochs: int = 15
+    Epochs: int = 20
 
     Batch8: int = 8
     Batch16: int = 16
@@ -62,7 +62,7 @@ class Loss:
 
 @dataclass
 class PreProcessType:
-    Base: str = "Base"  # No preprocessing => 구려
+    Base: str = "Base"  # No preprocessing
     ES: str = (
         "EntitySeparation"  # Entity Separation, method as baseline of boostcamp itself
     )
diff --git a/inference.py b/inference.py
index 049d0ce..23d2be9 100644
--- a/inference.py
+++ b/inference.py
@@ -69,7 +69,7 @@ def get_model_pretrained_type(load_state_dict: str):
 
 
 if __name__ == "__main__":
-    MODELNAME = "VanillaBert_bert-base-multilingual-cased_20210418164452/VanillaBert_bert-base-multilingual-cased_ep(02)acc(0.4878)loss(0.0048)id(20210418164452).pth"
+    MODELNAME = "VanillaBert_bert-base-multilingual-cased_20210419110000/VanillaBert_bert-base-multilingual-cased_ep(17)acc(0.7044)loss(0.0043)id(20210419110000).pth"
     LOAD_STATE_DICT = os.path.join(Config.CheckPoint, MODELNAME)
 
     parser = argparse.ArgumentParser()
diff --git a/models.py b/models.py
index b935e94..c169625 100644
--- a/models.py
+++ b/models.py
@@ -11,6 +11,7 @@ def load_model(
     num_classes: int = Config.NumClasses,
     load_state_dict: str = None,
     pooler_idx: int = 0,  # get last hidden state from CLS
+    dropout: float = 0.5
 ):
     print("Load Model...", end="\t")
     # make BERT configuration
@@ -30,6 +31,7 @@ def load_model(
             pretrained_type=pretrained_type,
             num_labels=num_classes,
             pooler_idx=pooler_idx,
+            dropout=dropout
         )
     else:
         raise NotImplementedError()
@@ -49,6 +51,7 @@ class VanillaBert(nn.Module):
         pretrained_type: str = PreTrainedType.MultiLingual,  # bert-base-multilingual-cased
         num_labels: int = Config.NumClasses,  # 42
         pooler_idx: int = 0,
+        dropout: float = 0.5,
     ):
         super(VanillaBert, self).__init__()
         # BERT로부터 얻은 128(=max_length)개 hidden state 중 몇 번째를 활용할 지 결정. Default - 0(CLS 토큰의 인덱스)
@@ -58,7 +61,7 @@ class VanillaBert(nn.Module):
             pretrained_type=pretrained_type,
         )
         self.layernorm = nn.LayerNorm(768)  # 768: output length of backbone, BERT
-        self.dropout = nn.Dropout()
+        self.dropout = nn.Dropout(p=dropout)
         self.relu = nn.ReLU()
         self.linear = nn.Linear(in_features=768, out_features=num_labels)
 
diff --git a/notebooks/Sketch ESP.ipynb b/notebooks/Sketch ESP.ipynb
index 1864f96..bfe77a8 100644
--- a/notebooks/Sketch ESP.ipynb	
+++ b/notebooks/Sketch ESP.ipynb	
@@ -15,7 +15,7 @@
   "orig_nbformat": 2,
   "kernelspec": {
    "name": "python377jvsc74a57bd098b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f",
-   "display_name": "Python 3.7.7 64-bit"
+   "display_name": "Python 3.7.7 64-bit ('base': conda)"
   }
  },
  "nbformat": 4,
@@ -23,27 +23,362 @@
  "cells": [
   {
    "cell_type": "code",
-   "execution_count": 47,
+   "execution_count": 7,
    "metadata": {},
    "outputs": [],
    "source": [
     "import os\n",
     "import re\n",
     "import sys\n",
+    "from time import time\n",
     "import pandas as pd\n",
+    "from torch.utils.data import DataLoader\n",
+    "from pykospacing import spacing\n",
     "from transformers import BertTokenizer\n",
     "from konlpy.tag import Mecab\n",
+    "# from googletrans import LANGUAGES, Translator\n",
     "\n",
     "sys.path.insert(0, '../')\n",
     "from dataset import REDataset, COLUMNS\n",
     "from tokenization import load_tokenizer\n",
+    "from models import load_model\n",
     "from tokenization import SpecialToken as ST\n",
-    "from config import PreTrainedType, Config"
+    "from config import PreTrainedType, Config, ModelType"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 3,
+   "execution_count": 5,
+   "metadata": {},
+   "outputs": [
+    {
+     "output_type": "stream",
+     "name": "stdout",
+     "text": [
+      "Load Model...\tSome weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
+      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
+      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
+      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
+      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
+      "done!\n"
+     ]
+    },
+    {
+     "output_type": "execute_result",
+     "data": {
+      "text/plain": [
+       "BertForSequenceClassification(\n",
+       "  (bert): BertModel(\n",
+       "    (embeddings): BertEmbeddings(\n",
+       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
+       "      (position_embeddings): Embedding(512, 768)\n",
+       "      (token_type_embeddings): Embedding(2, 768)\n",
+       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
+       "      (dropout): Dropout(p=0.1, inplace=False)\n",
+       "    )\n",
+       "    (encoder): BertEncoder(\n",
+       "      (layer): ModuleList(\n",
+       "        (0): BertLayer(\n",
+       "          (attention): BertAttention(\n",
+       "            (self): BertSelfAttention(\n",
+       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (dropout): Dropout(p=0.1, inplace=False)\n",
+       "            )\n",
+       "            (output): BertSelfOutput(\n",
+       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
+       "              (dropout): Dropout(p=0.1, inplace=False)\n",
+       "            )\n",
+       "          )\n",
+       "          (intermediate): BertIntermediate(\n",
+       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
+       "          )\n",
+       "          (output): BertOutput(\n",
+       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
+       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
+       "            (dropout): Dropout(p=0.1, inplace=False)\n",
+       "          )\n",
+       "        )\n",
+       "        (1): BertLayer(\n",
+       "          (attention): BertAttention(\n",
+       "            (self): BertSelfAttention(\n",
+       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (dropout): Dropout(p=0.1, inplace=False)\n",
+       "            )\n",
+       "            (output): BertSelfOutput(\n",
+       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
+       "              (dropout): Dropout(p=0.1, inplace=False)\n",
+       "            )\n",
+       "          )\n",
+       "          (intermediate): BertIntermediate(\n",
+       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
+       "          )\n",
+       "          (output): BertOutput(\n",
+       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
+       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
+       "            (dropout): Dropout(p=0.1, inplace=False)\n",
+       "          )\n",
+       "        )\n",
+       "        (2): BertLayer(\n",
+       "          (attention): BertAttention(\n",
+       "            (self): BertSelfAttention(\n",
+       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (dropout): Dropout(p=0.1, inplace=False)\n",
+       "            )\n",
+       "            (output): BertSelfOutput(\n",
+       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
+       "              (dropout): Dropout(p=0.1, inplace=False)\n",
+       "            )\n",
+       "          )\n",
+       "          (intermediate): BertIntermediate(\n",
+       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
+       "          )\n",
+       "          (output): BertOutput(\n",
+       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
+       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
+       "            (dropout): Dropout(p=0.1, inplace=False)\n",
+       "          )\n",
+       "        )\n",
+       "        (3): BertLayer(\n",
+       "          (attention): BertAttention(\n",
+       "            (self): BertSelfAttention(\n",
+       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (dropout): Dropout(p=0.1, inplace=False)\n",
+       "            )\n",
+       "            (output): BertSelfOutput(\n",
+       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
+       "              (dropout): Dropout(p=0.1, inplace=False)\n",
+       "            )\n",
+       "          )\n",
+       "          (intermediate): BertIntermediate(\n",
+       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
+       "          )\n",
+       "          (output): BertOutput(\n",
+       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
+       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
+       "            (dropout): Dropout(p=0.1, inplace=False)\n",
+       "          )\n",
+       "        )\n",
+       "        (4): BertLayer(\n",
+       "          (attention): BertAttention(\n",
+       "            (self): BertSelfAttention(\n",
+       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (dropout): Dropout(p=0.1, inplace=False)\n",
+       "            )\n",
+       "            (output): BertSelfOutput(\n",
+       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
+       "              (dropout): Dropout(p=0.1, inplace=False)\n",
+       "            )\n",
+       "          )\n",
+       "          (intermediate): BertIntermediate(\n",
+       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
+       "          )\n",
+       "          (output): BertOutput(\n",
+       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
+       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
+       "            (dropout): Dropout(p=0.1, inplace=False)\n",
+       "          )\n",
+       "        )\n",
+       "        (5): BertLayer(\n",
+       "          (attention): BertAttention(\n",
+       "            (self): BertSelfAttention(\n",
+       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (dropout): Dropout(p=0.1, inplace=False)\n",
+       "            )\n",
+       "            (output): BertSelfOutput(\n",
+       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
+       "              (dropout): Dropout(p=0.1, inplace=False)\n",
+       "            )\n",
+       "          )\n",
+       "          (intermediate): BertIntermediate(\n",
+       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
+       "          )\n",
+       "          (output): BertOutput(\n",
+       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
+       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
+       "            (dropout): Dropout(p=0.1, inplace=False)\n",
+       "          )\n",
+       "        )\n",
+       "        (6): BertLayer(\n",
+       "          (attention): BertAttention(\n",
+       "            (self): BertSelfAttention(\n",
+       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (dropout): Dropout(p=0.1, inplace=False)\n",
+       "            )\n",
+       "            (output): BertSelfOutput(\n",
+       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
+       "              (dropout): Dropout(p=0.1, inplace=False)\n",
+       "            )\n",
+       "          )\n",
+       "          (intermediate): BertIntermediate(\n",
+       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
+       "          )\n",
+       "          (output): BertOutput(\n",
+       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
+       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
+       "            (dropout): Dropout(p=0.1, inplace=False)\n",
+       "          )\n",
+       "        )\n",
+       "        (7): BertLayer(\n",
+       "          (attention): BertAttention(\n",
+       "            (self): BertSelfAttention(\n",
+       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (dropout): Dropout(p=0.1, inplace=False)\n",
+       "            )\n",
+       "            (output): BertSelfOutput(\n",
+       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
+       "              (dropout): Dropout(p=0.1, inplace=False)\n",
+       "            )\n",
+       "          )\n",
+       "          (intermediate): BertIntermediate(\n",
+       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
+       "          )\n",
+       "          (output): BertOutput(\n",
+       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
+       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
+       "            (dropout): Dropout(p=0.1, inplace=False)\n",
+       "          )\n",
+       "        )\n",
+       "        (8): BertLayer(\n",
+       "          (attention): BertAttention(\n",
+       "            (self): BertSelfAttention(\n",
+       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (dropout): Dropout(p=0.1, inplace=False)\n",
+       "            )\n",
+       "            (output): BertSelfOutput(\n",
+       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
+       "              (dropout): Dropout(p=0.1, inplace=False)\n",
+       "            )\n",
+       "          )\n",
+       "          (intermediate): BertIntermediate(\n",
+       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
+       "          )\n",
+       "          (output): BertOutput(\n",
+       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
+       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
+       "            (dropout): Dropout(p=0.1, inplace=False)\n",
+       "          )\n",
+       "        )\n",
+       "        (9): BertLayer(\n",
+       "          (attention): BertAttention(\n",
+       "            (self): BertSelfAttention(\n",
+       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (dropout): Dropout(p=0.1, inplace=False)\n",
+       "            )\n",
+       "            (output): BertSelfOutput(\n",
+       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
+       "              (dropout): Dropout(p=0.1, inplace=False)\n",
+       "            )\n",
+       "          )\n",
+       "          (intermediate): BertIntermediate(\n",
+       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
+       "          )\n",
+       "          (output): BertOutput(\n",
+       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
+       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
+       "            (dropout): Dropout(p=0.1, inplace=False)\n",
+       "          )\n",
+       "        )\n",
+       "        (10): BertLayer(\n",
+       "          (attention): BertAttention(\n",
+       "            (self): BertSelfAttention(\n",
+       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (dropout): Dropout(p=0.1, inplace=False)\n",
+       "            )\n",
+       "            (output): BertSelfOutput(\n",
+       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
+       "              (dropout): Dropout(p=0.1, inplace=False)\n",
+       "            )\n",
+       "          )\n",
+       "          (intermediate): BertIntermediate(\n",
+       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
+       "          )\n",
+       "          (output): BertOutput(\n",
+       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
+       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
+       "            (dropout): Dropout(p=0.1, inplace=False)\n",
+       "          )\n",
+       "        )\n",
+       "        (11): BertLayer(\n",
+       "          (attention): BertAttention(\n",
+       "            (self): BertSelfAttention(\n",
+       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (dropout): Dropout(p=0.1, inplace=False)\n",
+       "            )\n",
+       "            (output): BertSelfOutput(\n",
+       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
+       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
+       "              (dropout): Dropout(p=0.1, inplace=False)\n",
+       "            )\n",
+       "          )\n",
+       "          (intermediate): BertIntermediate(\n",
+       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
+       "          )\n",
+       "          (output): BertOutput(\n",
+       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
+       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
+       "            (dropout): Dropout(p=0.1, inplace=False)\n",
+       "          )\n",
+       "        )\n",
+       "      )\n",
+       "    )\n",
+       "    (pooler): BertPooler(\n",
+       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
+       "      (activation): Tanh()\n",
+       "    )\n",
+       "  )\n",
+       "  (dropout): Dropout(p=0.1, inplace=False)\n",
+       "  (classifier): Linear(in_features=768, out_features=42, bias=True)\n",
+       ")"
+      ]
+     },
+     "metadata": {},
+     "execution_count": 5
+    }
+   ],
+   "source": [
+    "model = load_model(ModelType.SequenceClf)\n",
+    "model.cpu()\n",
+    "model.train()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 11,
    "metadata": {},
    "outputs": [
     {
@@ -57,248 +392,289 @@
     }
    ],
    "source": [
-    "data = REDataset()"
+    "dataset = REDataset(device='cpu')"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 7,
+   "execution_count": 14,
    "metadata": {},
    "outputs": [],
    "source": [
-    "tokenizer = BertTokenizer.from_pretrained(PreTrainedType.MultiLingual)"
+    "loader = DataLoader(dataset, batch_size=4)"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 13,
+   "execution_count": 15,
    "metadata": {},
-   "outputs": [
-    {
-     "output_type": "execute_result",
-     "data": {
-      "text/plain": [
-       "2"
-      ]
-     },
-     "metadata": {},
-     "execution_count": 13
-    }
-   ],
+   "outputs": [],
    "source": [
-    "special_tokens_dict = {\n",
-    "    'additional_special_tokens': [ST.E1Open, ST.E1Close, ST.E2Open, ST.E2Close]}\n",
-    "tokenizer.add_special_tokens(special_tokens_dict)"
+    "for sents, labels in loader:\n",
+    "    break"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 36,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "loss, outputs = model(**sents, labels=labels).values()"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 72,
+   "execution_count": 38,
    "metadata": {},
    "outputs": [
     {
      "output_type": "execute_result",
      "data": {
       "text/plain": [
-       "'영국에서 사용되는 스포츠 유틸리티 자동차의 브랜드로는 랜드로버(Land Rover)와 지프(Jeep)가 있으며, 이 브랜드들은 자동차의 종류를 일컫는 말로 사용되기도 한다.'"
+       "tensor([[-0.2576,  0.0415, -0.0403, -0.1123, -0.0196, -0.1867,  0.0522, -0.1830,\n",
+       "         -0.1118, -0.1232,  0.0412, -0.0852, -0.1258,  0.0886,  0.0677,  0.0340,\n",
+       "         -0.1190,  0.1963,  0.1283,  0.0847,  0.0378, -0.0813,  0.0798,  0.0308,\n",
+       "         -0.1249, -0.0688, -0.1100, -0.0185, -0.0520,  0.0494,  0.0492,  0.1050,\n",
+       "         -0.1278,  0.0104, -0.1069,  0.1565,  0.0253,  0.0362, -0.0573,  0.1551,\n",
+       "         -0.1461, -0.1355],\n",
+       "        [-0.1006,  0.1558,  0.1570, -0.1249, -0.0013, -0.2368,  0.0351, -0.1673,\n",
+       "         -0.0706, -0.0630, -0.0654,  0.0985, -0.2683,  0.2805, -0.0239,  0.1307,\n",
+       "         -0.0509,  0.1575, -0.0761,  0.2519,  0.2930, -0.2371,  0.0665, -0.0909,\n",
+       "          0.0248, -0.1902, -0.0675, -0.1522,  0.1778,  0.0296, -0.0176,  0.1829,\n",
+       "          0.0697,  0.1478, -0.0558,  0.1785, -0.0739, -0.1009,  0.0863,  0.1146,\n",
+       "         -0.1634, -0.1050],\n",
+       "        [-0.2248,  0.0539,  0.0282, -0.1621, -0.0597, -0.1486,  0.0701, -0.1567,\n",
+       "         -0.1493, -0.0685, -0.0360, -0.0085, -0.0343,  0.0839,  0.0538, -0.0777,\n",
+       "         -0.0446,  0.1628, -0.0575,  0.1605,  0.0431, -0.1223, -0.0178,  0.2244,\n",
+       "         -0.0866, -0.1345, -0.1241,  0.0264, -0.0023,  0.0495,  0.0486,  0.0752,\n",
+       "         -0.0446,  0.0979,  0.0509,  0.2971,  0.0803,  0.0771,  0.0098,  0.1750,\n",
+       "         -0.1635, -0.0715],\n",
+       "        [-0.2944,  0.0483,  0.0210, -0.1458, -0.0538, -0.2169,  0.1477, -0.1319,\n",
+       "         -0.1317, -0.0788, -0.0310, -0.1035,  0.0413,  0.1764,  0.0946, -0.0385,\n",
+       "         -0.0888,  0.2171,  0.0420,  0.1125,  0.1233, -0.0101, -0.0034,  0.0533,\n",
+       "          0.0488, -0.0704, -0.0334, -0.1307,  0.0680,  0.0206,  0.0542,  0.0947,\n",
+       "         -0.0465,  0.0714, -0.0466,  0.2024,  0.0167,  0.0194,  0.0379,  0.0630,\n",
+       "         -0.1309, -0.0602]], grad_fn=<AddmmBackward>)"
       ]
      },
      "metadata": {},
-     "execution_count": 72
+     "execution_count": 38
     }
    ],
    "source": [
-    "sample"
+    "outputs"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 102,
+   "execution_count": 21,
    "metadata": {},
    "outputs": [
     {
      "output_type": "execute_result",
      "data": {
       "text/plain": [
-       "<re.Match object; span=(34, 46), match='(Land Rover)'>"
+       "tensor(3.7061, grad_fn=<NllLossBackward>)"
       ]
      },
      "metadata": {},
-     "execution_count": 102
+     "execution_count": 21
     }
    ],
    "source": [
-    "re.search(r\"\\([a-zA-Z가-힣0-9\\s?]+\\)\", sample)"
+    "output.loss"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 20,
-   "metadata": {},
-   "outputs": [],
-   "source": []
-  },
-  {
-   "cell_type": "code",
-   "execution_count": 70,
+   "execution_count": 34,
    "metadata": {},
    "outputs": [],
    "source": [
-    "raw = pd.read_csv(Config.Train, sep='\\t', header=None, names=COLUMNS)\n",
-    "raw = raw.drop(['id'], axis=1)"
+    "a, b = output.values()"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 104,
+   "execution_count": 35,
    "metadata": {},
-   "outputs": [],
+   "outputs": [
+    {
+     "output_type": "execute_result",
+     "data": {
+      "text/plain": [
+       "tensor([[-0.1884,  0.1002,  0.0080, -0.1926, -0.0929, -0.2002,  0.1053, -0.2207,\n",
+       "          0.0040, -0.0709, -0.0488, -0.0330, -0.1938,  0.1910,  0.0389, -0.0190,\n",
+       "         -0.0605,  0.2034,  0.0611,  0.0753,  0.0798, -0.1167,  0.1096,  0.0070,\n",
+       "         -0.0777, -0.1542, -0.1193, -0.1325,  0.0406, -0.0190,  0.0598,  0.1240,\n",
+       "         -0.0067,  0.1166, -0.1306,  0.2776,  0.0656, -0.0752,  0.0817,  0.0865,\n",
+       "         -0.1244, -0.1109],\n",
+       "        [-0.1549,  0.1705,  0.0495, -0.0540, -0.0843, -0.1717,  0.0775, -0.1785,\n",
+       "         -0.1004,  0.0228, -0.0049, -0.0520, -0.0769,  0.1905,  0.0623,  0.0853,\n",
+       "         -0.0525,  0.2831, -0.0504,  0.2236,  0.0913, -0.2786,  0.0249,  0.1152,\n",
+       "         -0.1319, -0.0654, -0.1120,  0.0092,  0.0649, -0.0491,  0.0654,  0.1766,\n",
+       "         -0.0360, -0.0084, -0.0500,  0.1527, -0.0202, -0.0326,  0.1777,  0.2141,\n",
+       "         -0.0242, -0.0608],\n",
+       "        [-0.1574,  0.0213,  0.0968, -0.1638, -0.0426, -0.2851,  0.0577, -0.1240,\n",
+       "         -0.1844, -0.0767, -0.1093, -0.0350, -0.1997,  0.1597, -0.0380,  0.2394,\n",
+       "         -0.1176,  0.1643, -0.0835,  0.0819,  0.2020, -0.1839,  0.0656,  0.1710,\n",
+       "         -0.0504, -0.0770, -0.0708,  0.1418,  0.0698, -0.0602,  0.2979,  0.0113,\n",
+       "          0.0547, -0.0388, -0.0291,  0.3736,  0.0136,  0.0113,  0.0551,  0.2149,\n",
+       "         -0.1297, -0.1422],\n",
+       "        [-0.2692,  0.1237,  0.0442, -0.1759, -0.0145, -0.2449,  0.1198, -0.2492,\n",
+       "         -0.1359, -0.1840, -0.0072, -0.1351, -0.0715,  0.1551,  0.0501, -0.0101,\n",
+       "         -0.0925,  0.2537,  0.0109,  0.1269,  0.1853, -0.1978,  0.0560,  0.0438,\n",
+       "         -0.0642, -0.1249, -0.0606, -0.1351,  0.0292,  0.0896,  0.1315,  0.0254,\n",
+       "         -0.0591,  0.0483,  0.0228,  0.2731, -0.0747,  0.0592,  0.0061,  0.1846,\n",
+       "         -0.0879, -0.0051]], grad_fn=<AddmmBackward>)"
+      ]
+     },
+     "metadata": {},
+     "execution_count": 35
+    }
+   ],
    "source": [
-    "tagger = Mecab()"
+    "b"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 108,
+   "execution_count": 5,
    "metadata": {},
    "outputs": [],
    "source": [
-    "raw['tokenized'] = raw['relation_state'].apply(lambda x: tagger.pos(x))"
+    "raw = pd.read_csv(Config.Train, sep='\\t', header=None, names=COLUMNS)\n",
+    "raw = raw.drop(['id'], axis=1)\n",
+    "tokenizer = BertTokenizer.from_pretrained(PreTrainedType.MultiLingual)\n",
+    "sample = raw['relation_state'].tolist()[0]\n"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 115,
+   "execution_count": 3,
    "metadata": {},
-   "outputs": [],
+   "outputs": [
+    {
+     "output_type": "execute_result",
+     "data": {
+      "text/plain": [
+       "4"
+      ]
+     },
+     "metadata": {},
+     "execution_count": 3
+    }
+   ],
    "source": [
-    "IDX = 1\n",
-    "sample = data.data['relation_state'].tolist()[IDX]\n",
-    "sample_pos = tagger.pos(sample)"
+    "special_tokens_dict = {\n",
+    "    'additional_special_tokens': [ST.E1Open, ST.E1Close, ST.E2Open, ST.E2Close]}\n",
+    "tokenizer.add_special_tokens(special_tokens_dict)"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 117,
+   "execution_count": 18,
    "metadata": {},
    "outputs": [
     {
-     "output_type": "execute_result",
-     "data": {
-      "text/plain": [
-       "                                      relation_state        e1  e1_start  \\\n",
-       "0  영국에서 사용되는 스포츠 유틸리티 자동차의 브랜드로는 랜드로버(Land Rover)...      랜드로버        30   \n",
-       "1  선거에서 민주당은 해산 전 의석인 230석에 한참 못 미치는 57석(지역구 27석,...       민주당         5   \n",
-       "2  유럽 축구 연맹(UEFA) 집행위원회는 2014년 1월 24일에 열린 회의를 통해 ...  유럽 축구 연맹         0   \n",
-       "3  용병 공격수 챠디의 부진과 시즌 초 활약한 강수일의 침체, 시즌 중반에 영입한 세르...       강수일        24   \n",
-       "4  람캄행 왕은 1237년에서 1247년 사이 수코타이의 왕 퍼쿤 씨 인트라팃과 쓰엉 ...       람캄행         0   \n",
-       "\n",
-       "   e1_end         e2  e2_start  e2_end  label  \\\n",
-       "0      33        자동차        19      21     17   \n",
-       "1       7        27석        42      44      0   \n",
-       "2       7       UEFA         9      12      6   \n",
-       "3      26        공격수         3       5      2   \n",
-       "4       2  퍼쿤 씨 인트라팃        32      40      8   \n",
-       "\n",
-       "                                               input  \n",
-       "0  영국에서 사용되는 스포츠 유틸리티 자동차의 브랜드로는 랜드로버(Land Rover)...  \n",
-       "1  선거에서 민주당은 해산 전 의석인 230석에 한참 못 미치는 57석(지역구 27석,...  \n",
-       "2  유럽 축구 연맹(UEFA) 집행위원회는 2014년 1월 24일에 열린 회의를 통해 ...  \n",
-       "3  용병 공격수 챠디의 부진과 시즌 초 활약한 강수일의 침체, 시즌 중반에 영입한 세르...  \n",
-       "4  람캄행 왕은 1237년에서 1247년 사이 수코타이의 왕 퍼쿤 씨 인트라팃과 쓰엉 ...  "
-      ],
-      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>relation_state</th>\n      <th>e1</th>\n      <th>e1_start</th>\n      <th>e1_end</th>\n      <th>e2</th>\n      <th>e2_start</th>\n      <th>e2_end</th>\n      <th>label</th>\n      <th>input</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>영국에서 사용되는 스포츠 유틸리티 자동차의 브랜드로는 랜드로버(Land Rover)...</td>\n      <td>랜드로버</td>\n      <td>30</td>\n      <td>33</td>\n      <td>자동차</td>\n      <td>19</td>\n      <td>21</td>\n      <td>17</td>\n      <td>영국에서 사용되는 스포츠 유틸리티 자동차의 브랜드로는 랜드로버(Land Rover)...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>선거에서 민주당은 해산 전 의석인 230석에 한참 못 미치는 57석(지역구 27석,...</td>\n      <td>민주당</td>\n      <td>5</td>\n      <td>7</td>\n      <td>27석</td>\n      <td>42</td>\n      <td>44</td>\n      <td>0</td>\n      <td>선거에서 민주당은 해산 전 의석인 230석에 한참 못 미치는 57석(지역구 27석,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>유럽 축구 연맹(UEFA) 집행위원회는 2014년 1월 24일에 열린 회의를 통해 ...</td>\n      <td>유럽 축구 연맹</td>\n      <td>0</td>\n      <td>7</td>\n      <td>UEFA</td>\n      <td>9</td>\n      <td>12</td>\n      <td>6</td>\n      <td>유럽 축구 연맹(UEFA) 집행위원회는 2014년 1월 24일에 열린 회의를 통해 ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>용병 공격수 챠디의 부진과 시즌 초 활약한 강수일의 침체, 시즌 중반에 영입한 세르...</td>\n      <td>강수일</td>\n      <td>24</td>\n      <td>26</td>\n      <td>공격수</td>\n      <td>3</td>\n      <td>5</td>\n      <td>2</td>\n      <td>용병 공격수 챠디의 부진과 시즌 초 활약한 강수일의 침체, 시즌 중반에 영입한 세르...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>람캄행 왕은 1237년에서 1247년 사이 수코타이의 왕 퍼쿤 씨 인트라팃과 쓰엉 ...</td>\n      <td>람캄행</td>\n      <td>0</td>\n      <td>2</td>\n      <td>퍼쿤 씨 인트라팃</td>\n      <td>32</td>\n      <td>40</td>\n      <td>8</td>\n      <td>람캄행 왕은 1237년에서 1247년 사이 수코타이의 왕 퍼쿤 씨 인트라팃과 쓰엉 ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
-     },
-     "metadata": {},
-     "execution_count": 117
+     "output_type": "error",
+     "ename": "AttributeError",
+     "evalue": "'NoneType' object has no attribute 'group'",
+     "traceback": [
+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
+      "\u001b[0;32m<ipython-input-18-be808d9f7116>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'안녕하세요'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
+      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/googletrans/client.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, text, dest, src, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0morigin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_translate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# this code will be updated when the format is changed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/googletrans/client.py\u001b[0m in \u001b[0;36m_translate\u001b[0;34m(self, text, dest, src, override)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_translate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_acquirer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         params = utils.build_params(query=text, src=src, dest=dest,\n\u001b[1;32m     80\u001b[0m                                     token=token, override=override)\n",
+      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/googletrans/gtoken.py\u001b[0m in \u001b[0;36mdo\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m         \u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/googletrans/gtoken.py\u001b[0m in \u001b[0;36m_update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# this will be the same as python code after stripping out a reserved word 'var'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRE_TKK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'var '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;31m# unescape special ascii characters such like a \\x3d(=)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unicode-escape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
+     ]
     }
    ],
    "source": [
-    "data.data.head()"
+    "translator.translate('안녕하세요')"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 116,
+   "execution_count": 64,
    "metadata": {},
    "outputs": [
     {
      "output_type": "execute_result",
      "data": {
       "text/plain": [
-       "'선거에서 민주당은 해산 전 의석인 230석에 한참 못 미치는 57석(지역구 27석, 비례대표 30석)을 획득하는 데 그쳤다.'"
+       "'영국에서 사용되는 스포츠 유틸리티 자동차의 브랜드로는 랜드로버Land Rover와 지프Jeep가 있으며 이 브랜드들은 자동차의 종류를 일컫는 말로 사용되기도 한다.'"
       ]
      },
      "metadata": {},
-     "execution_count": 116
+     "execution_count": 64
     }
    ],
    "source": [
-    "sample"
+    "# 특수문자만 선택\n",
+    "re.sub(r\"[^\\.\\s가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z0-9]\", '', sample)"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 114,
+   "execution_count": 49,
    "metadata": {},
    "outputs": [
     {
      "output_type": "execute_result",
      "data": {
       "text/plain": [
-       "[('영국', 'NNP'),\n",
-       " ('에서', 'JKB'),\n",
-       " ('사용', 'NNG'),\n",
-       " ('되', 'XSV'),\n",
-       " ('는', 'ETM'),\n",
-       " ('스포츠', 'NNG'),\n",
-       " ('유틸리티', 'NNG'),\n",
-       " ('자동차', 'NNG'),\n",
-       " ('의', 'JKG'),\n",
-       " ('브랜드', 'NNG'),\n",
-       " ('로', 'JKB'),\n",
-       " ('는', 'JX'),\n",
-       " ('랜', 'NNG'),\n",
-       " ('드로버', 'NNP'),\n",
-       " ('(', 'SSO'),\n",
-       " ('Land', 'SL'),\n",
-       " ('Rover', 'SL'),\n",
-       " (')', 'SSC'),\n",
-       " ('와', 'JC'),\n",
-       " ('지프', 'NNG'),\n",
-       " ('(', 'SSO'),\n",
-       " ('Jeep', 'SL'),\n",
-       " (')', 'SSC'),\n",
-       " ('가', 'JKS'),\n",
-       " ('있', 'VA'),\n",
-       " ('으며', 'EC'),\n",
-       " (',', 'SC'),\n",
-       " ('이', 'MM'),\n",
-       " ('브랜드', 'NNG'),\n",
-       " ('들', 'XSN'),\n",
-       " ('은', 'JX'),\n",
-       " ('자동차', 'NNG'),\n",
-       " ('의', 'JKG'),\n",
-       " ('종류', 'NNG'),\n",
-       " ('를', 'JKO'),\n",
-       " ('일컫', 'VV'),\n",
-       " ('는', 'ETM'),\n",
-       " ('말', 'NNG'),\n",
-       " ('로', 'JKB'),\n",
-       " ('사용', 'NNG'),\n",
-       " ('되', 'XSV'),\n",
-       " ('기', 'ETN'),\n",
-       " ('도', 'JX'),\n",
-       " ('한다', 'VV+EF'),\n",
-       " ('.', 'SF')]"
+       "'영국에서 사용되는 스포츠 유틸리티 자동차의 브랜드로는 랜드로버와 지프가 있으며, 이 브랜드들은 자동차의 종류를 일컫는 말로 사용되기도 한다.'"
       ]
      },
      "metadata": {},
-     "execution_count": 114
+     "execution_count": 49
     }
    ],
    "source": [
-    "['NNG', 'NNP' 'NNB' 'NNBC' 'NR' 'NP' 'VV' 'VA' 'XR']"
+    "# 괄호와 괄호 안의 문자 제거\n",
+    "re.sub(r\"\\([^)]*\\)\", '', sample)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 4,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "raw['relation_state'] = raw['relation_state'].apply(lambda x: re.sub(r\"\\([^)]*\\)\", '', x))\n",
+    "raw['relation_state'] = raw['relation_state'].apply(lambda x: re.sub(r\"[^\\.\\s가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z0-9]\", '', x))\n",
+    "raw['relation_state'] = raw['relation_state'].apply(lambda x: x.replace(' ', ''))"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 73,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "raw['relation_state'] = raw['relation_state'].apply(lambda x: spacing(x))"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "raw.head()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "asdf"
    ]
   }
  ]
diff --git a/requirements.txt b/requirements.txt
index c473563..0509457 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -6,4 +6,5 @@ fire==0.4.0
 transformers==4.2.0
 g2pK==0.9.4
 konlpy==0.5.2
-adamp==0.3.0
\ No newline at end of file
+adamp==0.3.0
+pykospacing @ git+https://github.com/haven-jeon/PyKoSpacing.git@1a36be492cc396559e7dce7825843af020ea231f
\ No newline at end of file
diff --git a/tokenization.py b/tokenization.py
index e27ad8d..5aa421e 100644
--- a/tokenization.py
+++ b/tokenization.py
@@ -9,18 +9,16 @@ class SpecialToken:
     SEP: str = "[SEP]"
     CLS: str = "[CLS]"
 
+    # 무엇(entity)은 무엇(entity)과 어떤 관계이다.
+    EOpen: str = "[ENT]"
+    EClose: str = "[/ENT]"
+    
     # 무엇(entity1)은 무엇(entity2)과 어떤 관계이다.
     E1Open: str = "[E1]"
     E1Close: str = "[/E1]"
     E2Open: str = "[E2]"
     E2Close: str = "[/E2]"
-
-    # 무엇(sub)은 무엇(obj)과 어떤 관계이다.
-    SUBOpen: str = "[SUB]"
-    SUBClose: str = "[/SUB]"
-    OBJOpen: str = "[OBJ]"
-    OBJClose: str = "[/OBJ]"
-
+    
 
 def load_tokenizer(type: str = PreProcessType.Base):
     """사전 학습된 tokenizer를 불러오는 함수
diff --git a/train.py b/train.py
index dec525c..c02264e 100644
--- a/train.py
+++ b/train.py
@@ -25,6 +25,7 @@ def train(
     pretrained_type: str = PreTrainedType.MultiLingual,  # 모델에 활용할 Pretrained BERT Backbone 이름
     num_classes: int = Config.NumClasses,  # 카테고리 수
     pooler_idx: int = 0,  # 인코딩 결과로부터 추출할 hidden state. 0: [CLS]
+    dropout: float = 0.8,
     load_state_dict: str = None,  # (optional) 저장한 weight 경로
     data_root: str = Config.Train,  # 학습 데이터 경로
     preprocess_type: str = PreProcessType.Base,  # 텍스트 전처리 타입
@@ -65,7 +66,7 @@ def train(
 
     # load model
     model = load_model(
-        model_type, pretrained_type, num_classes, load_state_dict, pooler_idx
+        model_type, pretrained_type, num_classes, load_state_dict, pooler_idx, dropout
     )
     model.to(device)
     model.train()
@@ -74,7 +75,9 @@ def train(
     criterion = get_criterion(type=loss_type)
     optimizer = get_optimizer(model=model, type=optim_type, lr=lr)
     if lr_scheduler is not None:
-        scheduler = get_scheduler(type=lr_scheduler, optimizer=optimizer, num_training_steps=TOTAL_STEPS)
+        scheduler = get_scheduler(
+            type=lr_scheduler, optimizer=optimizer, num_training_steps=TOTAL_STEPS
+        )
 
     # make checkpoint directory to save model during train
     checkpoint_dir = f"{model_type}_{pretrained_type}_{TIMESTAMP}"
@@ -94,8 +97,12 @@ def train(
         total_loss = 0
 
         for sentences, labels in tqdm(train_loader, desc="[Train]"):
-            outputs = model(**sentences)
-            loss = criterion(outputs, labels)
+            if model_type == ModelType.VanillaBert:
+                outputs = model(**sentences)
+                loss = criterion(outputs, labels)
+            else:
+                loss, outputs = model(**sentences, labels=labels).values()
+
             total_loss += loss.item()
 
             # backpropagation
@@ -134,9 +141,13 @@ def train(
                     }
                 )
 
+        # evaluate each epoch
         pred_arr = np.hstack(pred_list)
         true_arr = np.hstack(true_list)
 
+        train_eval = evaluate(y_true=true_arr, y_pred=pred_arr)  # ACC, F1, PRC, REC
+        train_loss = total_loss / len(true_arr)
+
         # validation phase
         valid_eval, valid_loss = validate(
             model=model,
@@ -235,14 +246,11 @@ def validate(model, model_type, valid_loader, criterion):
 
     with torch.no_grad():
         for sentences, labels in tqdm(valid_loader, desc="[Valid]"):
-            if model_type == ModelType.SequenceClf:
-                outputs = model(**sentences).logits
-            elif model_type == ModelType.Base:
-                outputs = model(**sentences).pooler_output
-            else:
+            if model_type == ModelType.VanillaBert:
                 outputs = model(**sentences)
-
-            loss = criterion(outputs, labels)
+                loss = criterion(outputs, labels)
+            else:
+                loss, outputs = model(**sentences, labels=labels).values()
             total_loss += loss.item()
 
             _, preds = torch.max(outputs, dim=1)
@@ -269,28 +277,31 @@ if __name__ == "__main__":
     LOAD_STATE_DICT = None
 
     parser = argparse.ArgumentParser()
-    parser.add_argument("--model-type", type=str, default=ModelType.VanillaBert)
+    parser.add_argument("--model-type", type=str, default=ModelType.SequenceClf)
     parser.add_argument(
         "--pretrained-type", type=str, default=PreTrainedType.MultiLingual
     )
     parser.add_argument("--num-classes", type=int, default=Config.NumClasses)
     parser.add_argument("--pooler-idx", type=int, default=0)
+    parser.add_argument("--dropout", type=float, default=0.8)
     parser.add_argument("--load-state-dict", type=str, default=LOAD_STATE_DICT)
     parser.add_argument("--data-root", type=str, default=Config.Train)
     parser.add_argument("--preprocess-type", type=str, default=PreProcessType.ES)
     parser.add_argument("--epochs", type=int, default=Config.Epochs)
     parser.add_argument("--valid-size", type=int, default=Config.ValidSize)
-    parser.add_argument("--train-batch-size", type=int, default=Config.Batch32)
+    parser.add_argument("--train-batch-size", type=int, default=Config.Batch8)
     parser.add_argument("--valid-batch-size", type=int, default=512)
     parser.add_argument("--optim-type", type=str, default=Optimizer.Adam)
-    parser.add_argument("--loss-type", type=str, default=Loss.CE)
-    parser.add_argument("--lr", type=float, default=Config.LRSlower)
+    parser.add_argument("--loss-type", type=str, default=Loss.LS)
+    parser.add_argument("--lr", type=float, default=Config.LRSlow)
     parser.add_argument("--lr-scheduler", type=str, default=Optimizer.CosineAnnealing)
     parser.add_argument("--device", type=str, default=Config.Device)
     parser.add_argument("--seed", type=int, default=Config.Seed)
     parser.add_argument("--save-path", type=str, default=Config.CheckPoint)
 
     args = parser.parse_args()
+    if args.model_type == ModelType.SequenceClf:
+        args.loss_type = Loss.CE
 
     # register logs to wandb
     name = (
@@ -300,7 +311,9 @@ if __name__ == "__main__":
     wandb.config.update(args)
 
     # train
-    TOTAL_STEPS = args.epochs * (int(TOTAL_SAMPLES * (1-args.valid_size)) // args.train_batch_size)
+    TOTAL_STEPS = args.epochs * (
+        int(TOTAL_SAMPLES * (1 - args.valid_size)) // args.train_batch_size
+    )
     print("=" * 100)
     print(args)
     print("=" * 100)
diff --git a/wandb/latest-run b/wandb/latest-run
index f1bf844..9a281c8 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20210418_123951-2v936pyt
\ No newline at end of file
+run-20210419_031333-lruztr83
\ No newline at end of file
