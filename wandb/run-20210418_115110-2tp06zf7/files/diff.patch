diff --git a/config.py b/config.py
index 0016017..188ea6b 100644
--- a/config.py
+++ b/config.py
@@ -24,7 +24,7 @@ class Config:
     Label: str = LABEL if os.path.isfile(LABEL) else DOT + LABEL
     Logs: str = LOGS if os.path.isfile(LOGS) else DOT + LOGS
     NumClasses: int = 42
-    Epochs: int = 3
+    Epochs: int = 15
 
     Batch8: int = 8
     Batch16: int = 16
diff --git a/optimizers.py b/optimizers.py
index 9e58ed9..3fed46f 100644
--- a/optimizers.py
+++ b/optimizers.py
@@ -20,11 +20,11 @@ def get_optimizer(model: nn.Module, type: str, lr: float):
     return optimizer
 
 
-def get_scheduler(type: str, optimizer):
+def get_scheduler(type: str, optimizer, num_training_steps: int):
     if type == Optimizer.CosineScheduler:
         scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=0)
     elif type == Optimizer.LambdaLR:
-        scheduler = get_linear_schedule_with_warmup(optimizer)
+        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=num_training_steps)
     else:
         raise NotImplementedError()
 
diff --git a/train.py b/train.py
index 2c89900..ef4eebb 100644
--- a/train.py
+++ b/train.py
@@ -11,7 +11,7 @@ from models import load_model
 from dataset import REDataset, split_train_test_loader
 from optimizers import get_optimizer, get_scheduler
 from criterions import get_criterion
-from utils import get_timestamp, set_seed, verbose, ckpt_name
+from utils import get_timestamp, get_timestamp, set_seed, verbose, ckpt_name
 from config import ModelType, Config, Optimizer, PreTrainedType, PreProcessType, Loss
 
 warnings.filterwarnings("ignore")
@@ -74,7 +74,7 @@ def train(
     criterion = get_criterion(type=loss_type)
     optimizer = get_optimizer(model=model, type=optim_type, lr=lr)
     if lr_scheduler is not None:
-        scheduler = get_scheduler(type=lr_scheduler, optimizer=optimizer)
+        scheduler = get_scheduler(type=lr_scheduler, optimizer=optimizer, num_training_steps=TOTAL_STEPS)
 
     # make checkpoint directory to save model during train
     checkpoint_dir = f"{model_type}_{pretrained_type}_{TIMESTAMP}"
@@ -280,11 +280,11 @@ if __name__ == "__main__":
     parser.add_argument("--preprocess-type", type=str, default=PreProcessType.ES)
     parser.add_argument("--epochs", type=int, default=Config.Epochs)
     parser.add_argument("--valid-size", type=int, default=Config.ValidSize)
-    parser.add_argument("--train-batch-size", type=int, default=Config.Batch64)
+    parser.add_argument("--train-batch-size", type=int, default=Config.Batch32)
     parser.add_argument("--valid-batch-size", type=int, default=512)
     parser.add_argument("--optim-type", type=str, default=Optimizer.Adam)
     parser.add_argument("--loss-type", type=str, default=Loss.CE)
-    parser.add_argument("--lr", type=float, default=Config.LR)
+    parser.add_argument("--lr", type=float, default=Config.LRSlow)
     parser.add_argument("--lr-scheduler", type=str, default=Optimizer.LambdaLR)
     parser.add_argument("--device", type=str, default=Config.Device)
     parser.add_argument("--seed", type=int, default=Config.Seed)
@@ -300,6 +300,7 @@ if __name__ == "__main__":
     wandb.config.update(args)
 
     # train
+    TOTAL_STEPS = args.epochs * (int(TOTAL_SAMPLES * (1-args.valid_size)) // args.train_batch_size)
     print("=" * 100)
     print(args)
     print("=" * 100)
diff --git a/wandb/latest-run b/wandb/latest-run
index d3ac3e0..a7ebfe6 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20210418_111349-5xe2aknr
\ No newline at end of file
+run-20210418_115110-2tp06zf7
\ No newline at end of file
